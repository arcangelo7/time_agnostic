<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Time agnostic</title>
    <link type="text/css" href="./css/neumorphism.css" rel="stylesheet">
    <link type="text/css" href="./css/custom.css" rel="stylesheet">
    <!-- Fontawesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" />
</head>

<body>
    <header class="header-global">
        <div class="nav-wrapper container position-relative">
            <ul class="nav nav-pills nav-fill flex-column flex-sm-row">
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0 active" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#next-meet">Prossimo incontro</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#past-meet">Archivio</a>
                </li>
            </ul>
        </div>
    </header>

    <main>
        <!-- Hero -->
        <section class="section section bg-soft pb-5 overflow-hidden z-2">
            <div class="container z-2">
                <div class="row justify-content-center text-center pt-6">
                    <div class="col-lg-8 col-xl-8">
                        <h1 class="display-2 mb-3">Time agnostic</h1>
                        <p class="lead px-md-6 mb-5">Diario di bordo del tirocinio presso <a
                                href="http://opencitations.net/" alt="Link to the OpenCitations main page"
                                target="_blank"><span class="oc-purple">Open</span><span
                                    class="oc-blue">Citations</span></a>.</p>
                        <div class="d-flex flex-column flex-wrap flex-md-row justify-content-md-center mb-5">
                            <a href="https://github.com/arcangelo7/time_agnostic" target="_blank"
                                class="btn btn-primary mb-3 mb-lg-0 mr-3" alt="Link to the repository"><i
                                    class="fab fa-github mr-2"></i>Vai
                                alla repository</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section pb-5" id="next-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Prossimo incontro (19/05/2021)</h2>
                </div>
                <h3>Brainstorming sul Time Agnostic Browser</h3>
                <ol>
                    <li>La libreria si deve basare su RDF e dev'essere il più riutilizzabile possibile in diversi contesti, per tutti quei dataset che utilizzano lo stesso modello di provenance di OpenCitations.</li>
                    <li>Modello usato per la provenance: PROV-O ontology + oco:hasUpdateQuery.
                        <ol>
                            <li><em>oco:hasUpdateQuery</em>: proprietà che registra le aggiunte e le cancellazioni come query SPARQL INSERT e DELETE, mentre l'uso di variabili SPARQL è proibito nelle query di update.</li>
                            <li>Ogni entità del dataset è rappresentata da uno o più snapshot (cioè da un'istanza della classe pro:Entity).</li>
                            <li>Ogni snapshot registra la composizione dell'entità in un preciso momento temporale, descritto tramite la proprietà pro:generatedAtTime. Ogni snapshot è collegato ai precedenti tramite la proprietà pro:wasDerivedFrom, all'entità descritta tramite pro:specializationOf e all'agente responsabile tramite pro:wasAttributedTo.</li>
                            <li>Vantaggi:
                                <ol>
                                    <li>Facilità nell'ottenere gli statement correnti di un'entità, siccome sono quelli correntemente disponibili nel dataset.</li>
                                    <li>Facilità nel ripristinare un'entità a un certo snapshot applicando l'operazione inversa (INSERT anziché DELETE e viceversa).</li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Cosa significa fare una query agnostica sul tempo?
                        <ol>
                            <li>Poniamo che l'utente effettui la seguente ricerca:
                                <div class="card bg-primary shadow-inset border-light">
                                    <pre><code class="prettyprint">
    CONSTRUCT {
        &lt;res&gt; ?p ?o
    } 
    WHERE {
        &lt;res&gt; ?p ?o.
    }
                                    </code></pre>
                                </div>
                                Dove res è l'URI di una risorsa. Cosa avviene dietro le quinte? 
                                <ol>
                                    <li>La query viene arricchita in modo da includere informazioni sulla provenance e, in particolare, sul tempo di generazione dello snapshot e sulla query di update:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    PREFIX oco: &lt;https://w3id.org/oc/ontology/&gt;
    PREFIX pro: &lt;http://www.w3.org/ns/prov#&gt;
    CONSTRUCT {
        &lt;res&gt; ?p ?o. 
        ?snapshot pro:generatedAtTime ?t;      
                oco:hasUpdateQuery ?updateQuery.
    }
    WHERE {
        &lt;res&gt; ?p ?o.
        ?snapshot pro:specializationOf &lt;res&gt;;
                pro:generatedAtTime ?t.
        OPTIONAL {
            ?snapshot oco:hasUpdateQuery ?updateQuery.
        }        
    }
                                            </code></pre>
                                        </div>
                                        Dal numero di triple di provenance ritornate è possibile dedurre quanti snapshot esistono di quella risorsa e, dal tempo in cui sono state generate, il loro ordine. 
                                    </li>
                                    <li>
                                        Viene effettuata la query dell'utente arricchita, che restituisce lo stato corrente del dataset rispetto a quella risorsa più le informazioni sulla provenance. Il risultato viene salvato in un dizionario, che potrebbe avere la seguente struttura:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    snapshots = {
        &lt;t&gt;: {
            "graph": &lt;grafo della risorsa al tempo t&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-1&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-1&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-n&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-n&gt;,
            "hasUpdateQuery": ""
        },
    }
                                            </code></pre>
                                        </div>
                                        Dove le chiavi corrispondono alle proprietà pro:generatedAtTime degli snapshot ritornati. Da notare che il dizionario al tempo t-n non ha updateQuery, perché corrisponde al grafo dell'entità nel momento in cui è stata creata. 
                                    </li>
                                    <li>
                                        Se la query di update è presente, per ottenere lo stato del grafo rispetto alla risorsa al tempo t-1, bisogna effettuare la query inversa rispetto a quella di update. Per farlo, è sufficiente sostituire "DELETE" con "INSERT" e "INSERT" con "DELETE". Nota bene: la query di update non viene effettuata sul triplestore, ma sul grafo della risorsa al tempo t, in modo da non alterare lo stato del triplestore. 
                                    </li>
                                    <li>
                                        Effettuata la query inversa, il nuovo stato della risorsa viene salvato alla chiave "graph" della chiave &lt;t-1&gt; del dizionario degli snapshots. Questa operazione viene effettuata ricorsivamente per ogni query di update n ritornata sul grafo della risorsa al tempo t-n+1. 
                                    </li>
                                </ol>
                                <li>
                                    <strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: nel momento in cui ho una serie di snapshot di un'entità, ogni serie potrebbe essere collegata ad altre entità, che a loro volta hanno una serie di snapshot, quindi devono essere riallineate temporalmente per fare la query corretta. 
                                    <ol>
                                        <li><i class="far fa-lightbulb"></i> Poniamo che l'utente voglia andare da una risorsa al tempo t-n a un'altra risorsa collegata che sia sempre al tempo t-n. Lo snapshot corretto della risorsa collegata è quello la cui proprietà pro:generatedAtTime è la minima antecedente o coincidente a t-n.</li>
                                        <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: è più conveniente allineare le risorse al momento della richiesta dell'utente o pre-processare l'intero dataset in modo da avere un nuovo dataset con le risorse allineate?</li>
                                    </ol>
                                </li>
                            </li>
                        </ol>
                    </li>
                    <li>La libreria dev'essere configurabile, pluggabile per usare metodi di altre librerie per semplificare alcuni compiti.</li>
                    <li>Cosa mi serve? Quali librerie mi servono in Python?
                        <ol>
                            <li>RDFLib per la manipolazione delle triple RDF.</li>
                            <li>sparqlwrapper per leggere i dati da un triplestore.
                                <ol>
                                    <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: devo prevedere la possibilità di lavorare anche su file?</li>
                                </ol>
                            </li>
                            <li>oc-ocdm, per mostrare un caso d'uso.</li>
                        </ol>
                    </li>
                    <li>Bisogna realizzare un browser che utilizzi la libreria nel back-end.
                        <ol>
                            <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: solo un browser o anche un editor?</li>
                        </ol>
                    </li>
                </ol>
                <h3>Cosa ho fatto</h3>
                    <ol>
                        <li>Risolto il bug dell'updateQuery "DELETE DATA { GRAPH { .} }; INSERT DATA { GRAPH { .} }". Il bug era nel codice dell'editor, non in quello di oc-ocdm. La ragione del bug era che i caratteri riservati di HTML contenuti nella query di update non erano riportati come entità carattere e venivano interpretati dal browser come markup.</li>
                        <li>Risolto un bug nel KG Editor per cui le proprietà oco:hasUpdateQuery e dc:description venivano visualizzate come link poiché contenevano l'URI base.</li>
                    </ol>
                <h3>Cosa non ho capito</h3>
                    <ol>
                        <li>
                            Il problema della funzione di merge non funzionante è più profondo del previsto. Ricapitolando, il merge deve reindirizzare su self le triple che hanno come oggetto other. È Per farla funzionare occorre modificare il design del preexisting graph così come è stato previsto dalla libreria, ovvero tutte le triple che hanno un'entità come soggetto. Anche aggiungendo al preexisting_graph le triple che hanno l'entità come oggetto, queste ultime vengono rimosse dal preexisting_graph, attraverso il seguente codice:
                            <div class="card bg-primary shadow-inset border-light">
                                <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for p, o in preexisting_graph.predicate_objects(self.res):
            self.g.add((self.res, p, o))
            self.preexisting_graph.add((self.res, p, o))
                                </code></pre>
                            </div>
                            Come si vede, vengono considerate solo le triple che hanno self come soggetto. Perché il merge funzioni, ho modificato il codice nel seguente modo:
                            <div class="card bg-primary shadow-inset border-light">
                                <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for s, p, o in preexisting_graph.triples((None, None, None)):
            self.g.add((s, p, o))
            self.preexisting_graph.add((s, p, o))                                
                                    </code></pre>
                            </div>
                        </li>
                        <li>Infine, ho modificato la funzione di merge, di cui riporto prima la versione attuale e poi quella modificata.
                            <div class="card bg-primary shadow-inset border-light">
                                <pre><code class="prettyprint">
    # Versione attuale
    def merge(self, other: GraphEntity) -> None:
        for res, entity in self.g_set.res_to_entity.items():
            triples_list: List[Tuple] = list(entity.g.triples((res, None, other.res)))
            for triple in triples_list:
                entity.g.remove(triple)
                new_triple = (triple[0], triple[1], self.res)
                entity.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        # 'other' must be deleted AFTER the redirection of
        # triples pointing to it, since mark_as_to_be_deleted
        # also removes every triple pointing to 'other'
        other.mark_as_to_be_deleted()
    
    # Versione modificata
    def merge(self, other: GraphEntity) -> None:
        # Il ciclo non avviene sul grafo di self, ma di other, 
        # poiché è nel grafo di other che sono presenti le triple
        # che hanno other come oggetto. 
        for _, entity in other.g_set.res_to_entity.items():
            # res, entity -> _, entity, poiché res non viene utilizzata
            triples_list: List[Tuple] = list(entity.g.triples((None, None, other.res)))
            # Le triple considerate sono solo quelle che hanno other come oggetto
            for triple in triples_list:
                if triple[1] != URIRef("http://www.w3.org/ns/prov#specializationOf"):
                    # Questa condizione è stata aggiunta per evitare che venga 
                    # associata a self la provenance di other. 
                    # Sarebbe meglio riferirsi a quel predicato 
                    # come PROV.iri_specialization_of, ma non posso importare 
                    # la classe perché si verifica un'importazione circolare

                    # entity.g.remove(triple) è stato rimosso perché lo stesso
                    # compito è assolto da other.mark_as_to_be_deleted() in fondo
                    new_triple = (triple[0], triple[1], self.res)
                    self.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        other.mark_as_to_be_deleted()
                                </code></pre>
                            </div>
                            Dopo queste modifiche, le triple che hanno come oggetto other vengono correttamente rendirizzate su self, eccetto le triple di provenance. Tuttavia, ci sono ancora due bug:
                            <ul>
                                <li>Self viene spogliata di tutti i suoi outgoing links eccetto il tipo.</li>
                                <li>Other viene correttamente spogliata di tutti i suoi outgoing links, ma non è stato possibile cancellare gli incoming links. </li>
                            </ul>
                        </li>
                    </ol>
                <h3>Proposta di modifica a oc-ocdm:</h3>
                <ol>
                    <li>Ho trovato e risolto i bug dei <em>remove_name</em>, <em>remove_given_name</em> e <em>remove_family_name</em> non funzionanti. Per usare un termine tecnico, era una <em>stronzata</em>. Riporto il codice con e senza il bug per quanto riguarda <em>remove_family_name</em>, ma vale lo stesso discorso anche le altre due funzioni:
                        <div class="card bg-primary shadow-inset border-light">
                            <pre><code class="prettyprint">
    # Codice col bug: self.g non è il soggetto
    def remove_family_name(self) -> None:
        self.g.remove((self.g, GraphEntity.iri_family_name, None))
    
    # Codice fixato: self.g -> self.res
    def remove_family_name(self) -> None:
        self.g.remove((self.res, GraphEntity.iri_family_name, None))
                            </code></pre>
                        </div>
                    </li>
                </ol>
            </div>
        </section>

        <section class="section pb-5" id="past-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Archivio</h2>
                </div>
                <div class="accordion shadow-soft rounded" id="accordionExample1">
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-1" data-target="#panel-1" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-1">
                            <span class="h6 mb-0 font-weight-bold">10/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-1">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                        <ol>
                                            <li>Peroni, S., Shotton, D., Vitali, F. (2016). <em>A document-inspired way for
                                                    tracking changes
                                                    of RDF data</em>. <a
                                                    href="https://w3id.org/oc/paper/occ-driftalod2016.html"
                                                    alt="Link to the paper">https://w3id.org/oc/paper/occ-driftalod2016.html</a>.
                                            </li>
                                            <li>Daquino, M., Peroni, S., Shotton D. (2020). <em>The OpenCitations Data
                                                    Model</em>. <a
                                                    href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                                    alt="Link to the OpenCitations Data Model documentation">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>
                                            </li>
                                            <li>Documentazione delle seguenti librerie e API: <em>Crossref REST API</em>,
                                                <em>REST API for
                                                    COCI</em>, <em>oc_ocdm</em>.
                                            </li>
                                            <li>Ho sfogliato il lavoro fin qui svolto da Arianna Moretti.</li>
                                        </ol>
                                    </li>
                                    <li>Ho scritto il seguente codice con lo scopo di comprendere il funzionamento di
                                        tutte le librerie e API su menzionate, nonché di produrre output utili in vista
                                        della realizzazione del dataset su "Scientometrics": <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/scientometrics.py"
                                            target="_blank" alt="Link to scientometrics.py">scientometrics.py</a>.</li>
                                    <div class="col-12 mt-3 mb-4">
                                        <div class="card bg-primary shadow-inset border-light">
                                            <div class="card-body p-5">
                                                <pre>
                                                    <code class="prettyprint">
import requests, requests_cache, json
from oc_ocdm.graph import GraphSet
from oc_ocdm.storer import Storer
from oc_ocdm.support import create_date
from rdflib import URIRef


def get_journal_data(journal_issn, i_am_polite):
    journal_data = requests.get(url = f'http://api.crossref.org/journals/{{{journal_issn}}}/works?mailto={i_am_polite}')
    journal_data_json = journal_data.json()
    return journal_data_json


def get_all_references_from_journal(journal_data_json):
    journal_data_items = journal_data_json["message"]["items"]
    all_references = list()
    for item in journal_data_items:
        references = requests.get(url = f'https://w3id.org/oc/index/coci/api/v1/references/{item["DOI"]}?format=json')
        references_json = references.json()
        all_references.append(references_json)
    return all_references


def update_graph(journal_data_json, graphset):
    journal_data_items = journal_data_json["message"]["items"]
    for item in journal_data_items:
        try:
            # a volte gli articoli ritornati da crossref non hanno il campo "author". È molto raro, ma accade.
            responsible_agent_name = item["author"][0]["given"] + " " + item["author"][0]["family"]
            responsible_agent = scientometrics_graphset.add_ra(responsible_agent_name)
            responsible_agent.has_given_name(item["author"][0]["given"])
            responsible_agent.has_family_name(item["author"][0]["family"])
            # non ho ancora gestito il problema dell'omonimia, ma sono consapevole che vada gestito
            scientometrics_br = scientometrics_graphset.add_br(responsible_agent)
            scientometrics_br.has_title(item["title"][0])
            iso_date_string = create_date([item["published-print"]["date-parts"][0][0]])
            scientometrics_br.has_pub_date(iso_date_string)
        except KeyError:
            pass
    graphset.commit_changes()


# Crossref test
requests_cache.install_cache('cache')
issn_web_scientometrics = "1588-2861"
my_mail = "arcangelo.massari@studio.unibo.it"
journal_data = get_journal_data(issn_web_scientometrics, my_mail)

# REST API for COCI test
all_references = get_all_references_from_journal(journal_data)

# oc_ocdm test
scientometrics_graphset = GraphSet("https://arcangelo7.github.io/time_agnostic/")
update_graph(journal_data, scientometrics_graphset)

# Retrieved data dump
with open('data/scientometrics.json', 'w') as outfile:
    json.dump(journal_data, outfile)
with open('data/references.json', 'w') as outfile:
    json.dump(all_references, outfile)
storer = Storer(scientometrics_graphset)
storer.store_graphs_in_file("data/graph.json", "./")                      
                                                    </code>
                                                </pre>
                                            </div>
                                        </div>
                                    </div>
                                    <li>Ho ottenuto in output tre file json:</li>
                                    <ol>
                                        <li>Una lista di 20 lavori (articoli, libri, atti di convegni, etc) presenti
                                            nella rivista "Scientometrics", con relativi metadati: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/scientometrics.json"
                                                alt="Link to scientometrics.json"
                                                target="_blank">scientometrics.json</a>.</li>
                                        <li>I dati citazionali per tutti i riferimenti in uscita relativi ai DOI di 20
                                            lavori presenti nella rivista "Scientometrics": <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/references.json"
                                                target="_blank" alt="Link to references.json">references.json</a>.</li>
                                        <li>Una lista di grafi conformi a OCDM v2.0.1 contenente 20 entità di tipo
                                            BibliographicResource relative a 20 lavori presenti in "Scientometrics", con
                                            metadati relativi a titolo e data di pubblicazione: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/graph.json"
                                                target="_blank" alt="Link to the graph">graph.json</a>.</li>
                                    </ol>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non posso accedere a <em>The OpenCitations Data Model</em> su SpringerLink (<a
                                            href="https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28"
                                            alt="Link to the OpenCitations Data Model docuemntation on Springer"
                                            target="_blank">https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28</a>)
                                        neanche tramite proxy Unibo. È diverso rispetto a quello pubblicato su figshare?
                                        (<a href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                            alt="Link to the OpenCitations Data Model documentation on Figshare">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>)
                                    </li>
                                    <li>Gli articoli di "Scientometrics" ritornati tramite GET a
                                        http://api.crossref.org/journals/{1588-2861}/works, dove 1588-2861 è l'ISSN web
                                        di "Scientometrics", sono 20 alla volta. Tramite il parametro rows è possibile
                                        aumentare questo numero a massimo 1000. Come faccio a ritornarli tutti?</li>
                                    <li>A quale ISSN di "Scientometrics" devo fare riferimento, quello web (1588-2861),
                                        quello stampa (0138-9130) o entrambi?</li>
                                    <li>Per creare una nuova entità di qualunque tipo, sia essa una Bibliographic
                                        Resource o una Citation, occorre passare in input al metodo corrispondente il
                                        responsible agent. Ad esempio, <code
                                            class="prettyprint">scientometrics_graphset.add_br(responsible_agent)</code>.
                                        Dato che l'OCDM prevede il Responsible agent come tipo, credo ci si riferisca ad
                                        esso. Su questo ho due dubbi:
                                        <ol>
                                            <li>Nel grafo finale non vedo il collegamento tra i Responsible Agents e le
                                                corrispondenti Bibliographic Resources. Entrambi vengono indicati da un
                                                URL nella forma [dataset URL][entity dataset identifier] - ad esempio
                                                https://arcangelo7.github.io/time_agnostic/ra/6 - ma il grafo non
                                                riporta qual è la Bibliographic Resource corrispondente. Fa fede il
                                                numero? Ovvero il Responsible Agent 6 è responsabile della Bibliographic
                                                Resource 6?</li>
                                            <li>Di alcuni autori viene riportato l'ORCID, di altri no. Suggerimenti su
                                                come gestire le omonimie?</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-2" data-target="#panel-2" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-2">
                            <span class="h6 mb-0 font-weight-bold">17/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-2">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho riorganizzato il codice utilizzando la programmazione a oggetti per rendere
                                        lo sviluppo più mantenibile.</li>
                                    <li>Utilizzando il parametro <em>cursor</em>, ho aumentato da 20 a 6065 il numero di
                                        works contenuti in "Scientometrics" ritornati tramite Crossref. Ora ci sono
                                        tutti.</li>
                                    <li>Ho generato 3 output corrispondenti a 3 grafi, di cui si elencano i rispettivi
                                        metadati considerati:
                                        <ol>
                                            <li>MetadataSet (dcat:Dataset)
                                            <ol>
                                                <li>Titolo (dcterms:title)</li>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>Data di modifica (dcterms:modified)</li>
                                            </ol></li>
                                            <li>Provenance (prov:Entity)
                                            <ol>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>is snapshot of (prov:specializationOf)</li>
                                                <li>Attribuzione (prov:wasAttributedTo)</li>
                                                <li>Data di creazione (prov:generatedAtTime)</li>
                                            </ol></li>
                                            <li>Graphset (set di grafi sulle entità bibliografiche)
                                            <ol>
                                                <li>BibliographicResource, per la rivista, i suoi articoli e gli articoli
                                                    citati dai suoi articoli (fabio:Expression, fabio:Journal,
                                                    fabio:JournalArticle)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Titolo (dcterms:title)</li>
                                                    <li>Sottotitolo, laddove presente (fabio:hasSubtitle)</li>
                                                    <li>Identificatore, DOI per gli articoli, ISSN per la rivista
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Se articolo, autore (pro:isDocumentContextFor)</li>
                                                    <li>Se articolo, data di pubblicazione, laddove presente
                                                        (prism:publicationDate)</li>
                                                    <li>Se articolo, rivista di appartenenza (frbr:partOf)</li>
                                                    <li>Formato (frbr:embodiment)</li>
                                                </ol></li>
                                                <li>Citation (cito:Citation, cito:JournalSelfCitation,
                                                    cito:AuthorSelfCitation, cito:DistantCitation)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Documento citante (cito:hasCitingEntity)</li>
                                                    <li>Documento citato (cito:hasCitedEntity)</li>
                                                    <li>Data di creazione (cito:hasCitationCreationDate)</li>
                                                    <li>Time span (cito:hasCitationTimeSpan)</li>
                                                </ol></li>
                                                <li>ResourceEmbodiment (fabio:manifestation, fabio:DigitalManifestation,
                                                    fabio:PrintObject)
                                                <ol>
                                                    <li>Tipo, laddove presente (rdf:type)</li>
                                                    <li>Media type, laddove presente (dcterms:format)</li>
                                                    <li>Pagina iniziale, laddove presente (prism:startingPage)</li>
                                                    <li>Pagina finale, laddove presente (prism:endingPage)</li>
                                                    <li>URL, laddove presente (frbr:exemplar)</li>
                                                </ol></li>
                                                <li>ResponsibleAgent (foaf:Agent)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Identificativo, ovvero ORCID, laddove presente
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Nome, laddove presente (foaf:givenName)</li>
                                                    <li>Cognome, laddove presente (foaf:familyName)</li>
                                                    <li>Nome completo, laddove presente (foaf:name)</li>
                                                </ol></li>
                                                <li>AgentRole (pro:RoleInTime)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di ruolo (pro:withRole)</li>
                                                    <li>Is held by (pro:isHeldBy)</li>
                                                </ol></li>
                                                <li>Identifier (datacite:Identifier)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di identificatore (datacite:usesIdentifierScheme)</li>
                                                    <li>Stringa (literalreification:hasLiteralValue)</li>
                                                </ol></li>
                                            </ol></li>
                                        </ol>
                                    </li>
                                    <li>La mappatura è avvenuta nel modo più generico e rivista-indipendente possibile.
                                        Dovrebbe essere possibile utilizzare lo stesso software per mappare seguendo
                                        l'ocdm qualunque rivista ritornata da Crossref.</li>
                                    <li>È stata implementata una barra di caricamento per mostrare all'utente quanto
                                        occorra aspettare per ricevere tutti i dati tramite le varie API.</li>
                                    <li>Il codice è disponibile alla seguente repository: <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py"
                                            target=_blank
                                            alt="Link to the Python script">https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py</a>.
                                        Purtroppo, non è stato possibile caricare gli output su GitHub date le loro
                                        dimensioni.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Come faccio a esprimere che un certo issue fa parte di un certo volume
                                        utilizzando l'OCDM? Una BibliographicResource ha come metodo has_number, che
                                        però è generico.</li>
                                    <li>Un AgentRole ha tra i suoi metodi has_next. A questo proposito, la
                                        documentazione di oc_ocdm dice:
                                        <blockquote class="blockquote ml-5 mt-3">The previous role in a sequence of
                                            agent
                                            roles of the same type associated with the same bibliographic resource (so
                                            as to define, for instance, an ordered list of authors).</blockquote>
                                        Tuttavia, il documento <em>The Open Citations Data Model</em> parla di
                                        "following role", non di "previous". Credo faccia quindi fede quest'ultimo. In
                                        ogni caso, non ho capito a cosa ci si sta riferendo, per ragioni affini a quelle
                                        della domanda successiva.
                                    </li>
                                    <li>Come faccio a collegare una BibliographicResource ai suoi ResponsibleAgents? È
                                        presente il metodo has_contributor, che però riceve come argomento un AgentRole,
                                        non un ResponsibleAgent. Devo quindi avere tanti AgentRoles quanti sono i
                                        ResponsibleAgents e usarli come intermediari tra la BibliographicResource e i
                                        ResponsibleAgents? Sembrerebbe di sì, dato che tale predicato viene mappato con
                                        pro:isDocumentContextFor, che presenta la stessa contorsione logica: è come se
                                        invece di dire "<em>The Open Citations Data Model</em> è scritto da Silvio
                                        Peroni" dicessi "<em>The Open Citations Data Model</em> è il contesto nel quale
                                        il ruolo di autore di Silvio Peroni si manifesta". </li>
                                    <li>Alcuni articoli di rivista hanno sia una data di pubblicazione a stampa, che una
                                        data per la pubblicazione digitale. Tuttavia, OCDM prevede un'unica data di
                                        pubblicazione attraverso il metodo has_pub_date. Quale uso delle due? Oltre a
                                        questo, come tipo di ResourceEmbodiment devo indicare sia il formato stampa che
                                        il formato digitale?</li>
                                    <li>Tra i predicati di una Citation c'è has_citation_characterization. A questo
                                        proposito, la documentazione dell'OCDM recita:
                                        <blockquote class="blockquote ml-5 mt-3">The citation function characterizing
                                            the
                                            purpose of the citation.</blockquote>
                                        Tuttavia, non ho capito cosa voglia dire.
                                    </li>
                                    <li>Ogni articolo di Scientometrics cita altri articoli sia della rivista stessa che
                                        di riviste esterne. Devo creare una BibliograficResource per ciascuna degli
                                        articoli esterni a Scientometrics?</li>
                                    <li>Crossref riporta le reference di ciascun articolo indicando, tra gli altri, la
                                        "key". Che cosa indica? Ad esempio:
                                        <pre>
                                            <code class="prettyprint">
{
    "DOI": "10.1126/science.280.5364.698",
    "author": "M. Heller",
    "doi-asserted-by": "crossref",
    "first-page": "698",
    "journal-title": "Science",
    <strong>"key"</strong>: "207_CR21",
    "unstructured": "Heller, M., H. Eisenberg (1998), Can Patents Deter Innovation? The Anticommons in Biomedical Research, Science, 280: 698\u2013701.",
    "volume": "280",
    "year": "1998"
}
                                            </code>
                                        </pre>
                                    </li>
                                    <li>I dati sulle reference riportati da Crossref e dall'API for COCI sono spesso
                                        uguali, ma non sempre: a volte il COCI riporta citazioni che Crossref non
                                        riporta e viceversa. Quale dei due fa fede? Entrambi?</li>
                                    <li>A causa dell'elevato numero di articoli, i tempi di risposta di Crossref e
                                        dell'API for COCI sono biblici, si parla di ore. Per questo motivo, una volta
                                        ottenuti i dati la prima volta, li ho messi in cache per non doverli più
                                        richiedere. Inoltre, attraverso un dizionario, ho evitato di chiedere due volte
                                        metadati già richiesti al COCI. Oltre a queste misure, c'è qualcosa che non so e
                                        che dovrei fare per rendere i tempi di esecuzione accettabili?</li>
                                    <li>Ammettendo che le esecuzioni dei GET a Crossref e COCI siano istantanee, la
                                        creazione del grafo vero e proprio avendo i dati occupa il 95% di 16 GB di RAM
                                        DDR4 a 3000 MHz e dura ugualmente uno sproposito (per rendere l'idea, aprire
                                        qualunque altro programma che non sia Python causa il crash dello script per out
                                        of memory). Inoltre, volendo esportare il risultato in json, questo pesa oltre
                                        400 MB. Non è quindi caricabile su GitHub, neanche usando Git Large File
                                        Storage, perché andrei velocemente oltre il GB mensile gratuito consentito. Con
                                        questa mole di dati diventa insomma veramente complicato lavorare. Consigli?
                                    </li>
                                    <li>Idealmente, mi sarebbe piaciuto creare un software che, preso in input un ISSN,
                                        ritornasse in output il grafo relativo a quella rivista rispettando l'OCDM.
                                        Tuttavia, a causa dei lunghi tempi d'attesa, e quindi del rischio di crash lungo
                                        il percorso, sono stato costretto a spezzettare il processo su più funzioni da
                                        chiamare manualmente. È possibile realizzare l'ideale?</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-3" data-target="#panel-3" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-3" id="d2303cnhc7p">
                            <span class="h6 mb-0 font-weight-bold">23/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-3">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                    <ol>
                                        <li>Peroni, S., Shotton, D., Vitali, F. (2012). Scholarly publishing and Linked
                                            Data: describing roles, statuses, temporal and contextual extents.
                                            Association for Computing Machinery, New York. Retrieved from <a
                                                href="https://doi.org/10.1145/2362499.2362502" alt="Link to the article"
                                                target="_blank">https://doi.org/10.1145/2362499.2362502</a>.</li>
                                        <ol>
                                            <li>Vi ho appreso con entusiasmo che dietro l'entità apparentemente contorta
                                                pro:RoleInTime si cela invece la possibilità di definire feature tempo e
                                                contesto-dipendenti e di effettuare query più profonde.</li>
                                        </ol>
                                        <li>Peroni, S., Shotton, D. (2012). FaBiO and CiTO: Ontologies for describing
                                            bibliographic resources and citations. Journal of Web Semantics, Volume 17,
                                            Pages 33-43.
                                            Retrieved from <a href="https://doi.org/10.1016/j.websem.2012.08.001"
                                                alt="Link to the article"
                                                target="_blank">https://doi.org/10.1016/j.websem.2012.08.001</a>.</li>
                                        <ol>
                                            <li>Ne ho tratto informazioni sulla genesi di FaBio E CiTO e in particolare
                                                sul valore del predicato cito:hasCitationCharacterization.</li>
                                        </ol>
                                        <li>Documentazione di Blazegraph.</li>
                                    </ol></li>
                                    <li>Il Graphset contiene adesso informazioni sui volumi e sugli issues,
                                        rappresentati come BibliographicResources rispettivamente di tipo JournalVolume
                                        e JournalIssue, aventi come predicati fabio:hasSequenceIdentifier e frbr:partOf.
                                    </li>
                                    <li>Gli AgentRoles (pro:RoleInTime) presentano adesso il predicato oco:hasNext,
                                        avente come oggetto il ruolo successivo in una sequenza di agent roles dello
                                        stesso tipo associati alla stessa risorsa bibliografica, allo scopo di definire,
                                        ad esempio, una lista ordinata di autori.</li>
                                    <li>La data di pubblicazione considerata (prism:publicationDate) è adesso quella
                                        contenuta nel campo "issued". Sono stati inoltre creati tanti ResourceEmbodiment
                                        quante sono le manifestazioni, indicando la pagina iniziale e finale per i
                                        formati a stampa, il media-type e l'URL per i formati digitali.</li>
                                    <li>Sono state aggiunte tante entità di tipo BibliographicReference quante sono le
                                        reference riportate da Crossref con un DOI. Il loro contenuto, proveniente dal
                                        campo "unstructured", è stato riportato tramite c40:hasContent. Ciascuna
                                        BibliographicReference è stata collegata alla rispettiva BibliographicResource
                                        tramite biro:references. Allo stesso tempo, ogni BiblographicResource è stata
                                        collegata alle rispettive BibliographicReferences tramie frbr:part.</li>
                                    <li>È stata create una nuova classe, DatasetAutoEnhancer, pensata per contenere
                                        metodi e proprietà che migliorino la qualità del dataset in maniera automatica.
                                        Al momento presenta:
                                    <ol>
                                        <li>il metodo merge_by_id, il quale unisce due entità nel caso in cui gli
                                            identificativi ad esse associate coincidano. Nella fattispecie, unisce due
                                            BibliographicResources con lo stesso DOI e due ResponsibleAgents con lo
                                            stesso ORCID.</li>
                                        <li>Il metodo privato _generate_snaphot, che si occupa di aggiornare il grafo
                                            sulla provenance registrando i delta rispetto a quello precedente, in
                                            particolare:
                                        <ol>
                                            <li>Crea lo snaphsot.</li>
                                            <li>Definisce la data di creazione (has_generation_time).</li>
                                            <li>Definisce di quale entità è lo snaphot (is_snapshot_of).</li>
                                            <li>Definisce l'azione di update effettuata tramite una SPARQL query
                                                (has_update_action). La SPARQL query è stata a sua volta generata
                                                tramite il metodo get_update_query del modulo support.query_utils di
                                                oc_ocdm.</li>
                                            <li>Associa allo snapshot precedente la data di invalidazione
                                                (has_invalidation_time).</li>
                                            <li>Collega lo snapshot a quello precedente (derives_from).</li>
                                            <li>Definisce la risorsa primaria da cui hanno origine i metadati nello
                                                snapshot, ovvero Crossref (has_primary_source).</li>
                                            <li>Definisce l'agente responsabile delle modifiche (has_resp_agent).</li>
                                        </ol></li>
                                    </ol></li>
                                    <li>È stata implementata una nuova classe, Support, che contiene i seguenti metodi:
                                    
                                    <ol>
                                        <li>zip_data, per comprimere i dati in un archivio.</li>
                                        <li>minify_json, per minificare i file json in modo che siano parsabili dal
                                            metodo load della classe Reader di oc_ocdm.</li>
                                        <li>measure_runtime, per misurare i tempi di esecuzione delle varie funzioni.
                                        </li>
                                        <li>dump_dataset, per conservare i dataset in un file json.</li>
                                        <li>dump_json, per salvare gli oggetti json in un file.</li>
                                    </ol>
                                </ol></li>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non mi è mai capitato per "Scientometrics", ma come mi comporto se viene
                                        indicato l'issue e non il volume? Creo un entità Volume vuota e le collego
                                        l'issue?</li>
                                    <li>
                                        <p>Con il seguente codice ho espresso la relazione oco:hasNext per tutti gli
                                            AgentRoles (riporto solo le parti del codice rilevanti)</p>
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <pre>
                                                <code class="prettyprint">
author_agent_roles = list()
for author in item["author"]:
        # [...]
        author_ar = journal_graphset.add_ar(your_orcid)
        author_ar.create_author()
        author_ar.is_held_by(item_ra)
        item_br.has_contributor(author_ar)
        author_agent_roles.append(author_ar)
for index, author_agent_role in enumerate(author_agent_roles):
        if index+1 &lt; len(author_agent_roles):
                author_agent_role.has_next(author_agent_roles[index+1])
                                                </code>
                                            </pre>
                                        </div>
                                        A livello di AgentRole, il grafo finale riporta correttamente il successivo
                                        AgentRole. Tuttavia, guardando le proprietà pro:isDocumentContextFor delle
                                        BibliographicResources, la lista risulta disordinata. Sto sbagliando io o è un
                                        bug della libreria?
                                    </li>
                                    <li>Nel caso in cui per un articolo venga riportata una sola pagina e non un
                                        intervallo di pagine, è corretto indicare quella pagina sia come pagina iniziale
                                        che come pagina finale? Perché OCDM prevede solo i predicati per gli intervalli
                                        di pagine e non per le pagine singole.</li>
                                    <li>Una BibliographiResource ha tra i suoi predicati has_edition. La documentazione
                                        riporta:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An identifier for one of several alternative editions of a particular
                                            bibliographic resource.
                                        </blockquote>
                                        Non ho capito a cosa ci si riferisca.
                                    </li>
                                    <li>Sono riuscito ad avviare il NanoSparqlServer di Blazegraph utilizzando il
                                        comando: <code
                                            class="prettyprint">java -server -Xmx4g -jar blazegraph.jar</code>.
                                        In questo modo, la workbench è diventata disponibile all'indirizzo
                                        http://localhost:9999/blazegraph/.
                                        Tuttavia, non ho capito cosa passare come triplestore_url al metodo upload_all
                                        della classe Storer di oc_ocdm. Ho provato a passare sia
                                        http://localhost:9999/blazegraph/ che http://localhost:9999/bigdata/sparql e
                                        l'output del metodo è stato in entrambi i casi True, il che sembrerebbe
                                        incoraggiante, ma attraverso l'endpoint SPARQL della workbench mi sono accorto
                                        di non aver caricato alcuna tripla. A quale URL devo fare il POST?
                                    </li>
                                    <li>Allo scopo di aumentare la provenance, ho inizialmente provato ad aprire il
                                        grafo precedentemente creato utilizzando i metodi load e
                                        import_entities_from_graph della classe Reader di oc_ocdm. Tuttavia, il secondo
                                        metodo ritorna una lista di GraphEntity, che non mi permette di utilizzare
                                        metodi propri della classe GraphSet, come get_id() o get_br(), costringendomi a
                                        ciclare su tutti gli elementi della lista e controllare manualmente di che
                                        classe sono istanza. Inoltre, per qualche motivo che ignoro, il metodo
                                        import_entities_from_graph richiede notevolmente più tempo che creare il grafo
                                        da zero. Ho quindi deciso di aumentare la provenance direttamente durante la
                                        creazione del GraphSet, ovvero tramite più commit successivi. È corretto?</li>
                                    <li id="d2303cnhc7">Per quanto riguarda la creazione di uno snapshot, quanto deve essere esplicitato
                                        e quanto viene fatto in automatico? Mi spiego meglio: quando unisco un'entità a
                                        un'altra tramite il metodo merge, tutto ciò che ruota attorno a quelle entità
                                        rimane invariato. Ad esempio, se unisco due risorse bibliografiche dopo aver
                                        scoperto che hanno lo stesso DOI, le rispettive entità di tipo Identifier non
                                        vengono unite a loro volta. Confermi che io lo debba fare manualmente? Allo
                                        stesso tempo, la risorsa che è scomparsa dopo essere stata unita continua a
                                        essere richiamata in altre parti del grafo anche se non esiste più.</li>
                                    <li>Quando faccio un merge devo generare uno snapshost sia per la risorsa unita che
                                        per quella contenitore? Perché ho notato che così facendo si creano snapshot
                                        duplicati.</li>
                                    <li>Tra i metadati di uno snapshot c'è la sua descrizione. Per ottenerla devo
                                        parsare la stringa con la SPARQL query. Posso certamente farlo io da zero, ma
                                        non vorrei reinventare la ruota. Esistono delle librerie per farlo? Ne ho
                                        trovate alcune ma mi sembrano molto amatoriali.</li>
                                    <li>
                                        Allo scopo di ottenere i DOI a partire dal contenuto delle
                                        BibliographicReferences - ovvero il campo "unstructured" riportato da Crossref -
                                        ho provato a effetturare delle query su Crossref. Per fare un esempio:
                                        <pre class="mt-3">
            <code class="prettyprint">https://api.crossref.org/works?query.bibliographic=Carberry%2C+Josiah.+%E2%80%9CToward+a+Unified+Theory+of+High-Energy+Metaphysics%3A+Silly+String+Theory.%E2%80%9D+Journal+of+Psychoceramics+5.11+%282008%29%3A+1-3.</code>
                                        </pre>
                                        Dopodiché, ho controllato che il campo "score" contenesse un numero maggiore di
                                        90 e, se sì, ho estratto il DOI dall'item. A questo proposito ho tre domande:
                                    
                                    <ol>
                                        <li>Come funziona lo score? Perché ho notato che a volte risultati con score
                                            superiore a 100 sono sbagliati.</li>
                                        <li>La query non ritorna solo il risultato migliore, ma un numero variabile di
                                            hits più o meno pertinenti. Come faccio a ottimizzare la ricerca perché
                                            ritorni solo il risultato più pertinente? Ho provato con il parametro
                                            enable-multiple-hits impostato a false (peraltro di default), ma senza
                                            successo.</li>
                                        <li>È questo il metodo migliore per ottenere un DOI da una reference non
                                            strutturata?</li>
                                    </ol></li>
                                    <li>Persiste il problema della memoria. Ho provato ad aggiungere due banchetti che
                                        avevo in un altro PC e portare la RAM a 32 GB, ho attivato l'X.M.P., l'ho
                                        overclockata e ho portato il file di paging a 8 GB, ma non è bastato, il
                                        processo di creazione ed esportazione dei grafi l'ha nuovamente saturata.
                                        Riporto uno screen del messaggio di errore:
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/memory_error.png" alt="memory error image">
                                        </div>
                                        Utilizzando Jupyer Notebook e il monitoraggio risorse, ho potuto eseguire i vari
                                        passaggi singolarmente, per misurare quanta RAM occorresse a ciascuno:
                                        <ol>
                                            <li>
                                                <p>Creare il graphset ha occupato circa 18.4 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_graphset.png"
                                                        alt="RAM left after GraphSet creation">
                                                </div>
                                            </li>
                                            <li>
                                                <p>Creare il grafo sulla provenance ha portato la memoria occupata a
                                                    circa 26.8 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_prov.png"
                                                        alt="RAM left after Provenance creation">
                                                </div>
                                            </li>
                                            <li>Infine, il dump del dataset sulla provenance ha fatto crashare il
                                                programma.</li>
                                        </ol>
                                        Sono infine riuscito a risolvere portando il file di paging a 16 GB, ma in tutta
                                        onestà non la ritengo una soluzione valida, né filosoficamente né guardando al
                                        futuro prossimo in cui i dati saranno molti di più. Cosa ne pensi?
                                    </li>
                                    <li>Durante l'ultimo incontro hai fatto due riferimenti che sono rimasti in sospeso:
                                        <ol>
                                            <li>A proposito di OpenCitations Meta, hai menzionato un articolo che
                                                introduce a OpenCitations. Ho verificato di non averlo letto. Potresti
                                                inviarmelo?</li>
                                            <li>Hai menzionato alcune questioni di cui discutere dopo la risposta alle
                                                domande, che però non sono più state discusse.</li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Note</h3>
                                <p>La documentazione del metodo import_entities_from_graph della classe Reader di
                                    oc_ocdm non è aggiornata. Il metodo vuole infatti tre argomenti obbligatori, non
                                    due: il GraphSet, il Graph e il responsible agent. Quest'ultimo non viene menzionato
                                    dalla documentazione.</p>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-4" data-target="#panel-4" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-4">
                            <span class="h6 mb-0 font-weight-bold">31/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-4">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho caricato dati e provenance su Blazegraph.</li>
                                    <li>Ho implementato:
                                        <ol>
                                            <li>un sistema di gestione delle richieste tramite api più raffinato, con un timeout, un numero
                                                massimo di tentativi separati da intervalli progressivamente crescenti tramite backoff factor e
                                                con la possibilità di salvare gli errori in un file di log;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di verificare l'esistenza di
                                                entità associate allo stesso id. Nel caso le trovi, recupera i grafi associati e li unisce;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di ottenere tutti i DOI degli
                                                articoli appartenenti a una determinata rivista e i DOI di tutti gli articoli da essi citati.
                                                Dopodiché, controlla se su COCI sono presenti altri DOI di articoli citati e, se li trova,
                                                recupera il grafo dell'entità citante e gli aggancia le varie entità relative alla risorsa
                                                citata, ovvero l'Identifier, la BibliographicResource e la Citation (data di creazione,
                                                timespan, journal o author self-citation). Per quanto riguarda i DOI degli articoli citati già
                                                presenti sul triplestore, vengono aggiunte alle rispettive entità Citation il timespan e se sono
                                                autocitazioni della rivista o dell'autore;</li>
                                            <li>una gestione più sofisticata di volume, issue e articoli:
                                                <ol>
                                                    <li>se ci sono sia volume che issue l'articolo fa parte dell'issue, l'issue del volume e il
                                                        volume della rivista;</li>
                                                    <li>se c'è solo il volume, l'articolo fa parte del volume e il volume della rivista;</li>
                                                    <li>se c'è solo l'issue, l'articolo fa parte dell'issue e l'issue della rivista;</li>
                                                    <li>se non ci sono né issue né volume l'articolo fa parte della rivista. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho aperto i seguenti issue:
                                        <ol>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/4" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/4</a></li>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/5" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/5</a></li>
                                        </ol>
                                    </li>
                                    <li>Ho letto:
                                        <ol>
                                            <li>International DOI Foundation. (2019). DOI® Handbook. https://doi.org/10.1000/182.
                                                <ol>
                                                    <li>Sì, ho seriamente letto tutto l'handbook.</li>
                                                </ol>
                                            </li>
                                            <li>Fecher, B., & Friesike, S. (2014). Open Science: One Term, Five Schools of Thought. In S.
                                                Bartling & S. Friesike (Eds.), Opening Science (pp. 17–47). Springer International Publishing.
                                                https://doi.org/10.1007/978-3-319-00026-8_2.</li>
                                            <li>Kramer, B., & Bosman, J. (2015, June 18). The good, the efficient and the open—Changing research
                                                workflows and the need to move from Open Access to Open Science. CERN Workshop on Innovations in
                                                Scholarly Communication (OAI9), University of Geneva, Geneva, Switzerland.
                                                https://www.slideshare.net/bmkramer/the-good-the-efficient-and-the-open-oai9.</li>
                                            <li>Woelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature
                                                Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149.</li>
                                            <li>UNESCO. (2020). First draft of the UNESCO Recommendation on Open Science (Programme and Meeting
                                                Document SC-PCB-SPP/2020/OS/R1; p. 16). https://unesdoc.unesco.org/ark:/48223/pf0000374837.</li>
                                            <li>Documentazione di rdflib.</li>
                                        </ol>
                                    </li>
                                    <li>Ho seguito il corso Semantic Web Technologies del Prof. Harald Sack, reperibile all'indirizzo <a
                                            href="https://open.hpi.de/courses/semanticweb" target="_blank"
                                            alt="Link to the Semantic Web course">https://open.hpi.de/courses/semanticweb</a>, al fine di
                                        approfondire la mia conoscenza di SPARQL 1.1.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Allo scopo di ottenere tutte le triple di cui un'entità è soggetto da un triplestore, oc_ocdm fornisce
                                        il metodo import_entity_from_triplestore della classe Reader. Ecco un esempio di come ho provato a
                                        utilizzarla:
                                        <pre><code class="prettyprint">
g = GraphSet("https://github.com/arcangelo7/time_agnostic/")
qres = Reader().import_entity_from_triplestore(g, "http://localhost:9999/blazegraph/sparql", URIRef("https://github.com/arcangelo7/time_agnostic/br/1"),"https://orcid.org/0000-0002-8420-0696")
                                                            </code></pre>
                                        Nonostante l'entità esista ottengo però un ValueError, con messaggio di errore: "The required entity was
                                        not found or was not recognised as a proper OCDM entity". Dico che l'entità esiste perché facendo il
                                        medesimo CONSTRUCT tramite endpoint SPARQL ottengo i risultati attesi. Sto utilizzando correttamente il
                                        metodo? Ho infine risolto il problema utilizzando la classe SPARQLStore di rdflib o SPARQLWrapper, ma
                                        visto che oc_ocdm fornisce già uno shortcut preferirei utilizzare quello.
                                    </li>
                                    <li>La documentazione della classe SPARQLStore di rdflib, reperibile a <a
                                            href="https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.plugins.stores.html#rdflib.plugins.stores.sparqlstore.SPARQLStore"
                                            target="_blank" alt="SPARQLStore class documentation">questo indirizzo</a>, afferma:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An RDFLib store around a SPARQL endpoint
                                            This is context-aware and should work as expected
                                            when a context is specified.
                                            For ConjunctiveGraphs, reading is done from the "default graph". Exactly
                                            what this means depends on your endpoint, because SPARQL does not offer a
                                            simple way to query the union of all graphs as it would be expected for a
                                            ConjuntiveGraph. This is why we recommend using Dataset instead, which is
                                            motivated by the SPARQL 1.1.
                                        </blockquote>
                                        Non ho capito cosa intenda dire.
                                    </li>
                                    <li>Dopo avere unito le entità associate agli stessi id è necessario caricare le modifiche sul triplestore.
                                        Dal codice di oc_ocdm ho inteso che la libreria si occupa di elaborare la query di update, definendo
                                        cosa vada cancellato e cosa aggiunto, ma dai tentativi fatti mi continuano a risultare 0 modifiche, per
                                        cui la query non viene lanciata. Sospetto c'entri qualcosa il metodo commit_changes(), il cui
                                        funzionamento mi risulta però oscuro.</li>
                                    <li>Non sono riuscito ad aggiornare la provenance a partire da un grafo di provenance preesistente.</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-5" data-target="#panel-5" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-5" id="d0704cnhc1p">
                            <span class="h6 mb-0 font-weight-bold">07/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-5">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Le query SPARQL precedentemente effettuate tramite SPARQLStore utilizzano adesso SPARQLWrapper.</li>
                                    <li>Grafi di provenance e grafi sui dati vengono adesso generati in base alla storia pregressa tramite
                                        un
                                        indice.</li>
                                    <li>Effettuati i miglioramenti automatici, le nuove triple e loro provenance vengono adesso caricate
                                        correttamente sul triplestore.</li>
                                    <li>Grazie alla spiegazione fornitami da Simone, sono riuscito a importare correttamente un grafo da
                                        file
                                        json tramite metodo import_entities_from_graph della classe Reader di oc_ocdm.</li>
                                    <li>Sono state implementate due nuove funzioni, merge_by_id_from_file e add_coci_data_from_file, che
                                        anziché
                                        lavorare a partire da un triplestore lavorano a partire da un file. Di conseguenza, è possibile
                                        ottenere
                                        in output un nuovo file modificato anziché un upload su un triplestore. In questo modo potrò sia
                                        avere
                                        un file di backup che inviare a Cristian dei file rdf customizzati in base alle sue esigenze.</li>
                                    <li>Cristian mi ha fatto notare che avevo considerato solo gli autori come AgentRole. Ora il grafo
                                        iniziale
                                        comprende anche i publisher.</li>
                                    <li>Risolti alcuni bug nella funzione add_coci_data_from_triplestore:
                                        <ol>
                                            <li>Le entità di tipo cito:Citation preesistenti venivano prima ricercate solo tramite il DOI
                                                name
                                                del citato e non anche tramite il DOI name del citante.</li>
                                            <li>Le nuove entità venivano prima generate senza tenere conto dell'ultimo indice, provocando
                                                sovrapposizioni.</li>
                                            <li>Specificare l'argomento res nella ricreazione di un grafo preesistente permette adesso di
                                                prescindere dalla funzione di merge poiché, anche se presenti più entità associate ai
                                                medesimi
                                                identificativi, viene riprodotto solo il grafo dell'entità di interesse corrente.</li>
                                            <li>Nel caso in cui la funzione venga lanciata più volte sul medesimo triplestore, è stato
                                                aggiunto
                                                un controllo per cui le stesse informazioni non vengano aggiunte più volte.</li>
                                        </ol>
                                    </li>
                                    <li>Risolto un bug in merge_by_id_from_triplestore per cui le entità venivano unite a loro stesse.</li>
                                    <li>Aggiunta una nuova funzione di supporto che prende in input il percorso di un file rdf e restituisce
                                        in
                                        output un oggetto di tipo GraphSet.</li>
                                    <li>Tutte le funzioni hanno adesso parametri e output tipati, in osservanza di <a
                                            href="https://www.python.org/dev/peps/pep-3107/" alt="Link to PEP 3107" target="_blank">PEP
                                            3107</a>, allo scopo di aumentare la leggibilità del codice, sia per gli altri che per me. Si
                                        tratta
                                        quindi di semplici annotazioni, che non causeranno un'eccezione TypeError se non rispettate.</li>
                                    <li>Il vantaggio del lavorare con una certa mole di dati è che, mentre gli script vengono eseguiti, si
                                        ha
                                        molto tempo per leggere:
                                        <ol>
                                            <li>Belhajjame, K., B’Far, R., Cheney, J., Coppens, S., Cresswell, S., Gil, Y., Groth, P.,
                                                Klyne,
                                                G., Lebo, T., McCusker, J., Miles, S., Myers, J., Sahoo, S., & Tilmes, C. (2013). PROV-DM:
                                                The
                                                PROV Data Model (L. Moreau & P. Missier, Eds.). World Wide Web Consortium.
                                                https://www.w3.org/TR/prov-dm/.</li>
                                            <li>Wolfe, M. (2017, August 9). CC0 and Data Citation.
                                                https://www.library.ucdavis.edu/news/cc0-and-data-citation/.</li>
                                            <li>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A.,
                                                Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A.
                                                J.,
                                                Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons,
                                                B.
                                                (2016). The FAIR Guiding Principles for scientific data management and stewardship.
                                                Scientific
                                                Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18.</li>
                                            <li>GO FAIR. (2018). FAIR Principles. https://www.go-fair.org/fair-principles/.</li>
                                            <li>Open Knowledge Foundation. (2015). Open Definition 2.1.
                                                https://opendefinition.org/od/2.1/en/.
                                            </li>
                                            <li>Landi, A., Thompson, M., Giannuzzi, V., Bonifazi, F., Labastida, I., da Silva Santos, L. O.
                                                B.,
                                                & Roos, M. (2020). The “A” of FAIR – As Open as Possible, as Closed as Necessary. Data
                                                Intelligence, 2(1–2), 47–55. https://doi.org/10.1162/dint_a_00027.</li>
                                            <li>Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M.,
                                                L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V.,
                                                Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST
                                                Principles for digital repositories. Scientific Data, 7(1), 144.
                                                https://doi.org/10.1038/s41597-020-0486-7.</li>
                                            <li>Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS
                                                Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525.</li>
                                            <li>Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to
                                                uniquely identify researchers. Learned Publishing, 25(4), 259–264.
                                                https://doi.org/10.1087/20120404.</li>
                                            <li>Meng, X.-L. (2020). Reproducibility, Replicability, and Reliability. Harvard Data Science
                                                Review, 2(4). https://doi.org/10.1162/99608f92.dbfce7f9.</li>
                                            <li>Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604),
                                                452–454.
                                                https://doi.org/10.1038/533452a.
                                                <ol>
                                                    <li>Vince il premio lettura più sconcertante della settimana, una di quelle che cambia
                                                        il
                                                        modo di vedere le cose. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="d0704cnhc1">Talvolta, provando a fare un merge tra due BibliographicResources, ottengo un errore
                                        di tipo TypeError
                                        che dice:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            [BibliographicResource.contains_discourse_element] Expected argument type: de. Provided argument
                                            type: BibliographicReference.
                                        </blockquote>
                                        Poiché non sto in alcun modo utilizzando entità di tipo DiscourseElement, sospetto che si tratti di
                                        un
                                        bug.
                                    </li>
                                    <li>Portata a termine l'operazione di merge sulle entità associate agli stessi id e caricate le novità
                                        sul
                                        triplestore, ho riscontrato le stesse stranezze nell'output già riportate sul diario di bordo in
                                        data <a href="#d2303cnhc7">23/03 (<em>Cosa non ho capito</em>, punto 7)</a>. Da quello che ho visto,
                                        se
                                        unisco A a B, ad A viene associato l'id di B e B viene cancellata. Il problema è che A si ritrova ad
                                        essere associata a due entità di tipo datacite:Identifier che in realtà sono identiche, perché
                                        appunto A
                                        e B sono state unite in quando associate agli stessi id. Inoltre, l'entità B, nonostante sia stata
                                        cancellata, continua ad avere degli ingoing links da parte di tutte quelle entità che ad essa si
                                        riferivano in passato. C'è qualcosa del metodo merge che non sto capendo o qualcosa che dovrei fare
                                        e
                                        non sto facendo?</li>
                                    <li>È preferibile avere funzioni distinte che lavorino su triplestore e file o è meglio avere un'unica
                                        funzione che, a seconda degli argomenti passati, lavora indifferentemente su entrambi?</li>
                                    <li>Non so quanto sia ottimale effettuare query SPARQL su un'entità Graph di rdflib, perché credo manchi
                                        delle ottimizzazioni di un triplestore. Quando mi hai detto che devo lavorare sia su triplestore che
                                        su
                                        file intendevi che devo produrre entrambi gli output o che devo creare funzioni che lavorino in
                                        entrambe
                                        le modalità?</li>
                                    <li>Cristian mi ha detto che le librerie con cui lavora per produrre graph embeddings tendono a prendere
                                        in
                                        input un file tsv e non json. Tramite rdflib posso serializzare rdf in una moltitudine di notazioni,
                                        quali turtle, nt, json-ld, rdf/xml ed n3, ma onestamente non ho mai visto un grafo rdf serializzato
                                        in
                                        tsv. Si può fare?</li>
                                    <li>Mi piacerebbe portare avanti nuove idee per quanto riguarda possibili miglioramenti automatici del
                                        grafo, poiché quelle che ho al momento non mi convincono:
                                        <ol>
                                            <li>Correggere i DOI names sbagliati. Dato che è materia d'esame e che coinvolge altre 3 persone
                                                non
                                                mi sembra corretto occuparmene da solo per il tirocinio.</li>
                                            <li>Strutturare i campi unstructured forniti da Crossref precedentemente non considerati. Per
                                                farlo
                                                dovrei utilizzare nuovamente l'API di Crossref, mentre vorrei capire se è possibile
                                                migliorare
                                                ulteriormente quello che ho già senza chiedere altre risorse esterne.</li>
                                            <li>Arricchire i metadati delle entità citate, che mi convince poco per le stesse ragioni del
                                                punto
                                                precedente, ovvero che dovrei rivolgermi all'API di Crossref.</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-6" data-target="#panel-6" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-6">
                            <span class="h6 mb-0 font-weight-bold">14/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-6">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho implementato l'algoritmo <strong>add_reference_data_without_doi</strong> che, preso in input un
                                        file json contenente l'output di Crossref con le informazioni sui lavori di una determinata rivista,
                                        controlla quali reference non hanno DOI name e hanno altri metadati oltre all'unstructured.
                                        Dopodiché, chiama il metodo privato <strong>_generate_crossref_query_from_metadata</strong>, che
                                        trasforma i metadati in una stringa di query da lanciare tramite API di Crossref. Se la ricerca va a
                                        buon fine, seleziona il primo risultato, cerca sul triplestore l'entità citante corrente, verifica
                                        che il DOI name della citata trovata non esista già (FILTER NOT EXISTS) e solo allora crea
                                        Identifier, BibliographicResource e Citation corrispondenti e li aggiunge al preexisting graph
                                        dell'entità citante corrente. </li>
                                    <li>Ho implementato il metodo <strong>add_crossref_reference_data</strong>, che arricchisce il dataset
                                        corredando le risorse bibliografiche citate di numerose informazioni tratte da Crossref, quali
                                        publisher, titolo, sottotitolo, volume, issue, data di pubblicazione, resource embodiment e autori,
                                        ciascuna correttamente mappata seguendo l'OCDM. </li>
                                    <li>Il metodo <strong>merge_by_id_from_triplestore</strong> unisce adesso anche le entità di tipo
                                        datacite:Identifier associate alle entità unite sulla base dell'identificativo.</li>
                                    <li>È stato implementato il metodo <strong>_get_entity_and_ids_from_res</strong>, che si occupa di
                                        restituire un'entità e la lista delle entità degli id associati dato un URIRef e un triplestore.
                                    </li>
                                    <li>Il metodo <strong>_manage_br_type</strong> si occupa adesso di assegnare a ciascuna risorsa
                                        bibliografica il tipo corretto, tra book, book-chapter, component, journal-article, monograph,
                                        other, posted-content, proceeding-article, report e report-series. </li>
                                    <li>Sono stati estratti dal metodo generate_graph tre nuovi metodi privati statici, i quali sono stati
                                        riutilizzati dal metodo add_crossref_reference_data:
                                        <ol>
                                            <li><strong>_manage_author_ra_ar</strong>;</li>
                                            <li><strong>_manage_volume_issue</strong>;</li>
                                            <li><strong>_manage_resource_embdiment</strong>.</li>
                                        </ol>
                                    </li>
                                    <li>Prima di avventurarmi nella scrittura di un programma di editing interattivo per Knowledge Graph, ho
                                        esplorato la letteratura esistente sull'argomento:
                                        <ol>
                                            <li>Wright J., Rodríguez Méndez S.J., Haller A., Taylor K., Omran P.G. (2020) Schímatos: A
                                                SHACL-Based Web-Form Generator for Knowledge Graph Editing. In: Pan J.Z. et al. (eds) The
                                                Semantic Web – ISWC 2020. ISWC 2020. Lecture Notes in Computer Science, vol 12507. Springer,
                                                Cham. https://doi.org/10.1007/978-3-030-62466-8_5.
                                                <ol>
                                                    <li>L'articolo introduce <strong>Schímatos</strong>, un editor di KG interattivo, che si
                                                        occupa della validazione degli input utilizzando SHACL Shapes Constraint Language
                                                        (https://www.w3.org/TR/shacl/). Non sembra esserci però un modo immediato per
                                                        riutilizzare il software con KG che non siano WikiData. Sembra infatti essere
                                                        soltanto una demo a scopo dimostrativo, per quanto molto interessante.</li>
                                                </ol>
                                            </li>
                                            <li>Heyvaert P., Dimou A., Verborgh R., Mannens E., Van de Walle R. (2016) Graph-Based Editing
                                                of Linked Data Mappings Using the RMLEditor. In: Sack H., Rizzo G., Steinmetz N., Mladenić
                                                D., Auer S., Lange C. (eds) The Semantic Web. ESWC 2016. Lecture Notes in Computer Science,
                                                vol 9989. Springer, Cham. https://doi.org/10.1007/978-3-319-47602-5_25.
                                                <ol>
                                                    <li><strong>RMLEditor</strong> offre una GUI per consentire ai data publishers, esperti
                                                        di dominio, di modellare la conoscenza derivata da dati di origini multiple ed
                                                        eterogenee. RMLEditor utilizza RML come linguaggio di mappatura sottostante. Ad oggi
                                                        è presente una demo che permette di modellare fino a 20 nodi per un massimo di 2 MB
                                                        per singolo file. </li>
                                                </ol>
                                            </li>
                                            <li>Ho letto la documentazione e ho provato a usare <strong>Web-Karma</strong> (<a
                                                    href="https://github.com/usc-isi-i2/Web-Karma" target="_blank"
                                                    alt="Web-Karm documentation">https://github.com/usc-isi-i2/Web-Karma</a>), un tool per
                                                modificare un KG in base a un'ontologia specificata. Problema: non ha un sistema di
                                                validazione rispetto all'ontologia ed è possibile modificare l'ontologia stessa,
                                                intenzionalmente o per errore. In sintesi, utilizzabile solo da chi conosce già molto bene
                                                il data model. </li>
                                        </ol>
                                    </li>
                                    <li>Ho letto la documentazione di <strong>Flask</strong>, allo scopo di utilizzare Python per il
                                        back-end della mia applicazione di KG editing. Ho anche letto la documentazione di
                                        <strong>Jinja</strong>, dato che è il template-engine predefinito di Flask.</li>
                                    <li>Utilizzando Flask per il server, Jinja2 come motore di template e Blazegraph come database ho
                                        implementato una prima interfaccia grafica che mostra dieci triple. </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Oltre ai due bug del metodo merge di oc_ocdm già rilevati in <a href="#d0704cnhc1">07/04/2021 (Cosa
                                            non ho capito, punti 1 e 2)</a> credo di averne trovato un terzo specifico per il merge tra
                                        entità di tipo datacite:Identifier. Ho miniaturizzato e commentato il codice utilizzato al fine di
                                        rendere l'errore più facile da comprendere e riprodurre. Inoltre, mi piacerebbe capire se io stia
                                        procedendo nel modo corretto prima di mandare un'altra mail a Simone.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre>
                                                                    <code class="prettyprint">
    from oc_ocdm.graph.graph_entity import GraphEntity
    from oc_ocdm.graph import GraphSet
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, JSON, RDFXML


    def get_entity_from_res(res:URIRef, graphset:GraphSet) -> GraphEntity:
        res = URIRef(res)
        query = f"""
            CONSTRUCT {{&lt;{res}&gt; ?p ?o}}
            WHERE {{&lt;{res}&gt; ?p ?o}}
        """
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        results = sparql.query().convert()
        preexisting_graph = Graph().parse(data=results.serialize(format='xml'), format='xml')
        entity = graphset.add_id(resp_agent="https://orcid.org/0000-0002-8420-0696", res=res, preexisting_graph=preexisting_graph)
        return entity

    # Estrai Identifier e DOI name di tutte le br dal triplestore
    queryString = """
        PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
        PREFIX fabio: &lt;http://purl.org/spar/fabio/&gt;
        PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
        SELECT ?id ?literalValue
        WHERE
        {{
            ?s a fabio:Expression;
                datacite:hasIdentifier ?id.
            ?id literal:hasLiteralValue ?literalValue.
        }}
    """
    sparql = SPARQLWrapper("http://localhost:9999/bigdata/sparql")
    sparql.setQuery(queryString)
    sparql.setReturnFormat(JSON)
    results = sparql.query().convert()
    dois_found = dict()
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    for result in results["results"]["bindings"]:
        # Se il DOI name è già stato trovato e l'Identifier è diverso da quello già trovato
        if result["literalValue"]["value"] in dois_found and result["id"]["value"] != dois_found[result["literalValue"]["value"]]:
            id_duplicated_entity = get_entity_from_res(result["id"]["value"], graphset)
            id_preexisting_entity = get_entity_from_res(dois_found[result["literalValue"]["value"]], graphset)
            print(f"Merging entity {id_preexisting_entity.res} with {id_duplicated_entity.res}")
            id_preexisting_entity.merge(id_duplicated_entity)
        else:
            # Registra di aver trovato questo DOI name e che è associato a questo Identifier
            dois_found[result["literalValue"]["value"]] = result["id"]["value"]
                                                                </code>
                                                            </pre>
                                        </div>
                                        Se a questo punto provo a fare l'upload attraverso il metodo upload_all passando il graphset,
                                        l'upload non avviene a causa di un errore nella formattazione della query di upload. In particolare,
                                        pare che manchi un ";" prima di un INSERT.
                                    </li>
                                    <li>Per qualche ragione a volte l'update_query vuota non è "" ma None. Il metodo upload_all della classe
                                        Storer di oc_ocdm non gestiste il caso in cui l'update_query sia None, sollevando un'eccezione nel
                                        momento in cui viene fatta la seguente concatenazione: query_string += " ; " + update_query, dato
                                        che non è possibile concatenare una stringa con None. Ho risolto in locale aggiungendo:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre>
                                                                <code class="prettyprint">
    # for idx, entity in enumerate(self.a_set.res_to_entity.values()):
        # update_query, n_added, n_removed = get_update_query(entity, entity_type=self._class_to_entity_type(entity))
            if update_query == "" or update_query is None:
                skipped_queries += 1
                                                                </code>
                                                            </pre>
                                        </div>
                                        Tuttavia, dato che l'update_query vuota dovrebbe sempre essere "" e non None, credo ci sia un altro
                                        bug a monte.
                                    </li>
                                    <li>Non ho trovato documentazione su tutti i possibili metadati delle reference di un work restituite da
                                        Crossref, quindi ho scoperto tramite un algoritmo che, sui quasi 7000 articoli citanti di
                                        Scientometrics, i citati posso riportare le chiavi sotto elencate. Di fianco a ciascuna viene
                                        indicato tramite quale notazione sono state usate nella query a Crossref al fine di recuperare il
                                        DOI name:
                                        <ul>
                                            <li>journal-title → query.bibliographic=value&</li>
                                            <li>author → query.author=value&</li>
                                            <li>journal-title → query.container-title=value&</li>
                                            <li>ISBN → filter=isbn:value</li>
                                            <li>year → filter=from-index-date:value,
                                                <ul>
                                                    <li>Perché proprio from-index-date? Perché nella documentazione si legge:
                                                        <blockquote class="blockquote ml-5 mt-3">
                                                            When using time filters to retrieve periodic, incremental metadata updates, the
                                                            from-index-date filter should be used over from-update-date, from-deposit-date,
                                                            from-created-date and from-pub-date. The timestamp that from-index-date filters
                                                            on is guaranteed to be updated every time there is a change to metadata
                                                            requiring a reindex.
                                                        </blockquote>
                                                    </li>
                                                </ul>
                                            </li>
                                        </ul>
                                        Tuttavia, ci sono molti altri campi per i quali Crossref non sembra non fornire né field queries né
                                        filtri, ovvero:
                                        <ul>
                                            <li>issue</li>
                                            <li>volume-title</li>
                                            <li>series-title</li>
                                            <li>edition</li>
                                            <li>key</li>
                                            <li>first-page</li>
                                            <li>volume</li>
                                            <li>isbn-type</li>
                                        </ul>
                                        Le domande a questo punto sono:
                                        <ol>
                                            <li>Ti risulta che io non possa fare query a Crossref utilizzando quelle informazioni?</li>
                                            <li>È possibile utilizzare volume-title e series-title come valore della field query
                                                <em>query.container-title</em>?</li>
                                            <li>I risultati ottenuti finora sono sorprendentemente buoni, vale la pena restringere
                                                ulteriormente la ricerca?</li>
                                        </ol>
                                    <li>Cristian mi ha detto che per il suo progetto un'informazione rilevante è l'affiliazione degli
                                        autori. Credo che l'OCDM non la preveda, giusto?</li>
                                    <li>Se una risorsa bibliografica viene indicata da Crossref come di tipo "posted-content" con quale tipo
                                        di BibliographicResource presente nell'OCDM devo mapparla? </li>
                                    </li>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-7" data-target="#panel-7" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-7">
                            <span class="h6 mb-0 font-weight-bold">21/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-7">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Novità relative al <strong>Knowledge Graph Editor</strong>:
                                        <ol>
                                            <li>È stata aggiunta una textarea che permette di effettuare query SPARQL.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_textarea.png" alt="SPARQL textarea">
                                                </div>
                                            </li>
                                            <li>Se la query richiede molto tempo, un'icona di caricamento comunica all'utente lo stato
                                                interno del sistema.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_loading.png" alt="SPARQL loading">
                                                </div>
                                            </li>
                                            <li>L'output della query viene mostrato in una tabella, in cui ogni colonna rappresenta una
                                                variabile della query stessa. Le entità vengono riportate come link, mentre non è possibile
                                                interagire con i predicati, con gli oggetti di tipo literal e in generale con gli oggetti il
                                                cui URI non contiene l'URI base del dataset.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_results.png" alt="SPARQL results">
                                                </div>
                                            </li>
                                            <li>Cliccando su un'entità, vengono mostrati tutti gli outgoing links e tutti gli incoming links
                                                relativi a quell'entità.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_entity.png" alt="SPARQL entity">
                                                </div>
                                            </li>
                                            <li>Il pulsante "Edit" permette di accedere alla modalità di modifica.</li>
                                        </ol>
                                    </li>
                                    <li>Novità relative al metodo per ottenere i DOI names delle reference riportate da Crossref senza DOI.
                                        <ol>
                                            <li>Il metodo <strong>add_reference_data_without_doi</strong> non utilizza più data di
                                                pubblicazione e ISBN per fare query su Crossref, mentre usa volume-title e series-title
                                                oltre al journal-title.</li>
                                            <li>Sono state aggiunti diversi metodi il cui scopo complessivo è quello di calcolare il
                                                <strong>matching score</strong> tra due dizionari di metadati, ovvero quelli sintentici e
                                                grezzi riportati da Crossref nel campo "reference" e quelli approfonditi risultanti da una
                                                query. L'algoritmo è stato mutuato dai capitoli 3.2 and soprattuttutto dall'appendice
                                                dell'articolo <em>Large-scale comparison of bibliographic data.
                                                    sources: Scopus, Web of Science, Dimensions,
                                                    Crossref, and Microsoft Academic</em> (Visser, Martijn et al., 2005), reperibile al
                                                seguente indirizzo: <a href="https://arxiv.org/abs/2005.10732" alt="Link to the paper"
                                                    target="_blank">https://arxiv.org/abs/2005.10732</a>. La formula adottata, rispetto a
                                                quella originale, non considera i DOI names, poiché, date le premesse, non sono mai presenti
                                                nei metadati di partenza.
                                                <ol>
                                                    <li>Il metodo <strong>_do_heuristic_match</strong> combina gli score derivanti dai
                                                        singoli match in un unico score finale.</li>
                                                    <li>Il metodo <strong>_levenshtein_distance</strong> calcola la distanza di Levenshtein,
                                                        ovvero il minimo numero di modifiche (cancellazioni, aggiunte e sostituzioni)
                                                        necessarie per trasformare una stringa in un'altra. È stato adottato l'algoritmo
                                                        iterativo basato su una matrice, poiché le soluzioni ricorsiva e tramite tecnica di
                                                        dynamic programming richiedevano innumerevoli passaggi in più, risultando meno
                                                        efficienti.
                                                        <p>Questa funzione vince il premio algoritmo più entusiasmante della settimana, se
                                                            non del mese. È talmente bella che la riporto nel diario:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    import numpy as np

    def levenshtein_distance(target:str, source:str) -> int:
        target = [k for k in "#" + target]
        source = [k for k in "#" + source]
        sol = np.zeros(shape=(len(source), len(target)))
        sol[0] = [j for j in range(len(target))]
        sol[:,0] = [j for j in range(len(source))]
        if target[1] != source[1]:
            sol[1,1] = 2
        for c in range(1, len(target)):
            for r in range(1, len(source)):
                if target[c] != source[r]:
                    sol[r,c] = min(sol[r-1,c], sol[r,c-1]) + 1
                else:
                    sol[r,c] = sol[r-1,c-1]
        min_edit_distance = int(sol[-1,-1])
        return min_edit_distance
                                                                            </code></pre>
                                                        </div>
                                                    </li>
                                                    <li>
                                                        <p>Il metodo <strong>_match_first_author</strong> compara le iniziali del nome e il
                                                            cognome dei primi autori dei due lavori, tramite la seguente formula:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    𝑚<sub>firstauthor</sub> = 0.8 − 0.8𝐷(𝑙<sub>A</sub>, 𝑙<sub>B</sub>) ⁄ max(𝐿(𝑙<sub>A</sub>), 𝐿(𝑙<sub>B</sub>)) + 0.2𝐸(𝑓<sub>A, 𝑓<sub>B</sub>)
                                                                                </code></pre>
                                                        </div>
                                                        <p>Dove l<sub>X</sub> ed f<sub>X</sub> denotano rispettivamente il cognome e le
                                                            iniziali del nome del primo autore del documento X. L(a) è uguale alla lunghezza
                                                            della stringa. D(a,b) è la distanza di Levenshtein tra la stringa a e la stringa
                                                            b. E(a,b) è uguale a 1 se a è uguale a b, altrimenti 0. Dunque,
                                                            𝑚<sub>firstauthor</sub> è uguale a 1 se i primi autori dei documenti A e B
                                                            hanno gli stessi cognomi e iniziali dei nomi.</p>
                                                    </li>
                                                    <li>
                                                        <p>Il metodo <strong>_match_title</strong> compara i titoli dei due lavori, tramite
                                                            la seguente formula:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    𝑚<sub>title</sub> = 1 − 𝐷(𝑡<sub>A</sub>, 𝑡<sub>B</sub>) ⁄ max(𝐿(𝑡<sub>A</sub>), 𝐿(𝑡<sub>B</sub>))
                                                                                </code></pre>
                                                        </div>
                                                        <p>Anche in questo caso, 𝑚<sub>title</sub> è uguale a 1 se i titoli dei documenti A
                                                            e B sono identici. </p>
                                                    </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho aperto il seguente issue: <a href="https://github.com/opencitations/oc_ocdm/issues/8"
                                            alt="Link to the GitHub issue on oc-ocdm"
                                            target="_blank">https://github.com/opencitations/oc_ocdm/issues/8</a>.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        <p>Per effettuare un'operazione di aggiunta, cancellazione e modifica sulle triple del dataset
                                            attraverso il KGEditor, devo necessariamente utilizzare oc_ocdm. Se lanciassi query di update in
                                            autonomia, infatti, perderei tutte le informazioni contenute negli indici del dataset e della
                                            provenance, oltre al fatto che dovrei riscrivere lo stesso codice già scritto da Simone per
                                            effettuare le medesime operazioni. Problema: come dico a Python quale metodo di oc_ocdm invocare
                                            per fare un delete, un create o un update? Mi spiego meglio con un esempio: poniamo che l'utente
                                            voglia eliminare una delle triple in figura cliccando sull'apposito pulsante "-":</p>
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/sparql_remove.png" alt="SPARQL remove entity">
                                        </div>
                                        <p>Ad esempio, potrebbe voler eliminare la prima tripla:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    &lt;https://github.com/arcangelo7/time_agnostic/br/1&gt; &lt;http://purl.org/spar/datacite/hasIdentifier&gt; &lt;https://github.com/arcangelo7/time_agnostic/id/1&gt;
                                            </code></pre>
                                        </div>
                                        <p>Niente di più facile, oc_ocdm mette a disposizione il metodo remove_identifier a questo scopo.
                                            Tuttavia, il pulsante "-" è generico e l'unica informazione per capire quale metodo di
                                            cancellazione invocare è il predicato <http: //purl.org/spar/datacite/hasIdentifier>. Come
                                                faccio dal predicato a risalire a quale metodo di cancellazione utilizzare? Il problema è il
                                                medesimo anche per aggiungere e modificare. Devo creare un enorme dizionario che mappi ogni
                                                predicato con il suo metodo? A mio parere, una soluzione più pulita sarebbe un unico metodo
                                                delete generico per una bibliographic_entity che prende in input qualunque oggetto e
                                                cancella la tripla.</p>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-8" data-target="#panel-8" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-8">
                            <span class="h6 mb-0 font-weight-bold">28/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-8">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <p>Tutte le novità di questa settimana sono relative al <strong>Knowledge Graph Editor</strong>.</p>
                                <ol>
                                    <li>È stato creato un file di configurazione in formato json, che mappa diversi tipi di entità e predicato
                                        con i relativi metodo di creazione e cancellazione di oc-ocdm. È possibile visualizzarlo a questo
                                        indirizzo: <a
                                            href="https://raw.githubusercontent.com/arcangelo7/time_agnostic/main/KGEditor/static/config/config.json"
                                            target="_blank"
                                            alt="Link to the config file">https://raw.githubusercontent.com/arcangelo7/time_agnostic/main/KGEditor/static/config/config.json</a>.
                                        Se ne riporta qui un'anteprima.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    "http://purl.org/spar/fabio/Expression": {
        "add": "add_br",
        "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": {
            "create": "",
            "delete": "remove_type"
        },
        "http://purl.org/spar/datacite/hasIdentifier": {
            "create": "has_identifier",
            "delete": "remove_identifier"
        },
        "http://purl.org/dc/terms/title": {
            "create": "has_title",
            "delete": "remove_title"
        },
        "http://purl.org/spar/fabio/hasSubtitle": {
            "create": "has_subtitle",
            "delete": "remove_subtitle"
        },
        "http://purl.org/vocab/frbr/core#partOf": {
            "create": "is_part_of",
            "delete": "remove_is_part_of"
        },
        "http://purl.org/spar/cito/cites": {
            "create": "has_citation",
            "delete": "remove_citation"
        },
        "http://prismstandard.org/namespaces/basic/2.0/publicationDate": {
            "create": "has_pub_date",
            "delete": "remove_pub_date"
        },
        "http://purl.org/vocab/frbr/core#embodiment": {
            "create": "has_format",
            "delete": "remove_format"
        },
        "http://purl.org/spar/fabio/hasSequenceIdentifier": {
            "create": "has_number",
            "delete": "remove_number"
        },
        "http://purl.org/vocab/frbr/core#part": {
            "create": "contains_in_reference_list",
            "delete": "remove_contained_in_reference_list"
        },
        "http://purl.org/spar/pro/isDocumentContextFor": {
            "create": "has_contributor",
            "delete": "remove_contributor"
        }
    }
                                                                        </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        La funzione di cancellazione delle triple è ora funzionante. Tramite la <strong>reflection</strong>,
                                        viene dinamicamente invocato il metodo appropriato in base al soggetto e predicato selezionati
                                        dall'utente per la cancellazione. In particolare, viene utilizzata la funzione <strong>getattr</strong>
                                        per invocare il metodo vero e proprio. Nel caso in cui questo metodo voglia un argomento, viene passata
                                        l'entità relativa all'oggetto. Per capire quanti parametri siano richiesti dal metodo è stata usata la
                                        funzione <strong>signature</strong> del modulo inspect di Python.
                                    </li>
                                    <li>
                                        Cliccando sul pulsante di cancellazione, la tripla viene decorata con un line-through e il bottone
                                        diventa un pulsante di aggiunta per annullare l'azione precedente. Finché non viene cliccato il pulsante
                                        "Done", tutte le query vengono salvate in un dizionario. Cliccando sul "+" viene cancellata dal
                                        dizionario l'operazione di cancellazione corrispondente. Lo schema delle chiavi del dizionario di update
                                        è "soggetto + predicato + oggetto".
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/sparql-line-through.png" alt="line-through">
                                        </div>
                                    </li>
                                    <li>
                                        È stato introdotto il pulsante di <strong>update</strong>, contraddistinto dall'icona della matita,
                                        poiché le modifiche sono reversibili finché non si preme il tasto "Done". Quest'ultimo, al contrario, è
                                        contraddistinto dall'icona della penna a inchiostro, poiché causa la modifica permanente del dataset e
                                        della provenance. Cliccando sul pulsante di update, la tripla da testo diventa input (textarea nel caso
                                        di valori che contengono spazi) e si può modificare. Così facendo, la funzione di cancellazione viene
                                        disabilitata. Viceversa, la funzione di cancellazione disabilita quella di update. Una volta completata
                                        la modifica, è possibile premere il pulsante con l'icona del "check" per salvare la relativa query, che
                                        non verrà effettuata fino al click sul tasto "Done".
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-update.png" alt="editor update">
                                        </div>
                                    </li>
                                    <li>
                                        È stato introdotta la funzionalità per <strong>creare</strong> una nuova tripla. Cliccando sul pulsante
                                        "+" di fianco a un'entità si apre un modale, nel quale è possibile inserire un nuovo predicato e un
                                        nuovo oggetto per quell'entità. L'input del predicato è dotato di una funzione di auto-completamento,
                                        che si basa sul medesimo file di configurazione utilizzato per mappare soggetti e predicati con relativi
                                        metodi di aggiunta e cancellazione di oc-ocdm.
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-create.png" alt="editor create.png">
                                        </div>
                                    </li>
                                    <li>
                                        Nel caso in cui l'utente non si sia ancora autenticato tramite ORCID, cliccando sul pulsante "Edit"
                                        viene aperto un modale per specificare il <strong>responsible agent</strong>.
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-ra.png" alt="editor responsible agent.png">
                                        </div>
                                        Il valore inserito dev'essere necessariamente un ORCID. Il campo viene infatti validato tramite la
                                        seguente espressione regolare.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    /(https?:\/\/orcid.org\/)?([0-9]{4})-([0-9]{4})-([0-9]{4})-([0-9]{4})/i
                                                                        </code></pre>
                                        </div>
                                        Una volta specificato il responsible agent, viene generato un <strong>cookie</strong> e l'ORCID viene
                                        salvato nella <strong>sessione</strong>, per cui non verrà più chiesto.
                                    </li>
                                    <li>Le <strong>triple</strong> di <strong>provenance</strong> sono adesso <strong>immutabili</strong>. Nel
                                        contesto di un'entità di provenance il pulsante di modifica non appare, mentre nel contesto di un
                                        qualunque altro tipo di entità non compaiono i pulsanti di modifica e cancellazione di fianco agli
                                        incoming links legati alla provenance.</li>
                                    <li>Risolto un bug per cui la rotella di caricamento girava all'infinito in caso di errori nella query o di
                                        database irraggiungibile. </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Implementando l'operazione di update del KGEditor mi sono nuovamente imbattuto nel bug delle query
                                        malformate, ovvero prive di ";" tra un'operazione di update e l'altra. L'update consiste infatti di due
                                        operazione, una di cancellazione e una di aggiunta. In questo modo ho potuto astrarre il problema e scrivere del codice per riprodurlo, che riporto qui sotto:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_br(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    br_16 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/16"), graphset=graphset)
    br_19 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/19"), graphset=graphset)
    br_99 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/99"), graphset=graphset)
    br_16.remove_citation(br_19)
    br_16.has_citation(br_99)
    storer = Storer(graphset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")                                </code></pre>
                                        </div>
                                        Purtroppo non sarà possibile parlarne con Simone, perché ha già pubblicato la versione 6.0.0 di oc_ocdm
                                        e mi ha detto che non metterà più mano al codice.
                                    </li>
                                </ol>
                            </div>
                        </div>
                        </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-9" data-target="#panel-9" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-9">
                            <span class="h6 mb-0 font-weight-bold">13/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-9">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>È stata ultimata la funzione <strong>add_reference_data_without_doi</strong>, che aggiunge al grafo
                                        le reference senza DOI, dopo aver trovato il DOI tramite query a Crossref e match euristico dei
                                        risultati.
                                        <ol>
                                            <li>È stata implementata la funzione <strong>_match_other</strong>, che calcola il match
                                                basandosi sull'anno di pubblicazione, il volume, l'issue e la pagina iniziale (quella finale
                                                non è mai indicata nel campo reference, <em>for reasons</em>), secondo la seguente formula:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    match<sub>other</sub> = 0.1𝐸(𝑦<sub>A</sub>, 𝑦<sub>B</sub>) + 0.2𝐸(𝑣<sub>A</sub>, 𝑣<sub>B</sub>) + 0.1𝐸(𝑖<sub>A</sub>, 𝑖<sub>B</sub>) + 0.6𝐸(𝑏<sub>A</sub>, 𝑏<sub>B</sub>)</code></pre>
                                                </div>
                                                dove y<sub>x</sub>, v<sub>x</sub>, i<sub>x</sub> e b<sub>x</sub> denotano, rispettivamente,
                                                l'anno di pubblicazione, il numero del volume, il numero dell'issue e la pagina iniziale del
                                                documento x. E(a,b) è uguale a 1 se a e b sono identici, 0 in caso contrario. La formula
                                                originale proposta da Martijn Visser (<a
                                                    href="https://arxiv.org/ftp/arxiv/papers/2005/2005.10732.pdf" target="_blank" ,
                                                    alt="Link to the Martijn Visser paper">https://arxiv.org/ftp/arxiv/papers/2005/2005.10732.pdf</a>)
                                                prevede anche la pagina finale e assegna un moltiplicatore di 0.3 sia alla pagina finale che
                                                a quella finale: per far fronte all'assenza della pagina finale, si è assegnato un
                                                moltiplicatore di 0.6 a quella iniziale, in modo che il risultato finale oscilli tra 0 e 1.
                                            </li>
                                            <li>
                                                È stata implementata la funzione <strong>_is_a_match</strong>, che combina i match basati
                                                sul primo autore, sul titolo, sulla fonte e sugli altri valori in un unico score finale,
                                                secondo la seguente formula:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    similarity<sub>A,B</sub> = 7 * match_first_author + 14 * match_title + 5 * match_source + 14 * match_other</code></pre>
                                                </div>
                                                La formula originale prevedeva anche l'aggiunta di 15 punti per il match tra il DOI della
                                                fonte e del target, valore che è stato rimosso perché il DOI della fonte non è mai
                                                disponibile, essendo l'incognita che si intende individuare. Per questo motivo, è stato
                                                ridotto il <em>threshold</em> oltre il quale viene stabilito un match, che nel paper di
                                                Martijn Visser era fissato a 30, mentre nel nostro caso è di 15, ovvero 30 meno i 15 punti
                                                del match tra DOI.
                                            </li>
                                        </ol>
                                    </li>
                                    <li>
                                        Ho generato i seguenti livelli di provenance:
                                        <ol>
                                            <li>Livello 0: creazione del grafo.</li>
                                            <li>Livello 1: aggiunta delle reference presenti in COCI e non nel grafo del livello 0, quindi
                                                creazione dei rispettivi br, id e ci. Se la reference era già presente nel livello 0,
                                                aggiunta del timespan e del tipo di citazione (journal self-citation o author
                                                self-citation).</li>
                                            <li>Livello 2: arricchimento dei dati delle reference tramite Crossref, in particolare aggiunta
                                                della rivista di origine (br, id, ra, ar), aggiunta del tipo di br della reference, del
                                                titolo, sottotitolo, data di pubblicazione, autore (ra, ar), volume, issue e resource
                                                embodiment.</li>
                                            <li>Livello 3 (<em>in progress</em>, è al 28% alle 11:36 del 12/05/2021): aggiunta di alcune
                                                citazioni andate perse poiché riportate da Crossref senza DOI.</li>
                                            <li>Ogni livello di provenance è stato salvato sia sul triplestore che in due file json (uno per
                                                i dati, l'altro per la provenance) . Inoltre, sono stati creati dei backup di ogni singolo
                                                stato del database, dei file json e degli info_dir.</li>
                                            <li><strong>Nota bene</strong>: i livelli di provenance attuali sono temporanei. Servono
                                                soltanto ad avere una base su cui lavorare per la creazione del Time agnostic browser.
                                                Quando la funzione di merge sarà funzionante, essa verrà lanciata dopo ogni livello, in modo
                                                da ottimizzare i processi successivi, che dovendo lavorare su meno entità impiegheranno meno
                                                tempo e meno RAM.</li>
                                        </ol>
                                    </li>
                                    <li>
                                        Novità sul <strong>Knowledge Graph Editor</strong>:
                                        <ol>
                                            <li>La funzione di update è ora pienamente funzionante. Simone ha infatti corretto il bug delle
                                                query malformate.</li>
                                            <li>L'editor crea e aggiorna i grafi di provenance.</li>
                                            <li>L'editor gestisce la creazione e l'update dei valori letterali degli identificatori.</li>
                                            <li>È adesso possibile creare e modificare il tipo delle entità di tipo pro:RoleInTime,
                                                fabio:Expression, foaf:Agent, fabio:Manifestation, datacite:Identifier,
                                                biro:BibliographicReference e cito:Citation.</li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        La funzione di merge è stata modificata, ma i bug sono aumentati. Ora non solo ci sono tutti quelli
                                        di cui abbiamo discusso (identificatori non uniti, incoming links a entità cancellate), ma ce n'è
                                        uno nuovo, ovvero che, se A e B vengono uniti, A viene privata di tutti i suoi outgoing links meno
                                        il tipo. Ti mostro il codice com'è adesso e provo a dare una spiegazione del perché non funziona.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    def merge(self, other: GraphEntity) -> None:
        # Here we must REDIRECT triples pointing
        # to 'other' to make them point to 'self':
        for res, entity in self.g_set.res_to_entity.items():
            triples_list: List[Tuple] = list(entity.g.triples((res, None, other.res)))
            for triple in triples_list:
                entity.g.remove(triple)
                new_triple = (triple[0], triple[1], self.res)
                entity.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        # 'other' must be deleted AFTER the redirection of
        # triples pointing to it, since mark_as_to_be_deleted
        # also removes every triple pointing to 'other'
        other.mark_as_to_be_deleted()
                                                            </code></pre>
                                        </div>
                                        La parte problematica è "triples_list = list(entity.g.triples((res, None, other.res)))". L'idea è
                                        quella di ottenere tutte le triple che puntano a other, in modo da poterle reindirizzare su self.
                                        Tuttavia la lista triples_list sarà pressoché sempre vuota, perché viene riempita da triple che da
                                        self puntano a other, condizione che non si verifica quasi mai. In generale, non è possibile
                                        risalire alle triple che hanno come oggetto self o other in questo modo, perché il preexisting_graph
                                        contiene solo le triple che hanno self o other come soggetto, mai come oggetto.
                                    </li>
                                    <li>
                                        Credo ci sia un bug in oc-ocdm per quanto riguarda la rimozione di triple che hanno come soggetto
                                        un'entità di tipo foaf:Agent. Ad esempio, i metodi remove_name, remove_given_name,
                                        remove_family_name non fanno alcunché. Purtroppo, il codice scritto da Simone mi sembra corretto,
                                        non sono riuscito a capire cosa causi l'errore. Riporto del codice utile a riprodurre il problema:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_ra(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    ra_399 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/ra/339"), graphset=graphset)
    ra_399.remove_family_name()
    storer = Storer(graphset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")
                                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        Credo ci sia un bug in oc-ocdm relativo alla creazione del grafo di provenance di entità di tipo
                                        pro:RoleInTime e fabio:Expression. Qualunque sia la modifica all'entità, lo snapshot riporta come
                                        update query la seguente stringa: "INSERT DATA { GRAPH { .} }". Riporto del codice utile a
                                        riprodurre il problema.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.prov import ProvSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_ar(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/")
    ar_338 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/ar/338"), graphset=graphset)
    ar_338.create_publisher()
    storer = Storer(graphset)
    provset = ProvSet(prov_subj_graph_set=graphset, base_iri="https://github.com/arcangelo7/time_agnostic/", info_dir="./data/info_dir/prov/")
    provset.generate_provenance()
    storer_prov = Storer(provset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")       
    storer_prov.upload_all("http://localhost:9999/blazegraph/sparql")
                                                            </code></pre>
                                        </div>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-10" data-target="#panel-10" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-10">
                            <span class="h6 mb-0 font-weight-bold">19/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-10">
                            <div class="pt-3">
                                <h3>Brainstorming sul Time Agnostic Browser</h3>
                                <ol>
                                    <li>La libreria si deve basare su RDF e dev'essere il più riutilizzabile possibile in diversi contesti, per
                                        tutti quei dataset che utilizzano lo stesso modello di provenance di OpenCitations.</li>
                                    <li>Modello usato per la provenance: PROV-O ontology + oco:hasUpdateQuery.
                                        <ol>
                                            <li><em>oco:hasUpdateQuery</em>: proprietà che registra le aggiunte e le cancellazioni come query SPARQL
                                                INSERT e DELETE, mentre l'uso di variabili SPARQL è proibito nelle query di update.</li>
                                            <li>Ogni entità del dataset è rappresentata da uno o più snapshot (cioè da un'istanza della classe
                                                pro:Entity).</li>
                                            <li>Ogni snapshot registra la composizione dell'entità in un preciso momento temporale, descritto
                                                tramite la proprietà pro:generatedAtTime. Ogni snapshot è collegato ai precedenti tramite la
                                                proprietà pro:wasDerivedFrom, all'entità descritta tramite pro:specializationOf e all'agente
                                                responsabile tramite pro:wasAttributedTo.</li>
                                            <li>Vantaggi:
                                                <ol>
                                                    <li>Facilità nell'ottenere gli statement correnti di un'entità, siccome sono quelli
                                                        correntemente disponibili nel dataset.</li>
                                                    <li>Facilità nel ripristinare un'entità a un certo snapshot applicando l'operazione inversa
                                                        (INSERT anziché DELETE e viceversa).</li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Cosa significa fare una query agnostica sul tempo?
                                        <ol>
                                            <li>Poniamo che l'utente effettui la seguente ricerca:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    CONSTRUCT {
        &lt;res&gt; ?p ?o
    } 
    WHERE {
        &lt;res&gt; ?p ?o.
    }
                                                                                </code></pre>
                                                </div>
                                                Dove res è l'URI di una risorsa. Cosa avviene dietro le quinte?
                                                <ol>
                                                    <li>La query viene arricchita in modo da includere informazioni sulla provenance e, in
                                                        particolare, sul tempo di generazione dello snapshot e sulla query di update:
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    PREFIX oco: &lt;https://w3id.org/oc/ontology/&gt;
    PREFIX pro: &lt;http://www.w3.org/ns/prov#&gt;
    CONSTRUCT {
        &lt;res&gt; ?p ?o. 
        ?snapshot pro:generatedAtTime ?t;      
                oco:hasUpdateQuery ?updateQuery.
    }
    WHERE {
        &lt;res&gt; ?p ?o.
        ?snapshot pro:specializationOf &lt;res&gt;;
                pro:generatedAtTime ?t.
        OPTIONAL {
            ?snapshot oco:hasUpdateQuery ?updateQuery.
        }        
    }
                                                                                        </code></pre>
                                                        </div>
                                                        Dal numero di triple di provenance ritornate è possibile dedurre quanti snapshot esistono di
                                                        quella risorsa e, dal tempo in cui sono state generate, il loro ordine.
                                                    </li>
                                                    <li>
                                                        Viene effettuata la query dell'utente arricchita, che restituisce lo stato corrente del
                                                        dataset rispetto a quella risorsa più le informazioni sulla provenance. Il risultato viene
                                                        salvato in un dizionario, che potrebbe avere la seguente struttura:
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    snapshots = {
        &lt;t&gt;: {
            "graph": &lt;grafo della risorsa al tempo t&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-1&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-1&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-n&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-n&gt;,
            "hasUpdateQuery": ""
        },
    }
                                                                                        </code></pre>
                                                        </div>
                                                        Dove le chiavi corrispondono alle proprietà pro:generatedAtTime degli snapshot ritornati. Da
                                                        notare che il dizionario al tempo t-n non ha updateQuery, perché corrisponde al grafo
                                                        dell'entità nel momento in cui è stata creata.
                                                    </li>
                                                    <li>
                                                        Se la query di update è presente, per ottenere lo stato del grafo rispetto alla risorsa al
                                                        tempo t-1, bisogna effettuare la query inversa rispetto a quella di update. Per farlo, è
                                                        sufficiente sostituire "DELETE" con "INSERT" e "INSERT" con "DELETE". Nota bene: la query di
                                                        update non viene effettuata sul triplestore, ma sul grafo della risorsa al tempo t, in modo
                                                        da non alterare lo stato del triplestore.
                                                    </li>
                                                    <li>
                                                        Effettuata la query inversa, il nuovo stato della risorsa viene salvato alla chiave "graph"
                                                        della chiave &lt;t-1&gt; del dizionario degli snapshots. Questa operazione viene effettuata
                                                        ricorsivamente per ogni query di update n ritornata sul grafo della risorsa al tempo t-n+1.
                                                    </li>
                                                </ol>
                                            <li>
                                                <strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: nel momento in cui ho una serie
                                                di snapshot di un'entità, ogni serie potrebbe essere collegata ad altre entità, che a loro volta
                                                hanno una serie di snapshot, quindi devono essere riallineate temporalmente per fare la query
                                                corretta.
                                                <ol>
                                                    <li><i class="far fa-lightbulb"></i> Poniamo che l'utente voglia andare da una risorsa al tempo
                                                        t-n a un'altra risorsa collegata che sia sempre al tempo t-n. Lo snapshot corretto della
                                                        risorsa collegata è quello la cui proprietà pro:generatedAtTime è la minima antecedente o
                                                        coincidente a t-n.</li>
                                                    <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: è più conveniente
                                                        allineare le risorse al momento della richiesta dell'utente o pre-processare l'intero
                                                        dataset in modo da avere un nuovo dataset con le risorse allineate?</li>
                                                </ol>
                                            </li>
                                    </li>
                                </ol>
                                </li>
                                <li>La libreria dev'essere configurabile, pluggabile per usare metodi di altre librerie per semplificare alcuni
                                    compiti.</li>
                                <li>Cosa mi serve? Quali librerie mi servono in Python?
                                    <ol>
                                        <li>RDFLib per la manipolazione delle triple RDF.</li>
                                        <li>sparqlwrapper per leggere i dati da un triplestore.
                                            <ol>
                                                <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: devo prevedere la
                                                    possibilità di lavorare anche su file?</li>
                                            </ol>
                                        </li>
                                        <li>oc-ocdm, per mostrare un caso d'uso.</li>
                                    </ol>
                                </li>
                                <li>Bisogna realizzare un browser che utilizzi la libreria nel back-end.
                                    <ol>
                                        <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: solo un browser o anche un editor?
                                        </li>
                                    </ol>
                                </li>
                                </ol>
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Risolto il bug dell'updateQuery "DELETE DATA { GRAPH { .} }; INSERT DATA { GRAPH { .} }". Il bug era nel
                                        codice dell'editor, non in quello di oc-ocdm. La ragione del bug era che i caratteri riservati di HTML
                                        contenuti nella query di update non erano riportati come entità carattere e venivano interpretati dal
                                        browser come markup.</li>
                                    <li>Risolto un bug nel KG Editor per cui le proprietà oco:hasUpdateQuery e dc:description venivano visualizzate
                                        come link poiché contenevano l'URI base.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Il problema della funzione di merge non funzionante è più profondo del previsto. Ricapitolando, il merge
                                        deve reindirizzare su self le triple che hanno come oggetto other. È Per farla funzionare occorre modificare
                                        il design del preexisting graph così come è stato previsto dalla libreria, ovvero tutte le triple che hanno
                                        un'entità come soggetto. Anche aggiungendo al preexisting_graph le triple che hanno l'entità come oggetto,
                                        queste ultime vengono rimosse dal preexisting_graph, attraverso il seguente codice:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for p, o in preexisting_graph.predicate_objects(self.res):
            self.g.add((self.res, p, o))
            self.preexisting_graph.add((self.res, p, o))
                                                                            </code></pre>
                                        </div>
                                        Come si vede, vengono considerate solo le triple che hanno self come soggetto. Perché il merge funzioni, ho
                                        modificato il codice nel seguente modo:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for s, p, o in preexisting_graph.triples((None, None, None)):
            self.g.add((s, p, o))
            self.preexisting_graph.add((s, p, o))                                
                                                                                </code></pre>
                                        </div>
                                    </li>
                                    <li>Infine, ho modificato la funzione di merge, di cui riporto prima la versione attuale e poi quella
                                        modificata.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    # Versione attuale
    def merge(self, other: GraphEntity) -> None:
        for res, entity in self.g_set.res_to_entity.items():
            triples_list: List[Tuple] = list(entity.g.triples((res, None, other.res)))
            for triple in triples_list:
                entity.g.remove(triple)
                new_triple = (triple[0], triple[1], self.res)
                entity.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        # 'other' must be deleted AFTER the redirection of
        # triples pointing to it, since mark_as_to_be_deleted
        # also removes every triple pointing to 'other'
        other.mark_as_to_be_deleted()

    # Versione modificata
    def merge(self, other: GraphEntity) -> None:
        # Il ciclo non avviene sul grafo di self, ma di other, 
        # poiché è nel grafo di other che sono presenti le triple
        # che hanno other come oggetto. 
        for _, entity in other.g_set.res_to_entity.items():
            # res, entity -> _, entity, poiché res non viene utilizzata
            triples_list: List[Tuple] = list(entity.g.triples((None, None, other.res)))
            # Le triple considerate sono solo quelle che hanno other come oggetto
            for triple in triples_list:
                if triple[1] != URIRef("http://www.w3.org/ns/prov#specializationOf"):
                    # Questa condizione è stata aggiunta per evitare che venga 
                    # associata a self la provenance di other. 
                    # Sarebbe meglio riferirsi a quel predicato 
                    # come PROV.iri_specialization_of, ma non posso importare 
                    # la classe perché si verifica un'importazione circolare

                    # entity.g.remove(triple) è stato rimosso perché lo stesso
                    # compito è assolto da other.mark_as_to_be_deleted() in fondo
                    new_triple = (triple[0], triple[1], self.res)
                    self.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        other.mark_as_to_be_deleted()
                                                                            </code></pre>
                                        </div>
                                        Dopo queste modifiche, le triple che hanno come oggetto other vengono correttamente rendirizzate su self,
                                        eccetto le triple di provenance. Tuttavia, ci sono ancora due bug:
                                        <ul>
                                            <li>Self viene spogliata di tutti i suoi outgoing links eccetto il tipo.</li>
                                            <li>Other viene correttamente spogliata di tutti i suoi outgoing links, ma non è stato possibile
                                                cancellare gli incoming links. </li>
                                        </ul>
                                    </li>
                                </ol>
                                <h3>Proposta di modifica a oc-ocdm:</h3>
                                <ol>
                                    <li>Ho trovato e risolto i bug dei <em>remove_name</em>, <em>remove_given_name</em> e
                                        <em>remove_family_name</em> non funzionanti. Per usare un termine tecnico, era una <em>stronzata</em>.
                                        Riporto il codice con e senza il bug per quanto riguarda <em>remove_family_name</em>, ma vale lo stesso
                                        discorso anche le altre due funzioni:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    # Codice col bug: self.g non è il soggetto
    def remove_family_name(self) -> None:
        self.g.remove((self.g, GraphEntity.iri_family_name, None))
    
    # Codice fixato: self.g -> self.res
    def remove_family_name(self) -> None:
        self.g.remove((self.res, GraphEntity.iri_family_name, None))
                                                                        </code></pre>
                                        </div>
                                    </li>
                                </ol>
                            </div>
                            </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-11" data-target="#panel-11" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-11">
                            <span class="h6 mb-0 font-weight-bold">25/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-11">
                            <div class="pt-3">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>    
    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0.0/jquery.counterup.min.js" integrity="sha512-d8F1J2kyiRowBB/8/pAWsqUl0wSEOkG5KATkVV4slfblq9VRQ6MyDZVxWl2tWd+mPhuCbpTB4M7uU/x9FlgQ9Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.countdown/2.2.0/jquery.countdown.min.js" integrity="sha512-lteuRD+aUENrZPTXWFRPTBcDDxIGWe5uu0apPEn+3ZKYDwDaEErIK9rvR0QzUGmUQ55KFE2RqGTVoZsKctGMVw==" crossorigin="anonymous"></script>    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/smooth-scroll/16.1.3/smooth-scroll.polyfills.min.js" integrity="sha512-LZ6YBzwuQvIG41twjliX3HUVeAd+ErnJ0UsqRnkI4firX2l71jxbKJoax/hu7XY2tiyLl0YA2kcnz/XEW+9O3g==" crossorigin="anonymous"></script>  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jarallax/1.12.5/jarallax.min.js" integrity="sha512-DI98Iva+99hqdsd+etSf/W9iJcmz5jornxiWr5nkr/kcKWlaCDwIsWW6AGxXu/X5u/yylsLYJowdPzIcLUDklw==" crossorigin="anonymous"></script>  
    <script src="./assets/js/neumorphism.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>