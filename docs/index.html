<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Time agnostic</title>
    <link type="text/css" href="./css/neumorphism.css" rel="stylesheet">
    <link type="text/css" href="./css/custom.css" rel="stylesheet">
    <!-- Fontawesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" />
</head>

<body>
    <header class="header-global">
        <div class="nav-wrapper container position-relative">
            <ul class="nav nav-pills nav-fill flex-column flex-sm-row">
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0 active" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#next-meet">Prossimo incontro</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#past-meet">Archivio</a>
                </li>
            </ul>
        </div>
    </header>

    <main>
        <!-- Hero -->
        <section class="section section bg-soft pb-5 overflow-hidden z-2">
            <div class="container z-2">
                <div class="row justify-content-center text-center pt-6">
                    <div class="col-lg-8 col-xl-8">
                        <h1 class="display-2 mb-3">Time agnostic</h1>
                        <p class="lead px-md-6 mb-5">Diario di bordo del tirocinio presso <a
                                href="http://opencitations.net/" alt="Link to the OpenCitations main page"
                                target="_blank"><span class="oc-purple">Open</span><span
                                    class="oc-blue">Citations</span></a>.</p>
                        <div class="d-flex flex-column flex-wrap flex-md-row justify-content-md-center mb-5">
                            <a href="https://github.com/arcangelo7/time_agnostic" target="_blank"
                                class="btn btn-primary mb-3 mb-lg-0 mr-3" alt="Link to the repository"><i
                                    class="fab fa-github mr-2"></i>Time Agnostic Dataset</a>
                            <a href="https://github.com/arcangelo7/time-agnostic-browser" target="_blank"
                            class="btn btn-primary mb-3 mb-lg-0 mr-3" alt="Link to the repository"><i
                                class="fab fa-github mr-2"></i>Time Agnostic Browser</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section pb-5" id="next-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Prossimo incontro (11/08/2021)</h2>
                </div>
                <h3>Cosa ho fatto</h3>
                    <ol>
                        <li>Novità relative alla <strong>cache</strong>:
                            <ol>
                                <li>Per utilizzare la cache, l'utente passa l'url del triplestore da usare come cache nel nuovo campo "<strong>cache_triplestore_url</strong>" del file di configurazione, che di default ha come valore una stringa vuota. I metodi usati per gestire la cache sono generici, non specifici per Blazegraph. Pertanto, la cache è utilizzabile sia tramite la classe AgnosticQuery, che tramite la classe BlazegraphQuery che eredita dalla prima.</li>
                                <li>Vantaggi del meccanismo di cache:
                                    <ol>
                                        <li>Tutti i grafi del passato ricostruiti vengono salvati su triplestore e mai sulla RAM. L'impatto del processo sulla RAM in caso di molte entità è quindi abbattuto.</li>
                                        <li>Le query agnostiche sul tempo vengono lanciate sul triplestore di cache e non su dei grafi salvati in RAM, quindi sono ordini di magnitudo più veloci.</li>
                                        <li>Se una query viene lanciata una seconda volta, la storia delle entità rilevanti per quella query non viene ricostruita, ma ricavata dalla cache.</li>
                                    </ol>
                                </li>
                                <li>Svantaggi della cache:
                                    <ol>
                                        <li>Occupa spazio.</li>
                                        <li>Non permette di accelerare il processo di scoperta delle entità rilevanti per una query. Le variabili devono comunque essere risolte ogni volta. Se si tratta di variabili in triple isolate, per esempio, la ricerca su tutte le query di update va ripetuta.</li>
                                    </ol>
                                </li>
                                <li>L'implementazione della cache ha richiesto modifiche profonde, che non hanno riguardato solo l'aggiunta di nuovi metodi, ma anche un ripensamento di quelli già esistenti.
                                    <ol>
                                        <li>Metodi nuovi:
                                            <ol>
                                                <li><strong>_cache_entity_graph</strong>: dato un grafo e un timestamp, salva quel grafo nel triplestore in un named-graph il cui URI ha la forma "https://github.com/opencitations/time-agnostic-library/{timestamp}".</li>
                                                <li><strong>_cache_provenance</strong>: salva unicamente il nome dello snapshot collegato all'entità tramite prov:spacializationOf. Lo scopo è quello di poter verificare, in un secondo momento, quali snapshot sono già in cache e quali no.</li>
                                                <li><strong>_entity_in_cache</strong>: data un entità, restituisce True se il grafo di quell'entità è in cache, False altrimenti.</li>
                                                <li><strong>_store_relevant_timestamps</strong>: data un'entità, cerca nel dataset (non nella cache) i timestamp in cui quell'entità ha subito modifiche, quindi li salva. Lo scopo è restituire in output solo snapshot significativi per una certa query, non tutti gli snapshot del dataset.</li>
                                                <li>Metodo di supporto <strong>empty_the_cache</strong>, che permette di cancellare l'intera cache.</li>
                                            </ol>
                                        </li>
                                        <li>Per quanto riguarda i metodi preesistenti, alcuni sono stati modificati per poter funzionare sia con che senza cache:
                                            <ol>
                                                <li><strong>run_agnostic_query</strong>: in caso di cache, lancia la time-travel query su tutti i grafi della cache rilevanti per quella query, ognuno dei quali corrisponde a uno snapshot. In pratica, la query dell'utente viene modificata aggiungendo "FROM https://github.com/opencitations/time-agnostic-library/{timestamp}", dove timestamp è uno dei tempi rilevanti salvati tramite il già citato _store_relevant_timestamps.</li>
                                                <li><strong>_rebuild_relevant_entity</strong>: se un'entità è in cache, vengono salvati i suoi timestamp rilevanti e la funzione viene interrotta, saltando la ricostruzione dell'entità. Se non è in cache ma c'è una cache, l'entità viene ricostruita e salvata in cache. Se una cache non c'è, l'entità viene ricostruita e salvata in RAM.</li>
                                                <li><strong>_align_snapshots</strong>: all'interno del triplestore di cache, se c'è una cache, i grafi delle entità che non sono cambiate in uno snapshot in cui sono assenti vengono copiate in quello snapshot. Se non c'è una cache, il metodo opera come in passato.</li>
                                                <li><strong>_explicit_solvable_variables</strong>: se c'è una cache, le variabili in triple non isolate di una query agnostica sul tempo vengono esplicitate tramite il triplestore di cache.</li>
                                            </ol>
                                        </li>
                                        <li>Tutti i nuovi metodi sono stati testati.</li>
                                        <li>Tutte le query testate in passato senza cache sono state testate con la cache.</li>
                                        <li>Tutti i metodi già esistenti e modificati per gestire la cache sono stati ritestati senza la cache, con la cache vuota e con la cache piena.</li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                        <li>È adesso possibile effettuare <strong>query su una specifica versione</strong>. È sufficiente inizializzare la classe AgnosticQuery specificando un tempo.
                            <div class="card bg-primary shadow-inset border-light">
                                <pre><code class="prettyprint">  
    query = """
        PREFIX pro: &lt;http://purl.org/spar/pro/&gt;
        PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
        SELECT DISTINCT ?o
        WHERE {
            &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy ?o;
                rdf:type pro:RoleInTime.
        }
    """
    agnostic_query = AgnosticQuery(query, on_time="2021-05-31T18:19:47")
    agnostic_query.run_agnostic_query()
                                </code></pre>
                            </div>
                            <ol>
                                <li>Il tempo può essere indicato in qualunque formato standard e non deve essere necessariamente esistente tra gli snapshot. Se non esiste, viene considerato quello immediatamente precedente. Infine, se il campo on_time non viene specificato, la query avviene su tutte le versioni.</li>
                                <li>Non sono stati introdotti metodi nuovi per effettuare query su una versione, ma sono stati modificati quelli già esistenti.</li>
                                <li>Se il campo on_time è stato specificato, per ciascuna entità viene ricostruita solo quella specifica versione, non tutta la storia.</li>
                                <li>Tutte le query precedentemente testate come <em>time-travel queries</em> sono state nuovamente testate su una singola versione.</li>
                            </ol>
                        </li>
                        <li>È stato aggiunto il supporto per gli <strong>inverse property paths</strong>.</li>
                        <li>time-agnostic-library è ufficialmente una libreria scaricabile con <strong>pip</strong>! <a href="https://pypi.org/project/time-agnostic-library/1.0.0b0/" target="_blank">https://pypi.org/project/time-agnostic-library/1.0.0b0/</a></li>
                        <li>Il README è stato aggiornato con una guida a come installare e utilizzare la libreria. È anche disponibile una versione della documentazione su <a href="https://time-agnostic-library.readthedocs.io" target="_blank">https://time-agnostic-library.readthedocs.io</a>, che si auto ricostruisce a ogni push.</li>
                        <li>La versione 1.0.0-beta è su Zenodo con il seguente DOI <a href="https://doi.org/10.5281/zenodo.5172996" target="_blank">10.5281/zenodo.5172996</a>.</li>
                    </ol>
                <h3>Cosa non ho capito</h3>
                    <ol>
                        <li>Ho riflettuto su come si potrebbero implementare query sui delta, e mi sono accorto che in realtà ci sono dei modi per farle senza libreria. Consideriamo la query: "Voglio tutti gli id il cui DOI è stato modificato". Posso ottenere una risposta senza scrivere una singola linea di codice con la seguente query:
                            <div class="card bg-primary shadow-inset border-light">
                                <pre><code class="prettyprint">  
    prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
    prefix oco: &lt;https://w3id.org/oc/ontology/&gt;
    prefix prov: &lt;http://www.w3.org/ns/prov#&gt;
    SELECT DISTINCT ?id
    WHERE {
        ?se prov:specializationOf ?id;
            oco:hasUpdateQuery ?updateQuery.
        ?id a datacite:Identifier. 
        FILTER CONTAINS (?updateQuery, "http://www.essepuntato.it/2010/06/literalreification/hasLiteralValue")
    }
                                </code></pre>
                            </div>
                            <p>Cosa ne pensi?</p>
                        </li>
                        <li>Posso ottenere in Python un meccanismo di <em>object swap</em>? Per esempio, posso forzare l'utilizzo di una certa classe se mi accorgo che l'utente ha usato quella sbagliata? Poniamo che l'utente utilizzi per sbaglio la classe AgnosticQuery per sfruttare gli indici testuali di Blazegraph al posto della classe BlazegraphQuery. Posso aggiustare il tiro via codice?</li>
                        <li>Ho cercato i classificatori PyPI che più si addicessero alla mia libreria per quanto riguarda l'argomento (<a href="https://pypi.org/classifiers/" target="_blank">https://pypi.org/classifiers/</a>). Non mi sembra ce ne siano di davvero pertinenti, per cui ho scelto solo Libraries :: Python Modules. Ritieni che ce ne siano altri più appropriati?</li>
                        <li>Non capisco come usare i trattini per nominare una libreria Python. Le librerie vengono nominate solo con i trattini alti, installate con i trattini indifferentemente alti o bassi, importate solo con i trattini bassi.
                            <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                <img src="./assets/img/wat_meme.png" alt="Wat meme">
                            </div>
                        </li>
                    </ol>
            </div>
        </section>

        <section class="section pb-5" id="past-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Archivio</h2>
                </div>
                <div class="accordion shadow-soft rounded" id="accordionExample1">
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-1" data-target="#panel-1" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-1">
                            <span class="h6 mb-0 font-weight-bold">10/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-1">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                        <ol>
                                            <li>Peroni, S., Shotton, D., Vitali, F. (2016). <em>A document-inspired way for
                                                    tracking changes
                                                    of RDF data</em>. <a
                                                    href="https://w3id.org/oc/paper/occ-driftalod2016.html"
                                                    alt="Link to the paper">https://w3id.org/oc/paper/occ-driftalod2016.html</a>.
                                            </li>
                                            <li>Daquino, M., Peroni, S., Shotton D. (2020). <em>The OpenCitations Data
                                                    Model</em>. <a
                                                    href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                                    alt="Link to the OpenCitations Data Model documentation">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>
                                            </li>
                                            <li>Documentazione delle seguenti librerie e API: <em>Crossref REST API</em>,
                                                <em>REST API for
                                                    COCI</em>, <em>oc_ocdm</em>.
                                            </li>
                                            <li>Ho sfogliato il lavoro fin qui svolto da Arianna Moretti.</li>
                                        </ol>
                                    </li>
                                    <li>Ho scritto il seguente codice con lo scopo di comprendere il funzionamento di
                                        tutte le librerie e API su menzionate, nonché di produrre output utili in vista
                                        della realizzazione del dataset su "Scientometrics": <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/scientometrics.py"
                                            target="_blank" alt="Link to scientometrics.py">scientometrics.py</a>.</li>
                                    <div class="col-12 mt-3 mb-4">
                                        <div class="card bg-primary shadow-inset border-light">
                                            <div class="card-body p-5">
                                                <pre>
                                                    <code class="prettyprint">
    import requests, requests_cache, json
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.storer import Storer
    from oc_ocdm.support import create_date
    from rdflib import URIRef


    def get_journal_data(journal_issn, i_am_polite):
        journal_data = requests.get(url = f'http://api.crossref.org/journals/{{{journal_issn}}}/works?mailto={i_am_polite}')
        journal_data_json = journal_data.json()
        return journal_data_json


    def get_all_references_from_journal(journal_data_json):
        journal_data_items = journal_data_json["message"]["items"]
        all_references = list()
        for item in journal_data_items:
            references = requests.get(url = f'https://w3id.org/oc/index/coci/api/v1/references/{item["DOI"]}?format=json')
            references_json = references.json()
            all_references.append(references_json)
        return all_references


    def update_graph(journal_data_json, graphset):
        journal_data_items = journal_data_json["message"]["items"]
        for item in journal_data_items:
            try:
                # a volte gli articoli ritornati da crossref non hanno il campo "author". È molto raro, ma accade.
                responsible_agent_name = item["author"][0]["given"] + " " + item["author"][0]["family"]
                responsible_agent = scientometrics_graphset.add_ra(responsible_agent_name)
                responsible_agent.has_given_name(item["author"][0]["given"])
                responsible_agent.has_family_name(item["author"][0]["family"])
                # non ho ancora gestito il problema dell'omonimia, ma sono consapevole che vada gestito
                scientometrics_br = scientometrics_graphset.add_br(responsible_agent)
                scientometrics_br.has_title(item["title"][0])
                iso_date_string = create_date([item["published-print"]["date-parts"][0][0]])
                scientometrics_br.has_pub_date(iso_date_string)
            except KeyError:
                pass
        graphset.commit_changes()


    # Crossref test
    requests_cache.install_cache('cache')
    issn_web_scientometrics = "1588-2861"
    my_mail = "arcangelo.massari@studio.unibo.it"
    journal_data = get_journal_data(issn_web_scientometrics, my_mail)

    # REST API for COCI test
    all_references = get_all_references_from_journal(journal_data)

    # oc_ocdm test
    scientometrics_graphset = GraphSet("https://arcangelo7.github.io/time_agnostic/")
    update_graph(journal_data, scientometrics_graphset)

    # Retrieved data dump
    with open('data/scientometrics.json', 'w') as outfile:
        json.dump(journal_data, outfile)
    with open('data/references.json', 'w') as outfile:
        json.dump(all_references, outfile)
    storer = Storer(scientometrics_graphset)
    storer.store_graphs_in_file("data/graph.json", "./")                      
                                                    </code>
                                                </pre>
                                            </div>
                                        </div>
                                    </div>
                                    <li>Ho ottenuto in output tre file json:</li>
                                    <ol>
                                        <li>Una lista di 20 lavori (articoli, libri, atti di convegni, etc) presenti
                                            nella rivista "Scientometrics", con relativi metadati: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/scientometrics.json"
                                                alt="Link to scientometrics.json"
                                                target="_blank">scientometrics.json</a>.</li>
                                        <li>I dati citazionali per tutti i riferimenti in uscita relativi ai DOI di 20
                                            lavori presenti nella rivista "Scientometrics": <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/references.json"
                                                target="_blank" alt="Link to references.json">references.json</a>.</li>
                                        <li>Una lista di grafi conformi a OCDM v2.0.1 contenente 20 entità di tipo
                                            BibliographicResource relative a 20 lavori presenti in "Scientometrics", con
                                            metadati relativi a titolo e data di pubblicazione: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/graph.json"
                                                target="_blank" alt="Link to the graph">graph.json</a>.</li>
                                    </ol>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non posso accedere a <em>The OpenCitations Data Model</em> su SpringerLink (<a
                                            href="https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28"
                                            alt="Link to the OpenCitations Data Model docuemntation on Springer"
                                            target="_blank">https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28</a>)
                                        neanche tramite proxy Unibo. È diverso rispetto a quello pubblicato su figshare?
                                        (<a href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                            alt="Link to the OpenCitations Data Model documentation on Figshare">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>)
                                    </li>
                                    <li>Gli articoli di "Scientometrics" ritornati tramite GET a
                                        http://api.crossref.org/journals/{1588-2861}/works, dove 1588-2861 è l'ISSN web
                                        di "Scientometrics", sono 20 alla volta. Tramite il parametro rows è possibile
                                        aumentare questo numero a massimo 1000. Come faccio a ritornarli tutti?</li>
                                    <li>A quale ISSN di "Scientometrics" devo fare riferimento, quello web (1588-2861),
                                        quello stampa (0138-9130) o entrambi?</li>
                                    <li>Per creare una nuova entità di qualunque tipo, sia essa una Bibliographic
                                        Resource o una Citation, occorre passare in input al metodo corrispondente il
                                        responsible agent. Ad esempio, <code
                                            class="prettyprint">scientometrics_graphset.add_br(responsible_agent)</code>.
                                        Dato che l'OCDM prevede il Responsible agent come tipo, credo ci si riferisca ad
                                        esso. Su questo ho due dubbi:
                                        <ol>
                                            <li>Nel grafo finale non vedo il collegamento tra i Responsible Agents e le
                                                corrispondenti Bibliographic Resources. Entrambi vengono indicati da un
                                                URL nella forma [dataset URL][entity dataset identifier] - ad esempio
                                                https://arcangelo7.github.io/time_agnostic/ra/6 - ma il grafo non
                                                riporta qual è la Bibliographic Resource corrispondente. Fa fede il
                                                numero? Ovvero il Responsible Agent 6 è responsabile della Bibliographic
                                                Resource 6?</li>
                                            <li>Di alcuni autori viene riportato l'ORCID, di altri no. Suggerimenti su
                                                come gestire le omonimie?</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-2" data-target="#panel-2" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-2">
                            <span class="h6 mb-0 font-weight-bold">17/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-2">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho riorganizzato il codice utilizzando la programmazione a oggetti per rendere
                                        lo sviluppo più mantenibile.</li>
                                    <li>Utilizzando il parametro <em>cursor</em>, ho aumentato da 20 a 6065 il numero di
                                        works contenuti in "Scientometrics" ritornati tramite Crossref. Ora ci sono
                                        tutti.</li>
                                    <li>Ho generato 3 output corrispondenti a 3 grafi, di cui si elencano i rispettivi
                                        metadati considerati:
                                        <ol>
                                            <li>MetadataSet (dcat:Dataset)
                                            <ol>
                                                <li>Titolo (dcterms:title)</li>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>Data di modifica (dcterms:modified)</li>
                                            </ol></li>
                                            <li>Provenance (prov:Entity)
                                            <ol>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>is snapshot of (prov:specializationOf)</li>
                                                <li>Attribuzione (prov:wasAttributedTo)</li>
                                                <li>Data di creazione (prov:generatedAtTime)</li>
                                            </ol></li>
                                            <li>Graphset (set di grafi sulle entità bibliografiche)
                                            <ol>
                                                <li>BibliographicResource, per la rivista, i suoi articoli e gli articoli
                                                    citati dai suoi articoli (fabio:Expression, fabio:Journal,
                                                    fabio:JournalArticle)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Titolo (dcterms:title)</li>
                                                    <li>Sottotitolo, laddove presente (fabio:hasSubtitle)</li>
                                                    <li>Identificatore, DOI per gli articoli, ISSN per la rivista
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Se articolo, autore (pro:isDocumentContextFor)</li>
                                                    <li>Se articolo, data di pubblicazione, laddove presente
                                                        (prism:publicationDate)</li>
                                                    <li>Se articolo, rivista di appartenenza (frbr:partOf)</li>
                                                    <li>Formato (frbr:embodiment)</li>
                                                </ol></li>
                                                <li>Citation (cito:Citation, cito:JournalSelfCitation,
                                                    cito:AuthorSelfCitation, cito:DistantCitation)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Documento citante (cito:hasCitingEntity)</li>
                                                    <li>Documento citato (cito:hasCitedEntity)</li>
                                                    <li>Data di creazione (cito:hasCitationCreationDate)</li>
                                                    <li>Time span (cito:hasCitationTimeSpan)</li>
                                                </ol></li>
                                                <li>ResourceEmbodiment (fabio:manifestation, fabio:DigitalManifestation,
                                                    fabio:PrintObject)
                                                <ol>
                                                    <li>Tipo, laddove presente (rdf:type)</li>
                                                    <li>Media type, laddove presente (dcterms:format)</li>
                                                    <li>Pagina iniziale, laddove presente (prism:startingPage)</li>
                                                    <li>Pagina finale, laddove presente (prism:endingPage)</li>
                                                    <li>URL, laddove presente (frbr:exemplar)</li>
                                                </ol></li>
                                                <li>ResponsibleAgent (foaf:Agent)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Identificativo, ovvero ORCID, laddove presente
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Nome, laddove presente (foaf:givenName)</li>
                                                    <li>Cognome, laddove presente (foaf:familyName)</li>
                                                    <li>Nome completo, laddove presente (foaf:name)</li>
                                                </ol></li>
                                                <li>AgentRole (pro:RoleInTime)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di ruolo (pro:withRole)</li>
                                                    <li>Is held by (pro:isHeldBy)</li>
                                                </ol></li>
                                                <li>Identifier (datacite:Identifier)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di identificatore (datacite:usesIdentifierScheme)</li>
                                                    <li>Stringa (literalreification:hasLiteralValue)</li>
                                                </ol></li>
                                            </ol></li>
                                        </ol>
                                    </li>
                                    <li>La mappatura è avvenuta nel modo più generico e rivista-indipendente possibile.
                                        Dovrebbe essere possibile utilizzare lo stesso software per mappare seguendo
                                        l'ocdm qualunque rivista ritornata da Crossref.</li>
                                    <li>È stata implementata una barra di caricamento per mostrare all'utente quanto
                                        occorra aspettare per ricevere tutti i dati tramite le varie API.</li>
                                    <li>Il codice è disponibile alla seguente repository: <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py"
                                            target=_blank
                                            alt="Link to the Python script">https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py</a>.
                                        Purtroppo, non è stato possibile caricare gli output su GitHub date le loro
                                        dimensioni.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Come faccio a esprimere che un certo issue fa parte di un certo volume
                                        utilizzando l'OCDM? Una BibliographicResource ha come metodo has_number, che
                                        però è generico.</li>
                                    <li>Un AgentRole ha tra i suoi metodi has_next. A questo proposito, la
                                        documentazione di oc_ocdm dice:
                                        <blockquote class="blockquote ml-5 mt-3">The previous role in a sequence of
                                            agent
                                            roles of the same type associated with the same bibliographic resource (so
                                            as to define, for instance, an ordered list of authors).</blockquote>
                                        Tuttavia, il documento <em>The Open Citations Data Model</em> parla di
                                        "following role", non di "previous". Credo faccia quindi fede quest'ultimo. In
                                        ogni caso, non ho capito a cosa ci si sta riferendo, per ragioni affini a quelle
                                        della domanda successiva.
                                    </li>
                                    <li>Come faccio a collegare una BibliographicResource ai suoi ResponsibleAgents? È
                                        presente il metodo has_contributor, che però riceve come argomento un AgentRole,
                                        non un ResponsibleAgent. Devo quindi avere tanti AgentRoles quanti sono i
                                        ResponsibleAgents e usarli come intermediari tra la BibliographicResource e i
                                        ResponsibleAgents? Sembrerebbe di sì, dato che tale predicato viene mappato con
                                        pro:isDocumentContextFor, che presenta la stessa contorsione logica: è come se
                                        invece di dire "<em>The Open Citations Data Model</em> è scritto da Silvio
                                        Peroni" dicessi "<em>The Open Citations Data Model</em> è il contesto nel quale
                                        il ruolo di autore di Silvio Peroni si manifesta". </li>
                                    <li>Alcuni articoli di rivista hanno sia una data di pubblicazione a stampa, che una
                                        data per la pubblicazione digitale. Tuttavia, OCDM prevede un'unica data di
                                        pubblicazione attraverso il metodo has_pub_date. Quale uso delle due? Oltre a
                                        questo, come tipo di ResourceEmbodiment devo indicare sia il formato stampa che
                                        il formato digitale?</li>
                                    <li>Tra i predicati di una Citation c'è has_citation_characterization. A questo
                                        proposito, la documentazione dell'OCDM recita:
                                        <blockquote class="blockquote ml-5 mt-3">The citation function characterizing
                                            the
                                            purpose of the citation.</blockquote>
                                        Tuttavia, non ho capito cosa voglia dire.
                                    </li>
                                    <li>Ogni articolo di Scientometrics cita altri articoli sia della rivista stessa che
                                        di riviste esterne. Devo creare una BibliograficResource per ciascuna degli
                                        articoli esterni a Scientometrics?</li>
                                    <li>Crossref riporta le reference di ciascun articolo indicando, tra gli altri, la
                                        "key". Che cosa indica? Ad esempio:
                                        <pre>
                                            <code class="prettyprint">
{
    "DOI": "10.1126/science.280.5364.698",
    "author": "M. Heller",
    "doi-asserted-by": "crossref",
    "first-page": "698",
    "journal-title": "Science",
    <strong>"key"</strong>: "207_CR21",
    "unstructured": "Heller, M., H. Eisenberg (1998), Can Patents Deter Innovation? The Anticommons in Biomedical Research, Science, 280: 698\u2013701.",
    "volume": "280",
    "year": "1998"
}
                                            </code>
                                        </pre>
                                    </li>
                                    <li>I dati sulle reference riportati da Crossref e dall'API for COCI sono spesso
                                        uguali, ma non sempre: a volte il COCI riporta citazioni che Crossref non
                                        riporta e viceversa. Quale dei due fa fede? Entrambi?</li>
                                    <li>A causa dell'elevato numero di articoli, i tempi di risposta di Crossref e
                                        dell'API for COCI sono biblici, si parla di ore. Per questo motivo, una volta
                                        ottenuti i dati la prima volta, li ho messi in cache per non doverli più
                                        richiedere. Inoltre, attraverso un dizionario, ho evitato di chiedere due volte
                                        metadati già richiesti al COCI. Oltre a queste misure, c'è qualcosa che non so e
                                        che dovrei fare per rendere i tempi di esecuzione accettabili?</li>
                                    <li>Ammettendo che le esecuzioni dei GET a Crossref e COCI siano istantanee, la
                                        creazione del grafo vero e proprio avendo i dati occupa il 95% di 16 GB di RAM
                                        DDR4 a 3000 MHz e dura ugualmente uno sproposito (per rendere l'idea, aprire
                                        qualunque altro programma che non sia Python causa il crash dello script per out
                                        of memory). Inoltre, volendo esportare il risultato in json, questo pesa oltre
                                        400 MB. Non è quindi caricabile su GitHub, neanche usando Git Large File
                                        Storage, perché andrei velocemente oltre il GB mensile gratuito consentito. Con
                                        questa mole di dati diventa insomma veramente complicato lavorare. Consigli?
                                    </li>
                                    <li>Idealmente, mi sarebbe piaciuto creare un software che, preso in input un ISSN,
                                        ritornasse in output il grafo relativo a quella rivista rispettando l'OCDM.
                                        Tuttavia, a causa dei lunghi tempi d'attesa, e quindi del rischio di crash lungo
                                        il percorso, sono stato costretto a spezzettare il processo su più funzioni da
                                        chiamare manualmente. È possibile realizzare l'ideale?</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-3" data-target="#panel-3" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-3" id="d2303cnhc7p">
                            <span class="h6 mb-0 font-weight-bold">23/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-3">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                    <ol>
                                        <li>Peroni, S., Shotton, D., Vitali, F. (2012). Scholarly publishing and Linked
                                            Data: describing roles, statuses, temporal and contextual extents.
                                            Association for Computing Machinery, New York. Retrieved from <a
                                                href="https://doi.org/10.1145/2362499.2362502" alt="Link to the article"
                                                target="_blank">https://doi.org/10.1145/2362499.2362502</a>.</li>
                                        <ol>
                                            <li>Vi ho appreso con entusiasmo che dietro l'entità apparentemente contorta
                                                pro:RoleInTime si cela invece la possibilità di definire feature tempo e
                                                contesto-dipendenti e di effettuare query più profonde.</li>
                                        </ol>
                                        <li>Peroni, S., Shotton, D. (2012). FaBiO and CiTO: Ontologies for describing
                                            bibliographic resources and citations. Journal of Web Semantics, Volume 17,
                                            Pages 33-43.
                                            Retrieved from <a href="https://doi.org/10.1016/j.websem.2012.08.001"
                                                alt="Link to the article"
                                                target="_blank">https://doi.org/10.1016/j.websem.2012.08.001</a>.</li>
                                        <ol>
                                            <li>Ne ho tratto informazioni sulla genesi di FaBio E CiTO e in particolare
                                                sul valore del predicato cito:hasCitationCharacterization.</li>
                                        </ol>
                                        <li>Documentazione di Blazegraph.</li>
                                    </ol></li>
                                    <li>Il Graphset contiene adesso informazioni sui volumi e sugli issues,
                                        rappresentati come BibliographicResources rispettivamente di tipo JournalVolume
                                        e JournalIssue, aventi come predicati fabio:hasSequenceIdentifier e frbr:partOf.
                                    </li>
                                    <li>Gli AgentRoles (pro:RoleInTime) presentano adesso il predicato oco:hasNext,
                                        avente come oggetto il ruolo successivo in una sequenza di agent roles dello
                                        stesso tipo associati alla stessa risorsa bibliografica, allo scopo di definire,
                                        ad esempio, una lista ordinata di autori.</li>
                                    <li>La data di pubblicazione considerata (prism:publicationDate) è adesso quella
                                        contenuta nel campo "issued". Sono stati inoltre creati tanti ResourceEmbodiment
                                        quante sono le manifestazioni, indicando la pagina iniziale e finale per i
                                        formati a stampa, il media-type e l'URL per i formati digitali.</li>
                                    <li>Sono state aggiunte tante entità di tipo BibliographicReference quante sono le
                                        reference riportate da Crossref con un DOI. Il loro contenuto, proveniente dal
                                        campo "unstructured", è stato riportato tramite c40:hasContent. Ciascuna
                                        BibliographicReference è stata collegata alla rispettiva BibliographicResource
                                        tramite biro:references. Allo stesso tempo, ogni BiblographicResource è stata
                                        collegata alle rispettive BibliographicReferences tramie frbr:part.</li>
                                    <li>È stata create una nuova classe, DatasetAutoEnhancer, pensata per contenere
                                        metodi e proprietà che migliorino la qualità del dataset in maniera automatica.
                                        Al momento presenta:
                                    <ol>
                                        <li>il metodo merge_by_id, il quale unisce due entità nel caso in cui gli
                                            identificativi ad esse associate coincidano. Nella fattispecie, unisce due
                                            BibliographicResources con lo stesso DOI e due ResponsibleAgents con lo
                                            stesso ORCID.</li>
                                        <li>Il metodo privato _generate_snaphot, che si occupa di aggiornare il grafo
                                            sulla provenance registrando i delta rispetto a quello precedente, in
                                            particolare:
                                        <ol>
                                            <li>Crea lo snaphsot.</li>
                                            <li>Definisce la data di creazione (has_generation_time).</li>
                                            <li>Definisce di quale entità è lo snaphot (is_snapshot_of).</li>
                                            <li>Definisce l'azione di update effettuata tramite una SPARQL query
                                                (has_update_action). La SPARQL query è stata a sua volta generata
                                                tramite il metodo get_update_query del modulo support.query_utils di
                                                oc_ocdm.</li>
                                            <li>Associa allo snapshot precedente la data di invalidazione
                                                (has_invalidation_time).</li>
                                            <li>Collega lo snapshot a quello precedente (derives_from).</li>
                                            <li>Definisce la risorsa primaria da cui hanno origine i metadati nello
                                                snapshot, ovvero Crossref (has_primary_source).</li>
                                            <li>Definisce l'agente responsabile delle modifiche (has_resp_agent).</li>
                                        </ol></li>
                                    </ol></li>
                                    <li>È stata implementata una nuova classe, Support, che contiene i seguenti metodi:
                                    
                                    <ol>
                                        <li>zip_data, per comprimere i dati in un archivio.</li>
                                        <li>minify_json, per minificare i file json in modo che siano parsabili dal
                                            metodo load della classe Reader di oc_ocdm.</li>
                                        <li>measure_runtime, per misurare i tempi di esecuzione delle varie funzioni.
                                        </li>
                                        <li>dump_dataset, per conservare i dataset in un file json.</li>
                                        <li>dump_json, per salvare gli oggetti json in un file.</li>
                                    </ol>
                                </ol></li>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non mi è mai capitato per "Scientometrics", ma come mi comporto se viene
                                        indicato l'issue e non il volume? Creo un entità Volume vuota e le collego
                                        l'issue?</li>
                                    <li>
                                        <p>Con il seguente codice ho espresso la relazione oco:hasNext per tutti gli
                                            AgentRoles (riporto solo le parti del codice rilevanti)</p>
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <pre>
                                                <code class="prettyprint">
author_agent_roles = list()
for author in item["author"]:
        # [...]
        author_ar = journal_graphset.add_ar(your_orcid)
        author_ar.create_author()
        author_ar.is_held_by(item_ra)
        item_br.has_contributor(author_ar)
        author_agent_roles.append(author_ar)
for index, author_agent_role in enumerate(author_agent_roles):
        if index+1 &lt; len(author_agent_roles):
                author_agent_role.has_next(author_agent_roles[index+1])
                                                </code>
                                            </pre>
                                        </div>
                                        A livello di AgentRole, il grafo finale riporta correttamente il successivo
                                        AgentRole. Tuttavia, guardando le proprietà pro:isDocumentContextFor delle
                                        BibliographicResources, la lista risulta disordinata. Sto sbagliando io o è un
                                        bug della libreria?
                                    </li>
                                    <li>Nel caso in cui per un articolo venga riportata una sola pagina e non un
                                        intervallo di pagine, è corretto indicare quella pagina sia come pagina iniziale
                                        che come pagina finale? Perché OCDM prevede solo i predicati per gli intervalli
                                        di pagine e non per le pagine singole.</li>
                                    <li>Una BibliographiResource ha tra i suoi predicati has_edition. La documentazione
                                        riporta:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An identifier for one of several alternative editions of a particular
                                            bibliographic resource.
                                        </blockquote>
                                        Non ho capito a cosa ci si riferisca.
                                    </li>
                                    <li>Sono riuscito ad avviare il NanoSparqlServer di Blazegraph utilizzando il
                                        comando: <code
                                            class="prettyprint">java -server -Xmx4g -jar blazegraph.jar</code>.
                                        In questo modo, la workbench è diventata disponibile all'indirizzo
                                        http://localhost:9999/blazegraph/.
                                        Tuttavia, non ho capito cosa passare come triplestore_url al metodo upload_all
                                        della classe Storer di oc_ocdm. Ho provato a passare sia
                                        http://localhost:9999/blazegraph/ che http://localhost:9999/bigdata/sparql e
                                        l'output del metodo è stato in entrambi i casi True, il che sembrerebbe
                                        incoraggiante, ma attraverso l'endpoint SPARQL della workbench mi sono accorto
                                        di non aver caricato alcuna tripla. A quale URL devo fare il POST?
                                    </li>
                                    <li>Allo scopo di aumentare la provenance, ho inizialmente provato ad aprire il
                                        grafo precedentemente creato utilizzando i metodi load e
                                        import_entities_from_graph della classe Reader di oc_ocdm. Tuttavia, il secondo
                                        metodo ritorna una lista di GraphEntity, che non mi permette di utilizzare
                                        metodi propri della classe GraphSet, come get_id() o get_br(), costringendomi a
                                        ciclare su tutti gli elementi della lista e controllare manualmente di che
                                        classe sono istanza. Inoltre, per qualche motivo che ignoro, il metodo
                                        import_entities_from_graph richiede notevolmente più tempo che creare il grafo
                                        da zero. Ho quindi deciso di aumentare la provenance direttamente durante la
                                        creazione del GraphSet, ovvero tramite più commit successivi. È corretto?</li>
                                    <li id="d2303cnhc7">Per quanto riguarda la creazione di uno snapshot, quanto deve essere esplicitato
                                        e quanto viene fatto in automatico? Mi spiego meglio: quando unisco un'entità a
                                        un'altra tramite il metodo merge, tutto ciò che ruota attorno a quelle entità
                                        rimane invariato. Ad esempio, se unisco due risorse bibliografiche dopo aver
                                        scoperto che hanno lo stesso DOI, le rispettive entità di tipo Identifier non
                                        vengono unite a loro volta. Confermi che io lo debba fare manualmente? Allo
                                        stesso tempo, la risorsa che è scomparsa dopo essere stata unita continua a
                                        essere richiamata in altre parti del grafo anche se non esiste più.</li>
                                    <li>Quando faccio un merge devo generare uno snapshost sia per la risorsa unita che
                                        per quella contenitore? Perché ho notato che così facendo si creano snapshot
                                        duplicati.</li>
                                    <li>Tra i metadati di uno snapshot c'è la sua descrizione. Per ottenerla devo
                                        parsare la stringa con la SPARQL query. Posso certamente farlo io da zero, ma
                                        non vorrei reinventare la ruota. Esistono delle librerie per farlo? Ne ho
                                        trovate alcune ma mi sembrano molto amatoriali.</li>
                                    <li>
                                        Allo scopo di ottenere i DOI a partire dal contenuto delle
                                        BibliographicReferences - ovvero il campo "unstructured" riportato da Crossref -
                                        ho provato a effetturare delle query su Crossref. Per fare un esempio:
                                        <pre class="mt-3">
            <code class="prettyprint">https://api.crossref.org/works?query.bibliographic=Carberry%2C+Josiah.+%E2%80%9CToward+a+Unified+Theory+of+High-Energy+Metaphysics%3A+Silly+String+Theory.%E2%80%9D+Journal+of+Psychoceramics+5.11+%282008%29%3A+1-3.</code>
                                        </pre>
                                        Dopodiché, ho controllato che il campo "score" contenesse un numero maggiore di
                                        90 e, se sì, ho estratto il DOI dall'item. A questo proposito ho tre domande:
                                    
                                    <ol>
                                        <li>Come funziona lo score? Perché ho notato che a volte risultati con score
                                            superiore a 100 sono sbagliati.</li>
                                        <li>La query non ritorna solo il risultato migliore, ma un numero variabile di
                                            hits più o meno pertinenti. Come faccio a ottimizzare la ricerca perché
                                            ritorni solo il risultato più pertinente? Ho provato con il parametro
                                            enable-multiple-hits impostato a false (peraltro di default), ma senza
                                            successo.</li>
                                        <li>È questo il metodo migliore per ottenere un DOI da una reference non
                                            strutturata?</li>
                                    </ol></li>
                                    <li>Persiste il problema della memoria. Ho provato ad aggiungere due banchetti che
                                        avevo in un altro PC e portare la RAM a 32 GB, ho attivato l'X.M.P., l'ho
                                        overclockata e ho portato il file di paging a 8 GB, ma non è bastato, il
                                        processo di creazione ed esportazione dei grafi l'ha nuovamente saturata.
                                        Riporto uno screen del messaggio di errore:
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/memory_error.png" alt="memory error image">
                                        </div>
                                        Utilizzando Jupyer Notebook e il monitoraggio risorse, ho potuto eseguire i vari
                                        passaggi singolarmente, per misurare quanta RAM occorresse a ciascuno:
                                        <ol>
                                            <li>
                                                <p>Creare il graphset ha occupato circa 18.4 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_graphset.png"
                                                        alt="RAM left after GraphSet creation">
                                                </div>
                                            </li>
                                            <li>
                                                <p>Creare il grafo sulla provenance ha portato la memoria occupata a
                                                    circa 26.8 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_prov.png"
                                                        alt="RAM left after Provenance creation">
                                                </div>
                                            </li>
                                            <li>Infine, il dump del dataset sulla provenance ha fatto crashare il
                                                programma.</li>
                                        </ol>
                                        Sono infine riuscito a risolvere portando il file di paging a 16 GB, ma in tutta
                                        onestà non la ritengo una soluzione valida, né filosoficamente né guardando al
                                        futuro prossimo in cui i dati saranno molti di più. Cosa ne pensi?
                                    </li>
                                    <li>Durante l'ultimo incontro hai fatto due riferimenti che sono rimasti in sospeso:
                                        <ol>
                                            <li>A proposito di OpenCitations Meta, hai menzionato un articolo che
                                                introduce a OpenCitations. Ho verificato di non averlo letto. Potresti
                                                inviarmelo?</li>
                                            <li>Hai menzionato alcune questioni di cui discutere dopo la risposta alle
                                                domande, che però non sono più state discusse.</li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Note</h3>
                                <p>La documentazione del metodo import_entities_from_graph della classe Reader di
                                    oc_ocdm non è aggiornata. Il metodo vuole infatti tre argomenti obbligatori, non
                                    due: il GraphSet, il Graph e il responsible agent. Quest'ultimo non viene menzionato
                                    dalla documentazione.</p>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-4" data-target="#panel-4" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-4">
                            <span class="h6 mb-0 font-weight-bold">31/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-4">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho caricato dati e provenance su Blazegraph.</li>
                                    <li>Ho implementato:
                                        <ol>
                                            <li>un sistema di gestione delle richieste tramite api più raffinato, con un timeout, un numero
                                                massimo di tentativi separati da intervalli progressivamente crescenti tramite backoff factor e
                                                con la possibilità di salvare gli errori in un file di log;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di verificare l'esistenza di
                                                entità associate allo stesso id. Nel caso le trovi, recupera i grafi associati e li unisce;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di ottenere tutti i DOI degli
                                                articoli appartenenti a una determinata rivista e i DOI di tutti gli articoli da essi citati.
                                                Dopodiché, controlla se su COCI sono presenti altri DOI di articoli citati e, se li trova,
                                                recupera il grafo dell'entità citante e gli aggancia le varie entità relative alla risorsa
                                                citata, ovvero l'Identifier, la BibliographicResource e la Citation (data di creazione,
                                                timespan, journal o author self-citation). Per quanto riguarda i DOI degli articoli citati già
                                                presenti sul triplestore, vengono aggiunte alle rispettive entità Citation il timespan e se sono
                                                autocitazioni della rivista o dell'autore;</li>
                                            <li>una gestione più sofisticata di volume, issue e articoli:
                                                <ol>
                                                    <li>se ci sono sia volume che issue l'articolo fa parte dell'issue, l'issue del volume e il
                                                        volume della rivista;</li>
                                                    <li>se c'è solo il volume, l'articolo fa parte del volume e il volume della rivista;</li>
                                                    <li>se c'è solo l'issue, l'articolo fa parte dell'issue e l'issue della rivista;</li>
                                                    <li>se non ci sono né issue né volume l'articolo fa parte della rivista. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho aperto i seguenti issue:
                                        <ol>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/4" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/4</a></li>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/5" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/5</a></li>
                                        </ol>
                                    </li>
                                    <li>Ho letto:
                                        <ol>
                                            <li>International DOI Foundation. (2019). DOI® Handbook. https://doi.org/10.1000/182.
                                                <ol>
                                                    <li>Sì, ho seriamente letto tutto l'handbook.</li>
                                                </ol>
                                            </li>
                                            <li>Fecher, B., & Friesike, S. (2014). Open Science: One Term, Five Schools of Thought. In S.
                                                Bartling & S. Friesike (Eds.), Opening Science (pp. 17–47). Springer International Publishing.
                                                https://doi.org/10.1007/978-3-319-00026-8_2.</li>
                                            <li>Kramer, B., & Bosman, J. (2015, June 18). The good, the efficient and the open—Changing research
                                                workflows and the need to move from Open Access to Open Science. CERN Workshop on Innovations in
                                                Scholarly Communication (OAI9), University of Geneva, Geneva, Switzerland.
                                                https://www.slideshare.net/bmkramer/the-good-the-efficient-and-the-open-oai9.</li>
                                            <li>Woelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature
                                                Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149.</li>
                                            <li>UNESCO. (2020). First draft of the UNESCO Recommendation on Open Science (Programme and Meeting
                                                Document SC-PCB-SPP/2020/OS/R1; p. 16). https://unesdoc.unesco.org/ark:/48223/pf0000374837.</li>
                                            <li>Documentazione di rdflib.</li>
                                        </ol>
                                    </li>
                                    <li>Ho seguito il corso Semantic Web Technologies del Prof. Harald Sack, reperibile all'indirizzo <a
                                            href="https://open.hpi.de/courses/semanticweb" target="_blank"
                                            alt="Link to the Semantic Web course">https://open.hpi.de/courses/semanticweb</a>, al fine di
                                        approfondire la mia conoscenza di SPARQL 1.1.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Allo scopo di ottenere tutte le triple di cui un'entità è soggetto da un triplestore, oc_ocdm fornisce
                                        il metodo import_entity_from_triplestore della classe Reader. Ecco un esempio di come ho provato a
                                        utilizzarla:
                                        <pre><code class="prettyprint">
    g = GraphSet("https://github.com/arcangelo7/time_agnostic/")
    qres = Reader().import_entity_from_triplestore(g, "http://localhost:9999/blazegraph/sparql", URIRef("https://github.com/arcangelo7/time_agnostic/br/1"),"https://orcid.org/0000-0002-8420-0696")
                                                            </code></pre>
                                        Nonostante l'entità esista ottengo però un ValueError, con messaggio di errore: "The required entity was
                                        not found or was not recognised as a proper OCDM entity". Dico che l'entità esiste perché facendo il
                                        medesimo CONSTRUCT tramite endpoint SPARQL ottengo i risultati attesi. Sto utilizzando correttamente il
                                        metodo? Ho infine risolto il problema utilizzando la classe SPARQLStore di rdflib o SPARQLWrapper, ma
                                        visto che oc_ocdm fornisce già uno shortcut preferirei utilizzare quello.
                                    </li>
                                    <li>La documentazione della classe SPARQLStore di rdflib, reperibile a <a
                                            href="https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.plugins.stores.html#rdflib.plugins.stores.sparqlstore.SPARQLStore"
                                            target="_blank" alt="SPARQLStore class documentation">questo indirizzo</a>, afferma:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An RDFLib store around a SPARQL endpoint
                                            This is context-aware and should work as expected
                                            when a context is specified.
                                            For ConjunctiveGraphs, reading is done from the "default graph". Exactly
                                            what this means depends on your endpoint, because SPARQL does not offer a
                                            simple way to query the union of all graphs as it would be expected for a
                                            ConjuntiveGraph. This is why we recommend using Dataset instead, which is
                                            motivated by the SPARQL 1.1.
                                        </blockquote>
                                        Non ho capito cosa intenda dire.
                                    </li>
                                    <li>Dopo avere unito le entità associate agli stessi id è necessario caricare le modifiche sul triplestore.
                                        Dal codice di oc_ocdm ho inteso che la libreria si occupa di elaborare la query di update, definendo
                                        cosa vada cancellato e cosa aggiunto, ma dai tentativi fatti mi continuano a risultare 0 modifiche, per
                                        cui la query non viene lanciata. Sospetto c'entri qualcosa il metodo commit_changes(), il cui
                                        funzionamento mi risulta però oscuro.</li>
                                    <li>Non sono riuscito ad aggiornare la provenance a partire da un grafo di provenance preesistente.</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-5" data-target="#panel-5" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-5" id="d0704cnhc1p">
                            <span class="h6 mb-0 font-weight-bold">07/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-5">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Le query SPARQL precedentemente effettuate tramite SPARQLStore utilizzano adesso SPARQLWrapper.</li>
                                    <li>Grafi di provenance e grafi sui dati vengono adesso generati in base alla storia pregressa tramite
                                        un
                                        indice.</li>
                                    <li>Effettuati i miglioramenti automatici, le nuove triple e loro provenance vengono adesso caricate
                                        correttamente sul triplestore.</li>
                                    <li>Grazie alla spiegazione fornitami da Simone, sono riuscito a importare correttamente un grafo da
                                        file
                                        json tramite metodo import_entities_from_graph della classe Reader di oc_ocdm.</li>
                                    <li>Sono state implementate due nuove funzioni, merge_by_id_from_file e add_coci_data_from_file, che
                                        anziché
                                        lavorare a partire da un triplestore lavorano a partire da un file. Di conseguenza, è possibile
                                        ottenere
                                        in output un nuovo file modificato anziché un upload su un triplestore. In questo modo potrò sia
                                        avere
                                        un file di backup che inviare a Cristian dei file rdf customizzati in base alle sue esigenze.</li>
                                    <li>Cristian mi ha fatto notare che avevo considerato solo gli autori come AgentRole. Ora il grafo
                                        iniziale
                                        comprende anche i publisher.</li>
                                    <li>Risolti alcuni bug nella funzione add_coci_data_from_triplestore:
                                        <ol>
                                            <li>Le entità di tipo cito:Citation preesistenti venivano prima ricercate solo tramite il DOI
                                                name
                                                del citato e non anche tramite il DOI name del citante.</li>
                                            <li>Le nuove entità venivano prima generate senza tenere conto dell'ultimo indice, provocando
                                                sovrapposizioni.</li>
                                            <li>Specificare l'argomento res nella ricreazione di un grafo preesistente permette adesso di
                                                prescindere dalla funzione di merge poiché, anche se presenti più entità associate ai
                                                medesimi
                                                identificativi, viene riprodotto solo il grafo dell'entità di interesse corrente.</li>
                                            <li>Nel caso in cui la funzione venga lanciata più volte sul medesimo triplestore, è stato
                                                aggiunto
                                                un controllo per cui le stesse informazioni non vengano aggiunte più volte.</li>
                                        </ol>
                                    </li>
                                    <li>Risolto un bug in merge_by_id_from_triplestore per cui le entità venivano unite a loro stesse.</li>
                                    <li>Aggiunta una nuova funzione di supporto che prende in input il percorso di un file rdf e restituisce
                                        in
                                        output un oggetto di tipo GraphSet.</li>
                                    <li>Tutte le funzioni hanno adesso parametri e output tipati, in osservanza di <a
                                            href="https://www.python.org/dev/peps/pep-3107/" alt="Link to PEP 3107" target="_blank">PEP
                                            3107</a>, allo scopo di aumentare la leggibilità del codice, sia per gli altri che per me. Si
                                        tratta
                                        quindi di semplici annotazioni, che non causeranno un'eccezione TypeError se non rispettate.</li>
                                    <li>Il vantaggio del lavorare con una certa mole di dati è che, mentre gli script vengono eseguiti, si
                                        ha
                                        molto tempo per leggere:
                                        <ol>
                                            <li>Belhajjame, K., B’Far, R., Cheney, J., Coppens, S., Cresswell, S., Gil, Y., Groth, P.,
                                                Klyne,
                                                G., Lebo, T., McCusker, J., Miles, S., Myers, J., Sahoo, S., & Tilmes, C. (2013). PROV-DM:
                                                The
                                                PROV Data Model (L. Moreau & P. Missier, Eds.). World Wide Web Consortium.
                                                https://www.w3.org/TR/prov-dm/.</li>
                                            <li>Wolfe, M. (2017, August 9). CC0 and Data Citation.
                                                https://www.library.ucdavis.edu/news/cc0-and-data-citation/.</li>
                                            <li>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A.,
                                                Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A.
                                                J.,
                                                Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons,
                                                B.
                                                (2016). The FAIR Guiding Principles for scientific data management and stewardship.
                                                Scientific
                                                Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18.</li>
                                            <li>GO FAIR. (2018). FAIR Principles. https://www.go-fair.org/fair-principles/.</li>
                                            <li>Open Knowledge Foundation. (2015). Open Definition 2.1.
                                                https://opendefinition.org/od/2.1/en/.
                                            </li>
                                            <li>Landi, A., Thompson, M., Giannuzzi, V., Bonifazi, F., Labastida, I., da Silva Santos, L. O.
                                                B.,
                                                & Roos, M. (2020). The “A” of FAIR – As Open as Possible, as Closed as Necessary. Data
                                                Intelligence, 2(1–2), 47–55. https://doi.org/10.1162/dint_a_00027.</li>
                                            <li>Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M.,
                                                L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V.,
                                                Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST
                                                Principles for digital repositories. Scientific Data, 7(1), 144.
                                                https://doi.org/10.1038/s41597-020-0486-7.</li>
                                            <li>Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS
                                                Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525.</li>
                                            <li>Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to
                                                uniquely identify researchers. Learned Publishing, 25(4), 259–264.
                                                https://doi.org/10.1087/20120404.</li>
                                            <li>Meng, X.-L. (2020). Reproducibility, Replicability, and Reliability. Harvard Data Science
                                                Review, 2(4). https://doi.org/10.1162/99608f92.dbfce7f9.</li>
                                            <li>Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604),
                                                452–454.
                                                https://doi.org/10.1038/533452a.
                                                <ol>
                                                    <li>Vince il premio lettura più sconcertante della settimana, una di quelle che cambia
                                                        il
                                                        modo di vedere le cose. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="d0704cnhc1">Talvolta, provando a fare un merge tra due BibliographicResources, ottengo un errore
                                        di tipo TypeError
                                        che dice:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            [BibliographicResource.contains_discourse_element] Expected argument type: de. Provided argument
                                            type: BibliographicReference.
                                        </blockquote>
                                        Poiché non sto in alcun modo utilizzando entità di tipo DiscourseElement, sospetto che si tratti di
                                        un
                                        bug.
                                    </li>
                                    <li>Portata a termine l'operazione di merge sulle entità associate agli stessi id e caricate le novità
                                        sul
                                        triplestore, ho riscontrato le stesse stranezze nell'output già riportate sul diario di bordo in
                                        data <a href="#d2303cnhc7">23/03 (<em>Cosa non ho capito</em>, punto 7)</a>. Da quello che ho visto,
                                        se
                                        unisco A a B, ad A viene associato l'id di B e B viene cancellata. Il problema è che A si ritrova ad
                                        essere associata a due entità di tipo datacite:Identifier che in realtà sono identiche, perché
                                        appunto A
                                        e B sono state unite in quando associate agli stessi id. Inoltre, l'entità B, nonostante sia stata
                                        cancellata, continua ad avere degli ingoing links da parte di tutte quelle entità che ad essa si
                                        riferivano in passato. C'è qualcosa del metodo merge che non sto capendo o qualcosa che dovrei fare
                                        e
                                        non sto facendo?</li>
                                    <li>È preferibile avere funzioni distinte che lavorino su triplestore e file o è meglio avere un'unica
                                        funzione che, a seconda degli argomenti passati, lavora indifferentemente su entrambi?</li>
                                    <li>Non so quanto sia ottimale effettuare query SPARQL su un'entità Graph di rdflib, perché credo manchi
                                        delle ottimizzazioni di un triplestore. Quando mi hai detto che devo lavorare sia su triplestore che
                                        su
                                        file intendevi che devo produrre entrambi gli output o che devo creare funzioni che lavorino in
                                        entrambe
                                        le modalità?</li>
                                    <li>Cristian mi ha detto che le librerie con cui lavora per produrre graph embeddings tendono a prendere
                                        in
                                        input un file tsv e non json. Tramite rdflib posso serializzare rdf in una moltitudine di notazioni,
                                        quali turtle, nt, json-ld, rdf/xml ed n3, ma onestamente non ho mai visto un grafo rdf serializzato
                                        in
                                        tsv. Si può fare?</li>
                                    <li>Mi piacerebbe portare avanti nuove idee per quanto riguarda possibili miglioramenti automatici del
                                        grafo, poiché quelle che ho al momento non mi convincono:
                                        <ol>
                                            <li>Correggere i DOI names sbagliati. Dato che è materia d'esame e che coinvolge altre 3 persone
                                                non
                                                mi sembra corretto occuparmene da solo per il tirocinio.</li>
                                            <li>Strutturare i campi unstructured forniti da Crossref precedentemente non considerati. Per
                                                farlo
                                                dovrei utilizzare nuovamente l'API di Crossref, mentre vorrei capire se è possibile
                                                migliorare
                                                ulteriormente quello che ho già senza chiedere altre risorse esterne.</li>
                                            <li>Arricchire i metadati delle entità citate, che mi convince poco per le stesse ragioni del
                                                punto
                                                precedente, ovvero che dovrei rivolgermi all'API di Crossref.</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-6" data-target="#panel-6" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-6">
                            <span class="h6 mb-0 font-weight-bold">14/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-6">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho implementato l'algoritmo <strong>add_reference_data_without_doi</strong> che, preso in input un
                                        file json contenente l'output di Crossref con le informazioni sui lavori di una determinata rivista,
                                        controlla quali reference non hanno DOI name e hanno altri metadati oltre all'unstructured.
                                        Dopodiché, chiama il metodo privato <strong>_generate_crossref_query_from_metadata</strong>, che
                                        trasforma i metadati in una stringa di query da lanciare tramite API di Crossref. Se la ricerca va a
                                        buon fine, seleziona il primo risultato, cerca sul triplestore l'entità citante corrente, verifica
                                        che il DOI name della citata trovata non esista già (FILTER NOT EXISTS) e solo allora crea
                                        Identifier, BibliographicResource e Citation corrispondenti e li aggiunge al preexisting graph
                                        dell'entità citante corrente. </li>
                                    <li>Ho implementato il metodo <strong>add_crossref_reference_data</strong>, che arricchisce il dataset
                                        corredando le risorse bibliografiche citate di numerose informazioni tratte da Crossref, quali
                                        publisher, titolo, sottotitolo, volume, issue, data di pubblicazione, resource embodiment e autori,
                                        ciascuna correttamente mappata seguendo l'OCDM. </li>
                                    <li>Il metodo <strong>merge_by_id_from_triplestore</strong> unisce adesso anche le entità di tipo
                                        datacite:Identifier associate alle entità unite sulla base dell'identificativo.</li>
                                    <li>È stato implementato il metodo <strong>_get_entity_and_ids_from_res</strong>, che si occupa di
                                        restituire un'entità e la lista delle entità degli id associati dato un URIRef e un triplestore.
                                    </li>
                                    <li>Il metodo <strong>_manage_br_type</strong> si occupa adesso di assegnare a ciascuna risorsa
                                        bibliografica il tipo corretto, tra book, book-chapter, component, journal-article, monograph,
                                        other, posted-content, proceeding-article, report e report-series. </li>
                                    <li>Sono stati estratti dal metodo generate_graph tre nuovi metodi privati statici, i quali sono stati
                                        riutilizzati dal metodo add_crossref_reference_data:
                                        <ol>
                                            <li><strong>_manage_author_ra_ar</strong>;</li>
                                            <li><strong>_manage_volume_issue</strong>;</li>
                                            <li><strong>_manage_resource_embdiment</strong>.</li>
                                        </ol>
                                    </li>
                                    <li>Prima di avventurarmi nella scrittura di un programma di editing interattivo per Knowledge Graph, ho
                                        esplorato la letteratura esistente sull'argomento:
                                        <ol>
                                            <li>Wright J., Rodríguez Méndez S.J., Haller A., Taylor K., Omran P.G. (2020) Schímatos: A
                                                SHACL-Based Web-Form Generator for Knowledge Graph Editing. In: Pan J.Z. et al. (eds) The
                                                Semantic Web – ISWC 2020. ISWC 2020. Lecture Notes in Computer Science, vol 12507. Springer,
                                                Cham. https://doi.org/10.1007/978-3-030-62466-8_5.
                                                <ol>
                                                    <li>L'articolo introduce <strong>Schímatos</strong>, un editor di KG interattivo, che si
                                                        occupa della validazione degli input utilizzando SHACL Shapes Constraint Language
                                                        (https://www.w3.org/TR/shacl/). Non sembra esserci però un modo immediato per
                                                        riutilizzare il software con KG che non siano WikiData. Sembra infatti essere
                                                        soltanto una demo a scopo dimostrativo, per quanto molto interessante.</li>
                                                </ol>
                                            </li>
                                            <li>Heyvaert P., Dimou A., Verborgh R., Mannens E., Van de Walle R. (2016) Graph-Based Editing
                                                of Linked Data Mappings Using the RMLEditor. In: Sack H., Rizzo G., Steinmetz N., Mladenić
                                                D., Auer S., Lange C. (eds) The Semantic Web. ESWC 2016. Lecture Notes in Computer Science,
                                                vol 9989. Springer, Cham. https://doi.org/10.1007/978-3-319-47602-5_25.
                                                <ol>
                                                    <li><strong>RMLEditor</strong> offre una GUI per consentire ai data publishers, esperti
                                                        di dominio, di modellare la conoscenza derivata da dati di origini multiple ed
                                                        eterogenee. RMLEditor utilizza RML come linguaggio di mappatura sottostante. Ad oggi
                                                        è presente una demo che permette di modellare fino a 20 nodi per un massimo di 2 MB
                                                        per singolo file. </li>
                                                </ol>
                                            </li>
                                            <li>Ho letto la documentazione e ho provato a usare <strong>Web-Karma</strong> (<a
                                                    href="https://github.com/usc-isi-i2/Web-Karma" target="_blank"
                                                    alt="Web-Karm documentation">https://github.com/usc-isi-i2/Web-Karma</a>), un tool per
                                                modificare un KG in base a un'ontologia specificata. Problema: non ha un sistema di
                                                validazione rispetto all'ontologia ed è possibile modificare l'ontologia stessa,
                                                intenzionalmente o per errore. In sintesi, utilizzabile solo da chi conosce già molto bene
                                                il data model. </li>
                                        </ol>
                                    </li>
                                    <li>Ho letto la documentazione di <strong>Flask</strong>, allo scopo di utilizzare Python per il
                                        back-end della mia applicazione di KG editing. Ho anche letto la documentazione di
                                        <strong>Jinja</strong>, dato che è il template-engine predefinito di Flask.</li>
                                    <li>Utilizzando Flask per il server, Jinja2 come motore di template e Blazegraph come database ho
                                        implementato una prima interfaccia grafica che mostra dieci triple. </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Oltre ai due bug del metodo merge di oc_ocdm già rilevati in <a href="#d0704cnhc1">07/04/2021 (Cosa
                                            non ho capito, punti 1 e 2)</a> credo di averne trovato un terzo specifico per il merge tra
                                        entità di tipo datacite:Identifier. Ho miniaturizzato e commentato il codice utilizzato al fine di
                                        rendere l'errore più facile da comprendere e riprodurre. Inoltre, mi piacerebbe capire se io stia
                                        procedendo nel modo corretto prima di mandare un'altra mail a Simone.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre>
                                                                    <code class="prettyprint">
    from oc_ocdm.graph.graph_entity import GraphEntity
    from oc_ocdm.graph import GraphSet
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, JSON, RDFXML


    def get_entity_from_res(res:URIRef, graphset:GraphSet) -> GraphEntity:
        res = URIRef(res)
        query = f"""
            CONSTRUCT {{&lt;{res}&gt; ?p ?o}}
            WHERE {{&lt;{res}&gt; ?p ?o}}
        """
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        results = sparql.query().convert()
        preexisting_graph = Graph().parse(data=results.serialize(format='xml'), format='xml')
        entity = graphset.add_id(resp_agent="https://orcid.org/0000-0002-8420-0696", res=res, preexisting_graph=preexisting_graph)
        return entity

    # Estrai Identifier e DOI name di tutte le br dal triplestore
    queryString = """
        PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
        PREFIX fabio: &lt;http://purl.org/spar/fabio/&gt;
        PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
        SELECT ?id ?literalValue
        WHERE
        {{
            ?s a fabio:Expression;
                datacite:hasIdentifier ?id.
            ?id literal:hasLiteralValue ?literalValue.
        }}
    """
    sparql = SPARQLWrapper("http://localhost:9999/bigdata/sparql")
    sparql.setQuery(queryString)
    sparql.setReturnFormat(JSON)
    results = sparql.query().convert()
    dois_found = dict()
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    for result in results["results"]["bindings"]:
        # Se il DOI name è già stato trovato e l'Identifier è diverso da quello già trovato
        if result["literalValue"]["value"] in dois_found and result["id"]["value"] != dois_found[result["literalValue"]["value"]]:
            id_duplicated_entity = get_entity_from_res(result["id"]["value"], graphset)
            id_preexisting_entity = get_entity_from_res(dois_found[result["literalValue"]["value"]], graphset)
            print(f"Merging entity {id_preexisting_entity.res} with {id_duplicated_entity.res}")
            id_preexisting_entity.merge(id_duplicated_entity)
        else:
            # Registra di aver trovato questo DOI name e che è associato a questo Identifier
            dois_found[result["literalValue"]["value"]] = result["id"]["value"]
                                                                </code>
                                                            </pre>
                                        </div>
                                        Se a questo punto provo a fare l'upload attraverso il metodo upload_all passando il graphset,
                                        l'upload non avviene a causa di un errore nella formattazione della query di upload. In particolare,
                                        pare che manchi un ";" prima di un INSERT.
                                    </li>
                                    <li>Per qualche ragione a volte l'update_query vuota non è "" ma None. Il metodo upload_all della classe
                                        Storer di oc_ocdm non gestiste il caso in cui l'update_query sia None, sollevando un'eccezione nel
                                        momento in cui viene fatta la seguente concatenazione: query_string += " ; " + update_query, dato
                                        che non è possibile concatenare una stringa con None. Ho risolto in locale aggiungendo:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre>
                                                                <code class="prettyprint">
    # for idx, entity in enumerate(self.a_set.res_to_entity.values()):
        # update_query, n_added, n_removed = get_update_query(entity, entity_type=self._class_to_entity_type(entity))
            if update_query == "" or update_query is None:
                skipped_queries += 1
                                                                </code>
                                                            </pre>
                                        </div>
                                        Tuttavia, dato che l'update_query vuota dovrebbe sempre essere "" e non None, credo ci sia un altro
                                        bug a monte.
                                    </li>
                                    <li>Non ho trovato documentazione su tutti i possibili metadati delle reference di un work restituite da
                                        Crossref, quindi ho scoperto tramite un algoritmo che, sui quasi 7000 articoli citanti di
                                        Scientometrics, i citati posso riportare le chiavi sotto elencate. Di fianco a ciascuna viene
                                        indicato tramite quale notazione sono state usate nella query a Crossref al fine di recuperare il
                                        DOI name:
                                        <ul>
                                            <li>journal-title → query.bibliographic=value&</li>
                                            <li>author → query.author=value&</li>
                                            <li>journal-title → query.container-title=value&</li>
                                            <li>ISBN → filter=isbn:value</li>
                                            <li>year → filter=from-index-date:value,
                                                <ul>
                                                    <li>Perché proprio from-index-date? Perché nella documentazione si legge:
                                                        <blockquote class="blockquote ml-5 mt-3">
                                                            When using time filters to retrieve periodic, incremental metadata updates, the
                                                            from-index-date filter should be used over from-update-date, from-deposit-date,
                                                            from-created-date and from-pub-date. The timestamp that from-index-date filters
                                                            on is guaranteed to be updated every time there is a change to metadata
                                                            requiring a reindex.
                                                        </blockquote>
                                                    </li>
                                                </ul>
                                            </li>
                                        </ul>
                                        Tuttavia, ci sono molti altri campi per i quali Crossref non sembra non fornire né field queries né
                                        filtri, ovvero:
                                        <ul>
                                            <li>issue</li>
                                            <li>volume-title</li>
                                            <li>series-title</li>
                                            <li>edition</li>
                                            <li>key</li>
                                            <li>first-page</li>
                                            <li>volume</li>
                                            <li>isbn-type</li>
                                        </ul>
                                        Le domande a questo punto sono:
                                        <ol>
                                            <li>Ti risulta che io non possa fare query a Crossref utilizzando quelle informazioni?</li>
                                            <li>È possibile utilizzare volume-title e series-title come valore della field query
                                                <em>query.container-title</em>?</li>
                                            <li>I risultati ottenuti finora sono sorprendentemente buoni, vale la pena restringere
                                                ulteriormente la ricerca?</li>
                                        </ol>
                                    <li>Cristian mi ha detto che per il suo progetto un'informazione rilevante è l'affiliazione degli
                                        autori. Credo che l'OCDM non la preveda, giusto?</li>
                                    <li>Se una risorsa bibliografica viene indicata da Crossref come di tipo "posted-content" con quale tipo
                                        di BibliographicResource presente nell'OCDM devo mapparla? </li>
                                    </li>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-7" data-target="#panel-7" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-7">
                            <span class="h6 mb-0 font-weight-bold">21/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-7">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Novità relative al <strong>Knowledge Graph Editor</strong>:
                                        <ol>
                                            <li>È stata aggiunta una textarea che permette di effettuare query SPARQL.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_textarea.png" alt="SPARQL textarea">
                                                </div>
                                            </li>
                                            <li>Se la query richiede molto tempo, un'icona di caricamento comunica all'utente lo stato
                                                interno del sistema.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_loading.png" alt="SPARQL loading">
                                                </div>
                                            </li>
                                            <li>L'output della query viene mostrato in una tabella, in cui ogni colonna rappresenta una
                                                variabile della query stessa. Le entità vengono riportate come link, mentre non è possibile
                                                interagire con i predicati, con gli oggetti di tipo literal e in generale con gli oggetti il
                                                cui URI non contiene l'URI base del dataset.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_results.png" alt="SPARQL results">
                                                </div>
                                            </li>
                                            <li>Cliccando su un'entità, vengono mostrati tutti gli outgoing links e tutti gli incoming links
                                                relativi a quell'entità.
                                                <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/sparql_entity.png" alt="SPARQL entity">
                                                </div>
                                            </li>
                                            <li>Il pulsante "Edit" permette di accedere alla modalità di modifica.</li>
                                        </ol>
                                    </li>
                                    <li>Novità relative al metodo per ottenere i DOI names delle reference riportate da Crossref senza DOI.
                                        <ol>
                                            <li>Il metodo <strong>add_reference_data_without_doi</strong> non utilizza più data di
                                                pubblicazione e ISBN per fare query su Crossref, mentre usa volume-title e series-title
                                                oltre al journal-title.</li>
                                            <li>Sono state aggiunti diversi metodi il cui scopo complessivo è quello di calcolare il
                                                <strong>matching score</strong> tra due dizionari di metadati, ovvero quelli sintentici e
                                                grezzi riportati da Crossref nel campo "reference" e quelli approfonditi risultanti da una
                                                query. L'algoritmo è stato mutuato dai capitoli 3.2 and soprattuttutto dall'appendice
                                                dell'articolo <em>Large-scale comparison of bibliographic data.
                                                    sources: Scopus, Web of Science, Dimensions,
                                                    Crossref, and Microsoft Academic</em> (Visser, Martijn et al., 2005), reperibile al
                                                seguente indirizzo: <a href="https://arxiv.org/abs/2005.10732" alt="Link to the paper"
                                                    target="_blank">https://arxiv.org/abs/2005.10732</a>. La formula adottata, rispetto a
                                                quella originale, non considera i DOI names, poiché, date le premesse, non sono mai presenti
                                                nei metadati di partenza.
                                                <ol>
                                                    <li>Il metodo <strong>_do_heuristic_match</strong> combina gli score derivanti dai
                                                        singoli match in un unico score finale.</li>
                                                    <li>Il metodo <strong>_levenshtein_distance</strong> calcola la distanza di Levenshtein,
                                                        ovvero il minimo numero di modifiche (cancellazioni, aggiunte e sostituzioni)
                                                        necessarie per trasformare una stringa in un'altra. È stato adottato l'algoritmo
                                                        iterativo basato su una matrice, poiché le soluzioni ricorsiva e tramite tecnica di
                                                        dynamic programming richiedevano innumerevoli passaggi in più, risultando meno
                                                        efficienti.
                                                        <p>Questa funzione vince il premio algoritmo più entusiasmante della settimana, se
                                                            non del mese. È talmente bella che la riporto nel diario:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    import numpy as np

    def levenshtein_distance(target:str, source:str) -> int:
        target = [k for k in "#" + target]
        source = [k for k in "#" + source]
        sol = np.zeros(shape=(len(source), len(target)))
        sol[0] = [j for j in range(len(target))]
        sol[:,0] = [j for j in range(len(source))]
        if target[1] != source[1]:
            sol[1,1] = 2
        for c in range(1, len(target)):
            for r in range(1, len(source)):
                if target[c] != source[r]:
                    sol[r,c] = min(sol[r-1,c], sol[r,c-1]) + 1
                else:
                    sol[r,c] = sol[r-1,c-1]
        min_edit_distance = int(sol[-1,-1])
        return min_edit_distance
                                                                            </code></pre>
                                                        </div>
                                                    </li>
                                                    <li>
                                                        <p>Il metodo <strong>_match_first_author</strong> compara le iniziali del nome e il
                                                            cognome dei primi autori dei due lavori, tramite la seguente formula:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    𝑚<sub>firstauthor</sub> = 0.8 − 0.8𝐷(𝑙<sub>A</sub>, 𝑙<sub>B</sub>) ⁄ max(𝐿(𝑙<sub>A</sub>), 𝐿(𝑙<sub>B</sub>)) + 0.2𝐸(𝑓<sub>A, 𝑓<sub>B</sub>)
                                                                                </code></pre>
                                                        </div>
                                                        <p>Dove l<sub>X</sub> ed f<sub>X</sub> denotano rispettivamente il cognome e le
                                                            iniziali del nome del primo autore del documento X. L(a) è uguale alla lunghezza
                                                            della stringa. D(a,b) è la distanza di Levenshtein tra la stringa a e la stringa
                                                            b. E(a,b) è uguale a 1 se a è uguale a b, altrimenti 0. Dunque,
                                                            𝑚<sub>firstauthor</sub> è uguale a 1 se i primi autori dei documenti A e B
                                                            hanno gli stessi cognomi e iniziali dei nomi.</p>
                                                    </li>
                                                    <li>
                                                        <p>Il metodo <strong>_match_title</strong> compara i titoli dei due lavori, tramite
                                                            la seguente formula:</p>
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    𝑚<sub>title</sub> = 1 − 𝐷(𝑡<sub>A</sub>, 𝑡<sub>B</sub>) ⁄ max(𝐿(𝑡<sub>A</sub>), 𝐿(𝑡<sub>B</sub>))
                                                                                </code></pre>
                                                        </div>
                                                        <p>Anche in questo caso, 𝑚<sub>title</sub> è uguale a 1 se i titoli dei documenti A
                                                            e B sono identici. </p>
                                                    </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho aperto il seguente issue: <a href="https://github.com/opencitations/oc_ocdm/issues/8"
                                            alt="Link to the GitHub issue on oc-ocdm"
                                            target="_blank">https://github.com/opencitations/oc_ocdm/issues/8</a>.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        <p>Per effettuare un'operazione di aggiunta, cancellazione e modifica sulle triple del dataset
                                            attraverso il KGEditor, devo necessariamente utilizzare oc_ocdm. Se lanciassi query di update in
                                            autonomia, infatti, perderei tutte le informazioni contenute negli indici del dataset e della
                                            provenance, oltre al fatto che dovrei riscrivere lo stesso codice già scritto da Simone per
                                            effettuare le medesime operazioni. Problema: come dico a Python quale metodo di oc_ocdm invocare
                                            per fare un delete, un create o un update? Mi spiego meglio con un esempio: poniamo che l'utente
                                            voglia eliminare una delle triple in figura cliccando sull'apposito pulsante "-":</p>
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/sparql_remove.png" alt="SPARQL remove entity">
                                        </div>
                                        <p>Ad esempio, potrebbe voler eliminare la prima tripla:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    &lt;https://github.com/arcangelo7/time_agnostic/br/1&gt; &lt;http://purl.org/spar/datacite/hasIdentifier&gt; &lt;https://github.com/arcangelo7/time_agnostic/id/1&gt;
                                            </code></pre>
                                        </div>
                                        <p>Niente di più facile, oc_ocdm mette a disposizione il metodo remove_identifier a questo scopo.
                                            Tuttavia, il pulsante "-" è generico e l'unica informazione per capire quale metodo di
                                            cancellazione invocare è il predicato <http: //purl.org/spar/datacite/hasIdentifier>. Come
                                                faccio dal predicato a risalire a quale metodo di cancellazione utilizzare? Il problema è il
                                                medesimo anche per aggiungere e modificare. Devo creare un enorme dizionario che mappi ogni
                                                predicato con il suo metodo? A mio parere, una soluzione più pulita sarebbe un unico metodo
                                                delete generico per una bibliographic_entity che prende in input qualunque oggetto e
                                                cancella la tripla.</p>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-8" data-target="#panel-8" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-8">
                            <span class="h6 mb-0 font-weight-bold">28/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-8">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <p>Tutte le novità di questa settimana sono relative al <strong>Knowledge Graph Editor</strong>.</p>
                                <ol>
                                    <li>È stato creato un file di configurazione in formato json, che mappa diversi tipi di entità e predicato
                                        con i relativi metodo di creazione e cancellazione di oc-ocdm. È possibile visualizzarlo a questo
                                        indirizzo: <a
                                            href="https://raw.githubusercontent.com/arcangelo7/time_agnostic/main/KGEditor/static/config/config.json"
                                            target="_blank"
                                            alt="Link to the config file">https://raw.githubusercontent.com/arcangelo7/time_agnostic/main/KGEditor/static/config/config.json</a>.
                                        Se ne riporta qui un'anteprima.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    "http://purl.org/spar/fabio/Expression": {
        "add": "add_br",
        "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": {
            "create": "",
            "delete": "remove_type"
        },
        "http://purl.org/spar/datacite/hasIdentifier": {
            "create": "has_identifier",
            "delete": "remove_identifier"
        },
        "http://purl.org/dc/terms/title": {
            "create": "has_title",
            "delete": "remove_title"
        },
        "http://purl.org/spar/fabio/hasSubtitle": {
            "create": "has_subtitle",
            "delete": "remove_subtitle"
        },
        "http://purl.org/vocab/frbr/core#partOf": {
            "create": "is_part_of",
            "delete": "remove_is_part_of"
        },
        "http://purl.org/spar/cito/cites": {
            "create": "has_citation",
            "delete": "remove_citation"
        },
        "http://prismstandard.org/namespaces/basic/2.0/publicationDate": {
            "create": "has_pub_date",
            "delete": "remove_pub_date"
        },
        "http://purl.org/vocab/frbr/core#embodiment": {
            "create": "has_format",
            "delete": "remove_format"
        },
        "http://purl.org/spar/fabio/hasSequenceIdentifier": {
            "create": "has_number",
            "delete": "remove_number"
        },
        "http://purl.org/vocab/frbr/core#part": {
            "create": "contains_in_reference_list",
            "delete": "remove_contained_in_reference_list"
        },
        "http://purl.org/spar/pro/isDocumentContextFor": {
            "create": "has_contributor",
            "delete": "remove_contributor"
        }
    }
                                                                        </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        La funzione di cancellazione delle triple è ora funzionante. Tramite la <strong>reflection</strong>,
                                        viene dinamicamente invocato il metodo appropriato in base al soggetto e predicato selezionati
                                        dall'utente per la cancellazione. In particolare, viene utilizzata la funzione <strong>getattr</strong>
                                        per invocare il metodo vero e proprio. Nel caso in cui questo metodo voglia un argomento, viene passata
                                        l'entità relativa all'oggetto. Per capire quanti parametri siano richiesti dal metodo è stata usata la
                                        funzione <strong>signature</strong> del modulo inspect di Python.
                                    </li>
                                    <li>
                                        Cliccando sul pulsante di cancellazione, la tripla viene decorata con un line-through e il bottone
                                        diventa un pulsante di aggiunta per annullare l'azione precedente. Finché non viene cliccato il pulsante
                                        "Done", tutte le query vengono salvate in un dizionario. Cliccando sul "+" viene cancellata dal
                                        dizionario l'operazione di cancellazione corrispondente. Lo schema delle chiavi del dizionario di update
                                        è "soggetto + predicato + oggetto".
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/sparql-line-through.png" alt="line-through">
                                        </div>
                                    </li>
                                    <li>
                                        È stato introdotto il pulsante di <strong>update</strong>, contraddistinto dall'icona della matita,
                                        poiché le modifiche sono reversibili finché non si preme il tasto "Done". Quest'ultimo, al contrario, è
                                        contraddistinto dall'icona della penna a inchiostro, poiché causa la modifica permanente del dataset e
                                        della provenance. Cliccando sul pulsante di update, la tripla da testo diventa input (textarea nel caso
                                        di valori che contengono spazi) e si può modificare. Così facendo, la funzione di cancellazione viene
                                        disabilitata. Viceversa, la funzione di cancellazione disabilita quella di update. Una volta completata
                                        la modifica, è possibile premere il pulsante con l'icona del "check" per salvare la relativa query, che
                                        non verrà effettuata fino al click sul tasto "Done".
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-update.png" alt="editor update">
                                        </div>
                                    </li>
                                    <li>
                                        È stato introdotta la funzionalità per <strong>creare</strong> una nuova tripla. Cliccando sul pulsante
                                        "+" di fianco a un'entità si apre un modale, nel quale è possibile inserire un nuovo predicato e un
                                        nuovo oggetto per quell'entità. L'input del predicato è dotato di una funzione di auto-completamento,
                                        che si basa sul medesimo file di configurazione utilizzato per mappare soggetti e predicati con relativi
                                        metodi di aggiunta e cancellazione di oc-ocdm.
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-create.png" alt="editor create.png">
                                        </div>
                                    </li>
                                    <li>
                                        Nel caso in cui l'utente non si sia ancora autenticato tramite ORCID, cliccando sul pulsante "Edit"
                                        viene aperto un modale per specificare il <strong>responsible agent</strong>.
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/editor-ra.png" alt="editor responsible agent.png">
                                        </div>
                                        Il valore inserito dev'essere necessariamente un ORCID. Il campo viene infatti validato tramite la
                                        seguente espressione regolare.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    /(https?:\/\/orcid.org\/)?([0-9]{4})-([0-9]{4})-([0-9]{4})-([0-9]{4})/i
                                                                        </code></pre>
                                        </div>
                                        Una volta specificato il responsible agent, viene generato un <strong>cookie</strong> e l'ORCID viene
                                        salvato nella <strong>sessione</strong>, per cui non verrà più chiesto.
                                    </li>
                                    <li>Le <strong>triple</strong> di <strong>provenance</strong> sono adesso <strong>immutabili</strong>. Nel
                                        contesto di un'entità di provenance il pulsante di modifica non appare, mentre nel contesto di un
                                        qualunque altro tipo di entità non compaiono i pulsanti di modifica e cancellazione di fianco agli
                                        incoming links legati alla provenance.</li>
                                    <li>Risolto un bug per cui la rotella di caricamento girava all'infinito in caso di errori nella query o di
                                        database irraggiungibile. </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Implementando l'operazione di update del KGEditor mi sono nuovamente imbattuto nel bug delle query
                                        malformate, ovvero prive di ";" tra un'operazione di update e l'altra. L'update consiste infatti di due
                                        operazione, una di cancellazione e una di aggiunta. In questo modo ho potuto astrarre il problema e scrivere del codice per riprodurlo, che riporto qui sotto:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_br(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    br_16 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/16"), graphset=graphset)
    br_19 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/19"), graphset=graphset)
    br_99 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/br/99"), graphset=graphset)
    br_16.remove_citation(br_19)
    br_16.has_citation(br_99)
    storer = Storer(graphset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")                                </code></pre>
                                        </div>
                                        Purtroppo non sarà possibile parlarne con Simone, perché ha già pubblicato la versione 6.0.0 di oc_ocdm
                                        e mi ha detto che non metterà più mano al codice.
                                    </li>
                                </ol>
                            </div>
                        </div>
                        </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-9" data-target="#panel-9" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-9">
                            <span class="h6 mb-0 font-weight-bold">13/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-9">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>È stata ultimata la funzione <strong>add_reference_data_without_doi</strong>, che aggiunge al grafo
                                        le reference senza DOI, dopo aver trovato il DOI tramite query a Crossref e match euristico dei
                                        risultati.
                                        <ol>
                                            <li>È stata implementata la funzione <strong>_match_other</strong>, che calcola il match
                                                basandosi sull'anno di pubblicazione, il volume, l'issue e la pagina iniziale (quella finale
                                                non è mai indicata nel campo reference, <em>for reasons</em>), secondo la seguente formula:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    match<sub>other</sub> = 0.1𝐸(𝑦<sub>A</sub>, 𝑦<sub>B</sub>) + 0.2𝐸(𝑣<sub>A</sub>, 𝑣<sub>B</sub>) + 0.1𝐸(𝑖<sub>A</sub>, 𝑖<sub>B</sub>) + 0.6𝐸(𝑏<sub>A</sub>, 𝑏<sub>B</sub>)</code></pre>
                                                </div>
                                                dove y<sub>x</sub>, v<sub>x</sub>, i<sub>x</sub> e b<sub>x</sub> denotano, rispettivamente,
                                                l'anno di pubblicazione, il numero del volume, il numero dell'issue e la pagina iniziale del
                                                documento x. E(a,b) è uguale a 1 se a e b sono identici, 0 in caso contrario. La formula
                                                originale proposta da Martijn Visser (<a
                                                    href="https://arxiv.org/ftp/arxiv/papers/2005/2005.10732.pdf" target="_blank" ,
                                                    alt="Link to the Martijn Visser paper">https://arxiv.org/ftp/arxiv/papers/2005/2005.10732.pdf</a>)
                                                prevede anche la pagina finale e assegna un moltiplicatore di 0.3 sia alla pagina finale che
                                                a quella finale: per far fronte all'assenza della pagina finale, si è assegnato un
                                                moltiplicatore di 0.6 a quella iniziale, in modo che il risultato finale oscilli tra 0 e 1.
                                            </li>
                                            <li>
                                                È stata implementata la funzione <strong>_is_a_match</strong>, che combina i match basati
                                                sul primo autore, sul titolo, sulla fonte e sugli altri valori in un unico score finale,
                                                secondo la seguente formula:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    similarity<sub>A,B</sub> = 7 * match_first_author + 14 * match_title + 5 * match_source + 14 * match_other</code></pre>
                                                </div>
                                                La formula originale prevedeva anche l'aggiunta di 15 punti per il match tra il DOI della
                                                fonte e del target, valore che è stato rimosso perché il DOI della fonte non è mai
                                                disponibile, essendo l'incognita che si intende individuare. Per questo motivo, è stato
                                                ridotto il <em>threshold</em> oltre il quale viene stabilito un match, che nel paper di
                                                Martijn Visser era fissato a 30, mentre nel nostro caso è di 15, ovvero 30 meno i 15 punti
                                                del match tra DOI.
                                            </li>
                                        </ol>
                                    </li>
                                    <li>
                                        Ho generato i seguenti livelli di provenance:
                                        <ol>
                                            <li>Livello 0: creazione del grafo.</li>
                                            <li>Livello 1: aggiunta delle reference presenti in COCI e non nel grafo del livello 0, quindi
                                                creazione dei rispettivi br, id e ci. Se la reference era già presente nel livello 0,
                                                aggiunta del timespan e del tipo di citazione (journal self-citation o author
                                                self-citation).</li>
                                            <li>Livello 2: arricchimento dei dati delle reference tramite Crossref, in particolare aggiunta
                                                della rivista di origine (br, id, ra, ar), aggiunta del tipo di br della reference, del
                                                titolo, sottotitolo, data di pubblicazione, autore (ra, ar), volume, issue e resource
                                                embodiment.</li>
                                            <li>Livello 3 (<em>in progress</em>, è al 28% alle 11:36 del 12/05/2021): aggiunta di alcune
                                                citazioni andate perse poiché riportate da Crossref senza DOI.</li>
                                            <li>Ogni livello di provenance è stato salvato sia sul triplestore che in due file json (uno per
                                                i dati, l'altro per la provenance) . Inoltre, sono stati creati dei backup di ogni singolo
                                                stato del database, dei file json e degli info_dir.</li>
                                            <li><strong>Nota bene</strong>: i livelli di provenance attuali sono temporanei. Servono
                                                soltanto ad avere una base su cui lavorare per la creazione del Time agnostic browser.
                                                Quando la funzione di merge sarà funzionante, essa verrà lanciata dopo ogni livello, in modo
                                                da ottimizzare i processi successivi, che dovendo lavorare su meno entità impiegheranno meno
                                                tempo e meno RAM.</li>
                                        </ol>
                                    </li>
                                    <li>
                                        Novità sul <strong>Knowledge Graph Editor</strong>:
                                        <ol>
                                            <li>La funzione di update è ora pienamente funzionante. Simone ha infatti corretto il bug delle
                                                query malformate.</li>
                                            <li>L'editor crea e aggiorna i grafi di provenance.</li>
                                            <li>L'editor gestisce la creazione e l'update dei valori letterali degli identificatori.</li>
                                            <li>È adesso possibile creare e modificare il tipo delle entità di tipo pro:RoleInTime,
                                                fabio:Expression, foaf:Agent, fabio:Manifestation, datacite:Identifier,
                                                biro:BibliographicReference e cito:Citation.</li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        La funzione di merge è stata modificata, ma i bug sono aumentati. Ora non solo ci sono tutti quelli
                                        di cui abbiamo discusso (identificatori non uniti, incoming links a entità cancellate), ma ce n'è
                                        uno nuovo, ovvero che, se A e B vengono uniti, A viene privata di tutti i suoi outgoing links meno
                                        il tipo. Ti mostro il codice com'è adesso e provo a dare una spiegazione del perché non funziona.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    def merge(self, other: GraphEntity) -> None:
        # Here we must REDIRECT triples pointing
        # to 'other' to make them point to 'self':
        for res, entity in self.g_set.res_to_entity.items():
            triples_list: List[Tuple] = list(entity.g.triples((res, None, other.res)))
            for triple in triples_list:
                entity.g.remove(triple)
                new_triple = (triple[0], triple[1], self.res)
                entity.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        # 'other' must be deleted AFTER the redirection of
        # triples pointing to it, since mark_as_to_be_deleted
        # also removes every triple pointing to 'other'
        other.mark_as_to_be_deleted()
                                                            </code></pre>
                                        </div>
                                        La parte problematica è "triples_list = list(entity.g.triples((res, None, other.res)))". L'idea è
                                        quella di ottenere tutte le triple che puntano a other, in modo da poterle reindirizzare su self.
                                        Tuttavia la lista triples_list sarà pressoché sempre vuota, perché viene riempita da triple che da
                                        self puntano a other, condizione che non si verifica quasi mai. In generale, non è possibile
                                        risalire alle triple che hanno come oggetto self o other in questo modo, perché il preexisting_graph
                                        contiene solo le triple che hanno self o other come soggetto, mai come oggetto.
                                    </li>
                                    <li>
                                        Credo ci sia un bug in oc-ocdm per quanto riguarda la rimozione di triple che hanno come soggetto
                                        un'entità di tipo foaf:Agent. Ad esempio, i metodi remove_name, remove_given_name,
                                        remove_family_name non fanno alcunché. Purtroppo, il codice scritto da Simone mi sembra corretto,
                                        non sono riuscito a capire cosa causi l'errore. Riporto del codice utile a riprodurre il problema:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_ra(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
    ra_399 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/ra/339"), graphset=graphset)
    ra_399.remove_family_name()
    storer = Storer(graphset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")
                                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        Credo ci sia un bug in oc-ocdm relativo alla creazione del grafo di provenance di entità di tipo
                                        pro:RoleInTime e fabio:Expression. Qualunque sia la modifica all'entità, lo snapshot riporta come
                                        update query la seguente stringa: "INSERT DATA { GRAPH { .} }". Riporto del codice utile a
                                        riprodurre il problema.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.prov import ProvSet
    from oc_ocdm.storer import Storer
    from rdflib import URIRef, Graph
    from SPARQLWrapper import SPARQLWrapper, RDFXML
    
    def get_entity_from_res(res:URIRef, graphset:GraphSet, ts_url:str="http://localhost:9999/blazegraph/sparql", resp_agent="https://orcid.org/0000-0002-8420-0696"):
        query = f"""
            CONSTRUCT {{<{res}> ?p ?o}}
            WHERE {{<{res}> ?p ?o}}
        """
        sparql = SPARQLWrapper(ts_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(RDFXML)
        data = sparql.query().convert()
        graph = Graph().parse(data=data.serialize(format='xml'), format='xml')
        entity = graphset.add_ar(resp_agent=resp_agent, res=res, preexisting_graph=graph)
        return entity
    
    graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/")
    ar_338 = get_entity_from_res(res=URIRef("https://github.com/arcangelo7/time_agnostic/ar/338"), graphset=graphset)
    ar_338.create_publisher()
    storer = Storer(graphset)
    provset = ProvSet(prov_subj_graph_set=graphset, base_iri="https://github.com/arcangelo7/time_agnostic/", info_dir="./data/info_dir/prov/")
    provset.generate_provenance()
    storer_prov = Storer(provset)
    storer.upload_all("http://localhost:9999/blazegraph/sparql")       
    storer_prov.upload_all("http://localhost:9999/blazegraph/sparql")
                                                            </code></pre>
                                        </div>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-10" data-target="#panel-10" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-10">
                            <span class="h6 mb-0 font-weight-bold">19/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-10">
                            <div class="pt-3">
                                <h3>Brainstorming sul Time Agnostic Browser</h3>
                                <ol>
                                    <li>La libreria si deve basare su RDF e dev'essere il più riutilizzabile possibile in diversi contesti, per
                                        tutti quei dataset che utilizzano lo stesso modello di provenance di OpenCitations.</li>
                                    <li>Modello usato per la provenance: PROV-O ontology + oco:hasUpdateQuery.
                                        <ol>
                                            <li><em>oco:hasUpdateQuery</em>: proprietà che registra le aggiunte e le cancellazioni come query SPARQL
                                                INSERT e DELETE, mentre l'uso di variabili SPARQL è proibito nelle query di update.</li>
                                            <li>Ogni entità del dataset è rappresentata da uno o più snapshot (cioè da un'istanza della classe
                                                pro:Entity).</li>
                                            <li>Ogni snapshot registra la composizione dell'entità in un preciso momento temporale, descritto
                                                tramite la proprietà pro:generatedAtTime. Ogni snapshot è collegato ai precedenti tramite la
                                                proprietà pro:wasDerivedFrom, all'entità descritta tramite pro:specializationOf e all'agente
                                                responsabile tramite pro:wasAttributedTo.</li>
                                            <li>Vantaggi:
                                                <ol>
                                                    <li>Facilità nell'ottenere gli statement correnti di un'entità, siccome sono quelli
                                                        correntemente disponibili nel dataset.</li>
                                                    <li>Facilità nel ripristinare un'entità a un certo snapshot applicando l'operazione inversa
                                                        (INSERT anziché DELETE e viceversa).</li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Cosa significa fare una query agnostica sul tempo?
                                        <ol>
                                            <li>Poniamo che l'utente effettui la seguente ricerca:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    CONSTRUCT {
        &lt;res&gt; ?p ?o
    } 
    WHERE {
        &lt;res&gt; ?p ?o.
    }
                                                                                </code></pre>
                                                </div>
                                                Dove res è l'URI di una risorsa. Cosa avviene dietro le quinte?
                                                <ol>
                                                    <li>La query viene arricchita in modo da includere informazioni sulla provenance e, in
                                                        particolare, sul tempo di generazione dello snapshot e sulla query di update:
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    PREFIX oco: &lt;https://w3id.org/oc/ontology/&gt;
    PREFIX pro: &lt;http://www.w3.org/ns/prov#&gt;
    CONSTRUCT {
        &lt;res&gt; ?p ?o. 
        ?snapshot pro:generatedAtTime ?t;      
                oco:hasUpdateQuery ?updateQuery.
    }
    WHERE {
        &lt;res&gt; ?p ?o.
        ?snapshot pro:specializationOf &lt;res&gt;;
                pro:generatedAtTime ?t.
        OPTIONAL {
            ?snapshot oco:hasUpdateQuery ?updateQuery.
        }        
    }
                                                                                        </code></pre>
                                                        </div>
                                                        Dal numero di triple di provenance ritornate è possibile dedurre quanti snapshot esistono di
                                                        quella risorsa e, dal tempo in cui sono state generate, il loro ordine.
                                                    </li>
                                                    <li>
                                                        Viene effettuata la query dell'utente arricchita, che restituisce lo stato corrente del
                                                        dataset rispetto a quella risorsa più le informazioni sulla provenance. Il risultato viene
                                                        salvato in un dizionario, che potrebbe avere la seguente struttura:
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    snapshots = {
        &lt;t&gt;: {
            "graph": &lt;grafo della risorsa al tempo t&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-1&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-1&gt;,
            "hasUpdateQuery": &lt;update query&gt;
        },
        &lt;t-n&gt;: {
            "graph": &lt;grafo della risorsa al tempo t-n&gt;,
            "hasUpdateQuery": ""
        },
    }
                                                                                        </code></pre>
                                                        </div>
                                                        Dove le chiavi corrispondono alle proprietà pro:generatedAtTime degli snapshot ritornati. Da
                                                        notare che il dizionario al tempo t-n non ha updateQuery, perché corrisponde al grafo
                                                        dell'entità nel momento in cui è stata creata.
                                                    </li>
                                                    <li>
                                                        Se la query di update è presente, per ottenere lo stato del grafo rispetto alla risorsa al
                                                        tempo t-1, bisogna effettuare la query inversa rispetto a quella di update. Per farlo, è
                                                        sufficiente sostituire "DELETE" con "INSERT" e "INSERT" con "DELETE". Nota bene: la query di
                                                        update non viene effettuata sul triplestore, ma sul grafo della risorsa al tempo t, in modo
                                                        da non alterare lo stato del triplestore.
                                                    </li>
                                                    <li>
                                                        Effettuata la query inversa, il nuovo stato della risorsa viene salvato alla chiave "graph"
                                                        della chiave &lt;t-1&gt; del dizionario degli snapshots. Questa operazione viene effettuata
                                                        ricorsivamente per ogni query di update n ritornata sul grafo della risorsa al tempo t-n+1.
                                                    </li>
                                                </ol>
                                            <li>
                                                <strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: nel momento in cui ho una serie
                                                di snapshot di un'entità, ogni serie potrebbe essere collegata ad altre entità, che a loro volta
                                                hanno una serie di snapshot, quindi devono essere riallineate temporalmente per fare la query
                                                corretta.
                                                <ol>
                                                    <li><i class="far fa-lightbulb"></i> Poniamo che l'utente voglia andare da una risorsa al tempo
                                                        t-n a un'altra risorsa collegata che sia sempre al tempo t-n. Lo snapshot corretto della
                                                        risorsa collegata è quello la cui proprietà pro:generatedAtTime è la minima antecedente o
                                                        coincidente a t-n.</li>
                                                    <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: è più conveniente
                                                        allineare le risorse al momento della richiesta dell'utente o pre-processare l'intero
                                                        dataset in modo da avere un nuovo dataset con le risorse allineate?</li>
                                                </ol>
                                            </li>
                                    </li>
                                </ol>
                                </li>
                                <li>La libreria dev'essere configurabile, pluggabile per usare metodi di altre librerie per semplificare alcuni
                                    compiti.</li>
                                <li>Cosa mi serve? Quali librerie mi servono in Python?
                                    <ol>
                                        <li>RDFLib per la manipolazione delle triple RDF.</li>
                                        <li>sparqlwrapper per leggere i dati da un triplestore.
                                            <ol>
                                                <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: devo prevedere la
                                                    possibilità di lavorare anche su file?</li>
                                            </ol>
                                        </li>
                                        <li>oc-ocdm, per mostrare un caso d'uso.</li>
                                    </ol>
                                </li>
                                <li>Bisogna realizzare un browser che utilizzi la libreria nel back-end.
                                    <ol>
                                        <li><strong><i class="fas fa-exclamation-circle"></i> Problema</strong>: solo un browser o anche un editor?
                                        </li>
                                    </ol>
                                </li>
                                </ol>
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Risolto il bug dell'updateQuery "DELETE DATA { GRAPH { .} }; INSERT DATA { GRAPH { .} }". Il bug era nel
                                        codice dell'editor, non in quello di oc-ocdm. La ragione del bug era che i caratteri riservati di HTML
                                        contenuti nella query di update non erano riportati come entità carattere e venivano interpretati dal
                                        browser come markup.</li>
                                    <li>Risolto un bug nel KG Editor per cui le proprietà oco:hasUpdateQuery e dc:description venivano visualizzate
                                        come link poiché contenevano l'URI base.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Il problema della funzione di merge non funzionante è più profondo del previsto. Ricapitolando, il merge
                                        deve reindirizzare su self le triple che hanno come oggetto other. È Per farla funzionare occorre modificare
                                        il design del preexisting graph così come è stato previsto dalla libreria, ovvero tutte le triple che hanno
                                        un'entità come soggetto. Anche aggiungendo al preexisting_graph le triple che hanno l'entità come oggetto,
                                        queste ultime vengono rimosse dal preexisting_graph, attraverso il seguente codice:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for p, o in preexisting_graph.predicate_objects(self.res):
            self.g.add((self.res, p, o))
            self.preexisting_graph.add((self.res, p, o))
                                                                            </code></pre>
                                        </div>
                                        Come si vede, vengono considerate solo le triple che hanno self come soggetto. Perché il merge funzioni, ho
                                        modificato il codice nel seguente modo:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    if preexisting_graph is not None:
        self.remove_every_triple()
        for s, p, o in preexisting_graph.triples((None, None, None)):
            self.g.add((s, p, o))
            self.preexisting_graph.add((s, p, o))                                
                                                                                </code></pre>
                                        </div>
                                    </li>
                                    <li>Infine, ho modificato la funzione di merge, di cui riporto prima la versione attuale e poi quella
                                        modificata.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    # Versione attuale
    def merge(self, other: GraphEntity) -> None:
        for res, entity in self.g_set.res_to_entity.items():
            triples_list: List[Tuple] = list(entity.g.triples((res, None, other.res)))
            for triple in triples_list:
                entity.g.remove(triple)
                new_triple = (triple[0], triple[1], self.res)
                entity.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        # 'other' must be deleted AFTER the redirection of
        # triples pointing to it, since mark_as_to_be_deleted
        # also removes every triple pointing to 'other'
        other.mark_as_to_be_deleted()

    # Versione modificata
    def merge(self, other: GraphEntity) -> None:
        # Il ciclo non avviene sul grafo di self, ma di other, 
        # poiché è nel grafo di other che sono presenti le triple
        # che hanno other come oggetto. 
        for _, entity in other.g_set.res_to_entity.items():
            # res, entity -> _, entity, poiché res non viene utilizzata
            triples_list: List[Tuple] = list(entity.g.triples((None, None, other.res)))
            # Le triple considerate sono solo quelle che hanno other come oggetto
            for triple in triples_list:
                if triple[1] != URIRef("http://www.w3.org/ns/prov#specializationOf"):
                    # Questa condizione è stata aggiunta per evitare che venga 
                    # associata a self la provenance di other. 
                    # Sarebbe meglio riferirsi a quel predicato 
                    # come PROV.iri_specialization_of, ma non posso importare 
                    # la classe perché si verifica un'importazione circolare

                    # entity.g.remove(triple) è stato rimosso perché lo stesso
                    # compito è assolto da other.mark_as_to_be_deleted() in fondo
                    new_triple = (triple[0], triple[1], self.res)
                    self.g.add(new_triple)

        types: List[URIRef] = other.get_types()
        for cur_type in types:
            self._create_type(cur_type)

        label: Optional[str] = other.get_label()
        if label is not None:
            self.create_label(label)

        self._was_merged = True
        self._merge_list = (*self._merge_list, other)

        other.mark_as_to_be_deleted()
                                                                            </code></pre>
                                        </div>
                                        Dopo queste modifiche, le triple che hanno come oggetto other vengono correttamente rendirizzate su self,
                                        eccetto le triple di provenance. Tuttavia, ci sono ancora due bug:
                                        <ul>
                                            <li>Self viene spogliata di tutti i suoi outgoing links eccetto il tipo.</li>
                                            <li>Other viene correttamente spogliata di tutti i suoi outgoing links, ma non è stato possibile
                                                cancellare gli incoming links. </li>
                                        </ul>
                                    </li>
                                </ol>
                                <h3>Proposta di modifica a oc-ocdm:</h3>
                                <ol>
                                    <li>Ho trovato e risolto i bug dei <em>remove_name</em>, <em>remove_given_name</em> e
                                        <em>remove_family_name</em> non funzionanti. Per usare un termine tecnico, era una <em>stronzata</em>.
                                        Riporto il codice con e senza il bug per quanto riguarda <em>remove_family_name</em>, ma vale lo stesso
                                        discorso anche le altre due funzioni:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    # Codice col bug: self.g non è il soggetto
    def remove_family_name(self) -> None:
        self.g.remove((self.g, GraphEntity.iri_family_name, None))
    
    # Codice fixato: self.g -> self.res
    def remove_family_name(self) -> None:
        self.g.remove((self.res, GraphEntity.iri_family_name, None))
                                                                        </code></pre>
                                        </div>
                                    </li>
                                </ol>
                            </div>
                            </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a id="d2505cnhc4p" href="#panel-11" data-target="#panel-11" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-11">
                            <span class="h6 mb-0 font-weight-bold">25/05/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-11">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                Tutte le novità sono relative al Time Agnostic Browser
                                <ol>
                                    <li>Tre nuovi moduli: <strong>agnostic_query.py</strong>, <strong>sparql.py</strong> e
                                        <strong>support.py</strong>: il primo contiene la classe <strong>Agnostic_query</strong>, che
                                        rappresenta una query agnostica, il secondo contiene la classe <strong>Sparql</strong>, che gestisce le
                                        query, il terzo contiene la classe <strong>File_manager</strong>, per la gestione dei file. Nei punti
                                        successivi ogni classe verrà descritta separatamente:</li>
                                    <li><strong>Agnostic_query</strong>: si istanzia passando come parametro una query SPARQL. Al momento
                                        dispone di un metodo pensato per l'utente e due metodi privati. Il metodo utilizzabile dall'utente è
                                        <strong>get_entity_history</strong>, che dato l'uri di una risorsa ne ricostruisce tutta la storia,
                                        restituendo in output un dizionario secondo il seguente modello:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    {
        "URI_entità": {
            "istante_creazione_snapshot_1": grafo_al_tempo_1,
            "istante_creazione_snapshot_2": grafo_al_tempo_2,
            "istante_creazione_snapshot_n": grafo_al_tempo_n
        }
    }
                                                                        </code></pre>
                                        </div>
                                        <p>Cosa si intende per grafo? Per grafo si intendono tutte le triple che hanno l'entità come soggetto
                                            più le informazioni di provenance presenti al tempo t (non tutte quelle esistenti, solo quelle
                                            esistenti in quel dato istante).</p>
                                        <p>Il metodo get_entity_history utilizza in sequenza due metodi privati, il cui principale scopo è
                                            quello di rendere il codice modulare e testabile:</p>
                                        <ol>
                                            <li><strong>_get_entity_current_state</strong> dato l'uri di una risorsa restituisce in output un
                                                dizionario secondo lo schema già visto, ma popolando solo l'istante al tempo t, ovvero quello
                                                presente.</li>
                                            <li><strong>_get_old_graphs</strong> preso in input l'output di _get_entity_current_state popola i
                                                grafi relativi agli snapshot passati della risorsa. </li>
                                        </ol>
                                    </li>
                                    <li>La classe <strong>Sparql</strong> si istanzia passando come parametro il percorso di un file di
                                        configurazione, la cui posizione predefinita è "./config.json". Il file di configurazione è un JSON il
                                        cui modello è il seguente:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    {
        "": [
            "url_1",
            "urltriplestore_url_2",
            "url_n"
        ],
        "file_path": [
            "path_1",
            "path_2",
            "path_n"			
        ]
    }
                                                                </code></pre>
                                        </div>
                                        <p>Il metodo <strong> execute_query</strong>, data una query, restituisce il risultato della query. Al
                                            momento vengono supportati i SELECT (a uso dell'utente) e i CONSTRUCT (a uso della libreria).
                                            L'output del SELECT è sempre un set di tuple, dove l'ordine degli elementi della tupla corrisponde
                                            all'ordine delle variabili richieste dall'utente. L'output del CONSTRUCT, invece, è sempre un
                                            rdflib.graph.ConjunctiveGraph. Nota bene: l'utente e la libreria non dicono al metodo dove
                                            effettuare la query, ma predispongono il file di configurazione e invocano il metodo passando la
                                            query. È infatti il metodo stesso a occuparsi di gestire i vari casi in maniera automatica.</p>
                                        <ol>
                                            <li id="i2505chf31">Ecco un elenco dei casi gestiti e di come sono stati gestiti:
                                                <ol>
                                                    <li>L'utente/la libreria effettua un SELECT su più triplestore e più file: la query viene
                                                        effettuata su tutti i triplestore e tutti i file, i risultati uniti e ritornati come set
                                                        di tuple, dove l'ordine degli elementi nelle tuple corrisponde all'ordine delle
                                                        variabili indicate dall'utente.
                                                        <ol>
                                                            <li>Perché un set di tuple e non una lista di tuple? Per eliminare le tuple
                                                                duplicate.</li>
                                                        </ol>
                                                    </li>
                                                    <li>L'utente/la libreria indica un LIMIT: ottenuti i risultati, il set viene tagliato
                                                        secondo il limite impostato dall'utente.</li>
                                                    <li>La libreria (mai l'utente) effettua un CONSTRUCT su più triplestore e più file: la query
                                                        viene effettuata su tutti i triplestore e tutti i file, i risultati uniti e ritornati
                                                        come rdflib.graph.ConjunctiveGraph.</li>
                                                    <li>La libreria (mai l'utente) indica un LIMIT sul CONSTRUCT: ottenuti i risultati,
                                                        l'rdflib.graph.ConjunctiveGraph viene tagliato secondo il limite impostato.</li>
                                                </ol>
                                            </li>
                                            <li>A seconda del contenuto del file di configurazione, vengono invocati i metodi privati
                                                <strong>_query_endpoints</strong>, che effettua la ricerca su tutti gli endpoint indicati, e
                                                <strong>_query_files</strong>, che effettua la ricerca su tutti i file indicati.</li>
                                        </ol>
                                    <li>La classe <strong>File_manager</strong> contiene i metodi:
                                        <ol>
                                            <li><strong>import_json</strong>, che dato il percorso di un file json restituisce un dizionario con
                                                il contenuto del file.</li>
                                            <li><strong>zip_data</strong>, che dato un percorso di un file restituisce il corrispondente file
                                                zippato. </li>
                                            <li><strong>minify_json</strong>, che dato un percorso di un file json restituisce il medesimo file
                                                compresso. </li>
                                        </ol>
                                    </li>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Tra i vari metodi implementati, _query_endpoints si occupa di eseguire una query dato uno o più
                                        triplestore. Come faccio a testare questo metodo in maniera riproducibile da altri? L'esito della query
                                        è infatti dipendente dai triplestore. Devo comprendere dei triplestore di test nella cartella di test?
                                    </li>
                                    <li>Il metodo get_entity_current_state ritorna un dizionario nella forma:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    {
        "URI_entità": {
            "istante_creazione": grafo
        }
    }
                                                            </code></pre>
                                        </div>
                                        Ad esempio:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    {
        'https://github.com/arcangelo7/time_agnostic/id/1': {
            rdflib.term.Literal('2021-05-14T17:07:03+00:00', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#dateTime')): 
                None, 
            rdflib.term.Literal('2021-05-21T19:08:56+00:00', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#dateTime')): 
                &lt;Graph identifier=N23d8d6f82cea4e948cd74f55afeeeb13 (&lt;class 'rdflib.graph.ConjunctiveGraph'&gt;)&gt;
        }
    }
                                                            </code></pre>
                                        </div>
                                        Come faccio a testare un output simile? In particolare, non riesco a testare il valore corrispondente
                                        all'istanza di rdflib.graph.ConjunctiveGraph.
                                    </li>
                                    <li>Come faccio a testare un risultato randomico?</li>
                                    <li id="d2505cnhc4">Il metodo update della classe Graph di rdflib sembra non funzionare. Riporto un esempio di esecuzione e
                                        il relativo output.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from sparql import Sparql
    from SPARQLWrapper import XML
    from rdflib import URIRef
    
    query = """
        CONSTRUCT {
            &lt;https://github.com/arcangelo7/time_agnostic/br/3> ?p ?o. 
        }
        WHERE {
            &lt;https://github.com/arcangelo7/time_agnostic/br/3&gt; ?p ?o.
        }    
        """
    
    # Il seguente metodo è stato testato, non lo riporto per ragioni di sintesi
    graph = Sparql().execute_query(query=query)
    
    for triple in graph.triples((None, URIRef("http://prismstandard.org/namespaces/basic/2.0/publicationDate"), None)):
        print(str(triple[0]), str(triple[1]), str(triple[2]))
    print("\n")
    
    update_query = """
        DELETE DATA { 
            GRAPH &lt;https://github.com/arcangelo7/time_agnostic/br/&gt; { 
                &lt;https://github.com/arcangelo7/time_agnostic/br/3&gt; 
                &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate&gt; 
                "2002"^^&lt;http://www.w3.org/2001/XMLSchema#gYear&gt; .} 
        }; INSERT DATA { 
            GRAPH &lt;https://github.com/arcangelo7/time_agnostic/br/&gt; { 
                &lt;https://github.com/arcangelo7/time_agnostic/br/3&gt; 
                &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate&gt;
                "2001"^^&lt;http://www.w3.org/2001/XMLSchema#gYear&gt;
        .} } 
    """
    
    graph.update(update_query)
    for triple in graph.triples((None, URIRef("http://prismstandard.org/namespaces/basic/2.0/publicationDate"), None)):
        print(str(triple[0]), str(triple[1]), str(triple[2]))
    
    # https://github.com/arcangelo7/time_agnostic/br/3 http://prismstandard.org/namespaces/basic/2.0/publicationDate 2002-01-01
    
    
    # https://github.com/arcangelo7/time_agnostic/br/3 http://prismstandard.org/namespaces/basic/2.0/publicationDate 2001
    # https://github.com/arcangelo7/time_agnostic/br/3 http://prismstandard.org/namespaces/basic/2.0/publicationDate 2002-01-01
                                                            </code></pre>
                                        </div>
                                        <p>Come si vede, la nuova data è stata aggiunta, ma la precedente non è stata rimossa. Mi sta sfuggendo
                                            qualcosa? Ottengo lo stesso output anche utilizzando la funzione processUpdate del modulo
                                            rdflib.plugins.sparql.processor. Preciso infine che lanciando la medesima query di update tramite
                                            interfaccia grafica di Blazegraph ottengo il risultato previsto.</p>
                                        <p>Sto tralasciando per un momento il fatto che una data riportata in formato gYear nel triplestore
                                            venga parsata da rdflib in un altro formato e che da 2002 diventi 2002-01-01, bug che conosci molto
                                            bene e che è descritto in <a href="https://github.com/RDFLib/rdflib/issues/806"
                                                target="_blank">https://github.com/RDFLib/rdflib/issues/806</a>. Ad ogni modo, questo bug non
                                            coinvolge il fallimento dell'update, che tramite interfaccia Blazegraph funziona e che, per quanto
                                            riguarda il DELETE, fallisce in molteplici altri casi che non riguardano date.</p>
                                    </li>
                                    <li>Per quanto riguarda il problema dei grafi di provenance e del dataset divisi su più file o più
                                        triplestore, ho voluto risolvere la questione secondo la filosofia "massima semplicità per l'utente,
                                        massima complessità per lo sviluppatore", ovvero tramite un file di configurazione dove vengono
                                        specificati tutti gli url e i percorsi. L'utente non deve indicare cosa contengano i vari triplestore o
                                        url e non deve aprire manualmente connessioni o file, deve solo compilare il file di configurazione ed
                                        effettuare la query, tutte le varie casistiche vengono gestite in maniera automatica. Vorrei sapere cosa
                                        ne pensi di questo approccio e se ti sembra ragionevole o ingenuo. Per un elenco dei casi gestiti ti
                                        rimando al punto <a href="#i2505chf31">3.1</a>.</li>
                                    <li>Può un grafo contenere triple duplicate? Risposta provvisoria: ni. Da quello che ho capito uno store di
                                        triple RDF è un set di triple, quindi per definizione triple identiche non possono apparire due volte.
                                        Tuttavia, molti store RDF sono cosiddetti "quad stores", ovvero set di grafi rdf anche noti come
                                        "datasets" e in quel caso le triple possono anche apparire più volte. Questo è qualche volta chiamato
                                        "contesto", a seconda dello store. Mi piacerebbe sviscerare ulteriormente la questione e ascoltare la
                                        tua risposta in merito.</li>
                                    <li>Posso riutilizzare metodi di oc-ocdm per svolgere compiti identici? Ad esempio, se ho bisogno di aprire
                                        un file json e parsarlo dentro un rdf.Graph, posso utilizzare il metodo load della classe Reader di
                                        oc-ocdm che fa precisamente la stessa cosa?</li>
                                </ol>
                        
                            </div>
                        </div>
                        </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a id="d0306cnhc7p" href="#panel-12" data-target="#panel-12" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-12">
                            <span class="h6 mb-0 font-weight-bold">03/06/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-12">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Con grande soddisfazione riporto che il metodo <strong>merge_by_id</strong> è finalmente funzionante.
                                        Rispetto a prima, aggiunge al graphset le entità che hanno other come oggetto e arricchisce gli
                                        identificatori con le informazioni sullo schema e sul valore letterale, in modo da consentire il
                                        funzionamento del metodo remove_duplicated_identifiers sviluppato da Simone. Inoltre, utilizza la
                                        reflection per definire dinamicamente i metodi di aggiunta al graphset, tramite lo stesso file di
                                        configurazione realizzato per il KGEditor.</li>
                                    <li>Per venire incontro alle esigenze di Cristian, ho modificato la funzione merge_by_id in modo che
                                        permettesse di specificare non solo il tipo di entità da unire, ma anche l'IRI dello schema
                                        dell'identificatore in base al quale effettuare l'unione. In questo modo, adesso è possibile unire le
                                        risorse di tipo foaf:Agent che hanno come id un ISSN ma non un ORCID, ovvero gli editori ma non gli
                                        autori.</li>
                                    <li>Il metodo merge_by_id riceve infine un nuovo argomento, ovvero un numero a virgola mobile indicante i GB
                                        di RAM che si vogliono dedicare al processo. Quando la RAM occupata dal processo raggiunge il valore
                                        indicato, il grafo con i merge costruito fino a quel momento viene ritornato. Dopodiché, una funzione a
                                        parte si occupa di caricare le modifiche e la provenance sul triplestore, in modo da salvare il lavoro
                                        fatto. Tuttavia, non è stato ancora trovato un modo per liberare la RAM e rilanciare la funzione in
                                        automatico.</li>
                                    <li>Novità relative al Time Agnostic Browser:
                                        <ol>
                                            <li>È stato risolto il problema relativo all'update non funzionante (<a
                                                    href="#d2505cnhc4">25/05/2021, <em>Cosa non ho capito</em>, punto 4</a>), quindi il metodo
                                                <strong>get_history</strong> e i relativi metodi _get_entity_current_state e _get_old_graphs
                                                restituiscono adesso l'output atteso. Per ulteriori dettagli su quale fosse il bug e come è
                                                stato risolto si veda il <a href="#i0306cnhc1">punto 1 nella sezione <em>Cosa non ho
                                                        capito</em></a>.</li>
                                            <li>È stato implementato il metodo <strong>get_state_at_time</strong>, che dato un URI e un tempo
                                                restituisce lo stato della risorsa a quel tempo e dei collegamenti agli stati precedenti e
                                                successivi. Nello specifico, viene restituito un dizionario secondo il seguente modello:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    {
        "t": Grafo al tempo t
        "before": {
            "t-1": Grafo al tempo t-1,
            "t-n": Grafo al tempo t-n
        }, 
        "after": {
            "t+1": Grafo al tempo t+1,
            "t+n": Grafo al tempo t+n
        }
    }
                                                                            </code></pre>
                                                </div>
                                            </li>
                                            <li>I metodi su menzionati non fanno più parte della classe Agnostic_query, bensì della classe
                                                <strong>AgnosticEntity</strong>, dato che si tratta di metodi per ottenere proprietà riguardanti
                                                una specifica entità in maniera agnostica sul tempo e non per effettuare query in maniera
                                                agnostica sul tempo. La classe AgnosticEntity si istanzia passando come argomenti l'URI
                                                dell'entità e, in maniera opzionale, un booleano per indicare se si vuole ottenere anche la
                                                storia delle entità correlate, ovvero di quelle che hanno l'URI dell'entità come oggetto e non
                                                come soggetto. Il parametro è impostato di default a False, è stato cioè preferito un
                                                comportamento lazy.</li>
                                            <li>È stata implementata la funzione <strong>get_entities_histories</strong>, che dato un set di URI
                                                restituisce la storia di tutte le relative entità. È anche possibile specificare tramite un
                                                booleano se si è interessati anche alla storia delle entità correlate. Questa funzione fa parte
                                                del modulo agnostic_entity ma non della classe AgnosticEntity: serve infatti a manipolare un
                                                gruppo di istanze di quella classe.</li>
                                            <li>Le query sulla provenance e sui dati sono adesso operazioni separate. Questa distinzione si
                                                riflette su tutta l'architettura della libreria.
                                                <ol>
                                                    <li>Il modello del file di configurazione è cambiato. Si riporta uno schema di quello nuovo:
                                                        <div class="card bg-primary shadow-inset border-light">
                                                            <pre><code class="prettyprint">
    {
        "dataset": {
            "triplestore_urls": ["url_1", "url_2", "url_n"],
            "file_paths": ["path_1", "path_2", "path_n"]
        },
        "provenance": {
            "triplestore_urls": ["url_1", "url_2", "url_n"],
            "file_paths": ["path_1", "path_2", "path_n"]
        }
    }
                                                                                    </code></pre>
                                                        </div>
                                                    </li>
                                                    <li>All'interno della classe AgnosticEntity, il metodo <strong>_query_provenance</strong>
                                                        ottiene informazioni sulla provenance, il metodo <strong>_query_dataset</strong> quelle
                                                        sui dati. Il modo in cui vengono ottenuti i dati in modo da poterli salvare in grafi
                                                        denominati è particolarmente interessante e viene approfondito al <a
                                                            href="#i0306cnhc2">punto 2 della sezione <em>Cosa non ho capito</em></a>.</li>
                                                    <li>La classe <strong>Sparql</strong> è stata completamente riscritta, sia per tenere conto
                                                        due due diversi tipi di query, sia perché la prima bozza era poco robusta. Sono stati
                                                        affrontati e risolti due problemi fondamentali:
                                                        <ol>
                                                            <li>La differenza tra l'output di un SELECT e di un CONSTRUCT. Il CONSTRUCT ritorna
                                                                sempre un ConjunctiveGraph, mentre il SELECT ritorna sempre un set di tuple. Ora
                                                                le due modalità di query sono distinte in due metodi diversi,
                                                                <strong>run_construct_query</strong> e <strong>run_select_query</strong>, che a
                                                                loro volta chiamano i metodi <strong>_get_graph_from_files</strong> e
                                                                <strong>_get_graph_from_triplestores</strong> per il CONSTRUCT,
                                                                <strong>_get_tuples_from_files</strong> e
                                                                <strong>_get_tuples_from_triplestores</strong> per il SELECT. Perché non
                                                                utilizzare un metodo di query unico?
                                                                <ol>
                                                                    <li>Perché esiste uno specifico SELECT da cui si vuole ritornare un grafo e
                                                                        non un set di tuple, ovvero quello che ho chiamato SELECT hack e che
                                                                        serve per ottenere grafi denominati, come illustrato al <a
                                                                            href="#i0306cnhc2">punto 2 della sezione <em>Cosa non ho
                                                                                capito</em></a>. Per evitare di rendere l'hack un parametro
                                                                        della funzione e complicarne quindi l'utilizzo per l'utente, si è
                                                                        preferito occultare l'hack e separare le due funzioni. Quando io
                                                                        sviluppatore voglio ottenere un ConjunctiveGraph da un SELECT utilizzo
                                                                        run_construct_query anziché run_select_query.</li>
                                                                    <li>Inoltre, dato che l'utente effettua sempre dei SELECT e mai dei
                                                                        CONSTRUCT, separare le due logiche serve a snellire il processo che
                                                                        interessa realmente all'utente.</li>
                                                                </ol>
                                                            </li>
                                                            <li>È stato inoltre risolto il problema dei LIMIT, che non funzionano nel momento in
                                                                cui una query viene eseguita su tante fonti diverse e i risultati uniti. È stato
                                                                introdotto il metodo <strong>_cut_by_limit</strong>, che utilizza il metodo
                                                                prepareQuery del modulo rdflib.plugins.sparql.processor per estrarre l'algebra
                                                                della query e individuare il limite, dopodiché taglia l'output in base al limite
                                                                ottenuto. Funziona anche per limiti innestati.</li>
                                                        </ol>
                                                    </li>
                                                </ol>
                                            </li>
                                            <li>Tutti i metodi menzionati sono stati testati con esito positivo.</li>
                                            <div class="alert alert-secondary shadow-soft my-4 mb-lg-5" role="alert">
                                                <span class="alert-inner--icon icon-lg"><i class="fas fa-seedling"></i></span>
                                                <p>Tutte le classi e i metodi esposti all'utente sono stati documentati utilizzando il
                                                    linguaggio di markup reStructuredText. Inoltre, è stato utilizzato Sphinx per generare
                                                    automaticamente la documentazione in formato HTML.</p>
                                                <hr>
                                                <p>Il risultato può essere visionato al seguente indirizzo: <a
                                                        href="https://arcangelo7.github.io/time-agnostic-browser/" target="_blank"
                                                        alt="Link to the time_agnostic_browser library documentation">https://arcangelo7.github.io/time-agnostic-browser/</a>.
                                                </p>
                                            </div>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="i0306cnhc1">Ho capito perché i metodi di update di rdflib non sortissero effetto quando l'update
                                        coinvolgeva un DELETE. La ragione è che i delete in questione non erano generici, ma specifici per dei
                                        grafi denominati. Prendiamo ad esempio la seguente query:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    DELETE DATA { 
        GRAPH &lt;https://github.com/arcangelo7/time_agnostic/br/&gt; { 
            &lt;https://github.com/arcangelo7/time_agnostic/br/3&gt; 
            &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate&gt; 
            "2002"^^&lt;http://www.w3.org/2001/XMLSchema#gYear&gt;.
        } 
    }
                                                                </code></pre>
                                        </div>
                                        <p>Questa query non cancella una generica tripla
                                            &lt;https://github.com/arcangelo7/time_agnostic/br/3&gt; prism:publicationDate "2002", ma cancella
                                            quella tripla all'interno del grafo di nome &lt;https://github.com/arcangelo7/time_agnostic/br/&gt;.
                                            Le query di update generate automaticamente da oc_ocdm specificano sempre il grafo a cui si
                                            riferiscono.</p>
                                        <p>C'è però un problema: SPARQL 1.1 è bello e caro, ma non offre alcun modo per produrre quad RDF in
                                            grafi denominati e la clausola CONSTRUCT è limitata alla produzione di triple RDF. Quindi, poiché
                                            l'informazione sul contesto non era presente nel ConjunctiveGraph creato con un CONSTRUCT tramite
                                            SparqlWrapper, il DELETE DATA non sortiva effetto.</p>
                                        <p>Il problema potrebbe essere risolto in futuro introducendo la seguente sintassi:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
                                        CONSTRUCT {
                                            GRAPH ?g { ?s ?p ?o }
                                        }
                                                                    </code></pre>
                                        </div>
                                        <p>Non pensi che potrebbe essere una valida proposta per SPARQL 1.2?</p>
                                    </li>
                                    <li id="i0306cnhc2">In attesa di SPARQL 1.2, vedo due soluzioni al problema del punto 1: o cancello i
                                        riferimenti al grafo denominato dalle query di update (poco lungimirante) o invece di un CONSTRUCT
                                        effettuo un SELECT che ritorna una quadrupla il cui quarto elemento è il contesto, dopodiché genero il
                                        ConjunctiveGraph passando la quadrupla al metodo add che, in caso di quadruple, interpreta il quarto
                                        elemento come contesto, in questo modo:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    query = """
    SELECT ?s ?p ?o ?c
        WHERE {
            GRAPH ?c {?s ?p ?o}
            VALUES ?s {&lt;res&gt;}
    }
    """
    
    # [...]
        
    cg = ConjunctiveGraph()
    for quad in results.quads():
        cg.add(quad)
                                                                </code></pre>
                                        </div>
                                        <p>Quest'ultima soluzione mi sembra più lungimirante, perché preserva l'informazione sul contesto e
                                            risolve il problema in maniera backwards e forwards-compatible. Che ne pensi?</p>
                                    </li>
                                    <li>Quando viene fatto un merge tra A e B, lo snapshot associato ad A (pro:specializationOf ) viene fatto
                                        derivare dallo snapshot di B (pro:wasDerivedFrom) precedente al merge. Per capirci, se A viene unita a
                                        B, C e D, lo snapshot relativo al merge di A deriverà gli snapshot precedenti di B, C e D. Ti mostro in
                                        foto quello a cui mi riferisco:
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/wasDerivedFrom.png" alt="line-through">
                                        </div>
                                        <p>Analizzando il codice della classe ProvSet di oc-ocdm mi sono reso conto che questo comportamento non
                                            è un bug, è una feature, perché c'è un'intera funzione dedicata esclusivamente a causarlo, ovvero <a
                                                href="https://github.com/opencitations/oc_ocdm/blob/5090df3969c53f4619f851bbf203eb65e35a51ae/oc_ocdm/prov/prov_set.py#L126"
                                                target="_blank"
                                                alt="Link to the _get_snapshots_from_merge_list method">_get_snapshots_from_merge_list</a>, che
                                            viene invocata all'interno del metodo generate_provenance. Ti chiedo quindi se è previsto che questo
                                            accada e perché.</p>
                                    </li>
                                    <li>Come faccio a forzare Python a liberare la RAM senza interrompere un processo? Da quello che ho capito,
                                        Python fa questo lavoro da solo tramite il Garbage Collector, che dovrebbe eliminare quegli oggetti con
                                        zero riferimenti. Dato che in Python tutto è un oggetto, ciò significa qualunque tipo di variabile. Ho
                                        letto che è possibile forzare l'intervento del Garbage Collector. In teoria, le seguenti righe di codice
                                        dovrebbero liberare molta RAM.
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    import gc

    del graphset
    del provset
    gc.collect()
                                                                </code></pre>
                                        </div>
                                        <p>Tuttavia, questo non avviene. Come mai? Possibile che la RAM non venga liberata fino a quando non
                                            finisce il processo?</p>
                                    </li>
                                    <li>È possibile che un secondo snapshot non abbia query di update?</li>
                                    <li id="d0306cnhc6">La funzione processUpdate del modulo rdflib.plugins.sparql.processor solleva un'eccezione di tipo
                                        RecursionError com messaggio "maximum recursion depth exceeded" nel caso in cui la query di update sia
                                        troppo lunga. È un problema emerso in questo issue del 2015 e mai risolto: <a
                                            href="https://github.com/RDFLib/rdflib/issues/481" target="_blank"
                                            alt="Link to the issue">https://github.com/RDFLib/rdflib/issues/481/</a>. Inoltre, aumentare il
                                        limite di profondità della ricorsione tramite sys.setrecursionlimit da 1000 a 2000 fa crashare
                                        l'applicativo. Tutti i valori precedenti sollevano l'eccezione RecursionError. Suggerimenti? Opinioni?
                                        Riporto del codice per riprodurre l'errore:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from rdflib import ConjunctiveGraph
    from rdflib.plugins.sparql.processor import processUpdate
    
    
    cg = ConjunctiveGraph()
    
    update_query = """
    INSERT DATA { GRAPH &lt;https://github.com/arcangelo7/time_agnostic/br/> { 
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/pro/isDocumentContextFor> &lt;https://github.com/arcangelo7/time_agnostic/ar/2995> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15717> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12885> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12855> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12894> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12833> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12875> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12900> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/dc/terms/title> 
        "Citation histories of scientific publications. The data sources"^^&lt;http://www.w3.org/2001/XMLSchema#string> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12892> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12872> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15713> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12859> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12883> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15665> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12853> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15725> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12870> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12886> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12824> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15723> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15660> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15702> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12823> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12864> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12908> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12917> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15748> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15686> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12916> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15680> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15757> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15678> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12844> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15746> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12862> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12921> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15739> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12888> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15659> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15685> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12832> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15704> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15735> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12831> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12882> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12902> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15756> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12877> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12871> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15692> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15699> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12881> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12889> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12825> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12914> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type> &lt;http://purl.org/spar/fabio/Expression> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12912> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12841> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12913> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12838> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15722> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15721> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12865> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15733> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15726> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12849> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15697> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12884> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15698> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12904> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15701> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#partOf> &lt;https://github.com/arcangelo7/time_agnostic/br/15657> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12878> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12876> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15681> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15716> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12856> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15740> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12879> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15658> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15719> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12848> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12822> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15693> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15664> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12899> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12905> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15663> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12897> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12827> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12826> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15661> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15676> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15747> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15755> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15668> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15684> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15696> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15734> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12919> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15683> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12866> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15669> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15728> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12911> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15743> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12898> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15750> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12907> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12893> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15712> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12869> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15674> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#embodiment> &lt;https://github.com/arcangelo7/time_agnostic/re/2015> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12837> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15707> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15662> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15666> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15752> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12920> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15718> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15738> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15753> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12821> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12846> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15689> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12918> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15729> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12903> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12895> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12836> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12851> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15709> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15742> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12861> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12915> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15673> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15679> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15671> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15758> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15751> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15670> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12867> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15691> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12839> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12896> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15675> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12890> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15714> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12842> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate> 
        "1985-01-01"^^&lt;http://www.w3.org/2001/XMLSchema#gYear> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15754> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15727> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15688> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15737> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15690> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12834> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15677> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12860> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12828> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15672> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15667> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15695> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15705> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15687> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12857> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12887> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15682> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12847> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12854> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15706> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type> &lt;http://purl.org/spar/fabio/JournalArticle> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15715> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/datacite/hasIdentifier> &lt;https://github.com/arcangelo7/time_agnostic/id/13890> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15736> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12863> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15731> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15703> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12910> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12909> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15749> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12840> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15710> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12829> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15711> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15730> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12891> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15744> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12873> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15700> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12852> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12906> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15732> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12858> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15720> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15724> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15708> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12901> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12880> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12843> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12835> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12830> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12874> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15745> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15694> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12845> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12868> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/vocab/frbr/core#part> &lt;https://github.com/arcangelo7/time_agnostic/be/12850> .
    &lt;https://github.com/arcangelo7/time_agnostic/br/15655> &lt;http://purl.org/spar/cito/cites> &lt;https://github.com/arcangelo7/time_agnostic/br/15741> .} }
    """
    
    processUpdate(cg, update_query)
                                                                </code></pre>
                                        </div>
                                    </li>
                                    <li id="d0306cnhc7">La porzione di libreria finora sviluppata risolve il problema di ricreare un'entità agnostica sul tempo,
                                        ovvero un'entità che contiene informazioni sul suo stato presente e su quelli passati. Ben altro
                                        problema è quello di una <strong>query agnostica sul tempo</strong>, che non solo ritorna il risultato
                                        in base allo stato corrente del triplestore, ma anche il risultato che ritornerebbe negli stati passati.
                                        Dopo una lunga riflessione, sono giunto alla conclusione provvisoria che l'unico modo per raggiungere
                                        questo risultato sia ricreare gli stati passati del triplestore ed effettuare la query agnostica sul
                                        tempo su ciascuno stato. Per ottenere questo scopo, ho realizzato il seguente algoritmo, che vado prima
                                        a spiegare a parole e di cui mostro successivamente il codice:
                                        <ol>
                                            <li>Ottengo i valori temporali di tutti gli snapshots contenenti una query di update, ovvero quante
                                                volte il ConjunctiveGraph è cambiato.</li>
                                            <li>Per ogni valore temporale, ottengo gli URI di tutte le entità che sono cambiate in quel tempo,
                                                quindi creo un set contenente questi URI e lo associo allo snapshot in un dizionario.</li>
                                            <li>Per ogni tempo, per ogni entità cambiata a quel tempo ricostruisco il grafo di quell'entità in
                                                quel tempo, quindi sommo tutti i grafi ottenuti: il risultato è il delta tra lo stato del
                                                triplestore in quel tempo e quello immediatamente futuro. Infine, associo questo delta al tempo
                                                corrispondente in un nuovo dizionario, che ritorno in output.</li>
                                        </ol>
                                        Vantaggi:
                                        <ul>
                                            <li>Impatto sulla RAM e sulla CPU molto basso, perché vengono processati e immagazzinati soltanto i
                                                delta.</li>
                                            <li>Una volta ottenute le informazioni, eseguire query agnostiche sul tempo diventa un problema
                                                triviale.</li>
                                        </ul>
                                        Svantaggio:
                                        <ul>
                                            <li>A seconda delle dimensioni del dataset e del numero di snapshot, l'operazione può richiedere
                                                molto tempo. L'idea è quella di salvare il risultato su file in modo da doverla eseguire una
                                                volta sola.</li>
                                        </ul>
                                        <p>L'algoritmo si chiama "calzino" perché invece di ottenere gli snapshot dalle entità ottiene le entità
                                            dagli snapshot, come un calzino che viene rivoltato. Inoltre, l'espressione "rivoltare come un
                                            calzino" significa "esaminare minuziosamente per conoscere a fondo", per cui la metafora mi sembrava
                                            calzante (scusa il gioco di parole).</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from time_agnostic_browser.sparql import Sparql
    from time_agnostic_browser.prov_entity import ProvEntity
    from time_agnostic_browser.agnostic_entity import AgnosticEntity
    
    
    def run_the_sock(self):
        dict_of_snapshots = dict()
        query_times = f"""
            SELECT DISTINCT ?time
            WHERE {{
                ?snapshot &lt;{ProvEntity.iri_generated_at_time}> ?time;
                    &lt;{ProvEntity.iri_has_update_query}> ?updateQuery.
            }}
        """
        times = Sparql().run_select_query(query_times)
        for time in times:
            query_entities_at_time = f"""
                SELECT DISTINCT ?s
                WHERE {{
                    ?snapshot &lt;{ProvEntity.iri_generated_at_time}> "{time[0]}"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;;
                        &lt;{ProvEntity.iri_specialization_of}> ?s.
                }}
            """
            entities_at_time = Sparql().run_select_query(query_entities_at_time)
            for entity_at_time in entities_at_time:
                dict_of_snapshots.setdefault(time[0], set())
                dict_of_snapshots[time[0]].add(entity_at_time[0])
        
        sock = dict()
        for snapshot, set_of_entities in dict_of_snapshots.items():
            graph_at_time = ConjunctiveGraph()
            for entity in set_of_entities:
                agnostic_entity = AgnosticEntity(res=entity, related_entities_history=False)
                state_at_time = agnostic_entity.get_state_at_time(time=snapshot, get_hooks=False)
                for time, state in state_at_time.items():
                    graph_at_time += state
            sock[snapshot] = graph_at_time

        return sock
                                                                </code></pre>
                                        </div>
                                        <p>Cosa ne pensi? Ti vengono in mente soluzioni alternative che permettano di eseguire query agnostiche
                                            sul tempo al volo?</p>
                                    </li>
                                </ol>
                                <h3>Proposta di modifica a oc-ocdm:</h3>
                                <p>oc-ocdm salva il tempo di generazione degli snapshot in formato %Y-%m-%dT%H:%M:%S, ovvero senza fuso orario.
                                    Quando il software verrà usato in tutto il mondo la mancanza del fuso orario sarà un problema, perché
                                    l'ordine di generazione degli snapshot è cruciale e la time-zone è un'informazione essenziale per
                                    determinare l'ordine, visto che altri corpora potrebbero usare nomenclature diverse da /prov/se/[iterative
                                    number]. Per questo motivo, la mia libreria assume che il tempo sia in formato %Y-%m-%dT%H:%M:%S%z. Pensi
                                    anche tu che questo debba essere un requisito? Se sì, propongo di modificare il formato del tempo degli
                                    snapshot prodotti da oc-ocdm per includere anche il fuso orario.</p>
                                <div class="card bg-primary shadow-inset border-light">
                                    <pre><code class="prettyprint">
    # Codice attuale
    def generate_provenance(self, c_time: float = None) -> None:
        time_string: str = '%Y-%m-%dT%H:%M:%S'
        if c_time is None:
            cur_time: str = datetime.now().strftime(time_string)
        else:
            cur_time: str = datetime.fromtimestamp(c_time).strftime(time_string)
    
    # Codice modificato
    def generate_provenance(self, c_time: float = None) -> None:
        time_string: str = '%Y-%m-%dT%H:%M:%S%z' # <- La modifica è qui
        if c_time is None:
            cur_time: str = datetime.now().strftime(time_string)
        else:
            cur_time: str = datetime.fromtimestamp(c_time).strftime(time_string)
                                                        </code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a id="d0906cnhc2p" href="#panel-13" data-target="#panel-13" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-13">
                            <span class="h6 mb-0 font-weight-bold">09/06/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-13">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <p>Miglioramenti e backtracking sul codice del tirocinio, ovvero bug che ho scoperto mentre cercavo la causa dei due bug discussi ai punti <a href="#i0906cnhc1">1 e 2 della sezione <em>Cosa non ho capito</em></a>, ma che non c'entravano niente coi bug di partenza.</p>
                                <ol>
                                    <li>Si è reso necessario modificare il funzionamento del <strong>Knowledge Graph Editor</strong> per renderlo compatibile con la versione 6.0.1 di oc-ocdm, che tra le varie novità introduce un safety check sui metodi add_*.</li>
                                    <li>Quando l'editor modifica o crea una data, utilizza adesso il formato ISO 8601 per funzionare con il metodo has_pub_date di oc-ocdm.</li>
                                    <li>Risolto un bug in generate_graph e add_coci_data per cui il metodo <strong>create_date</strong> di oc-ocdm riceveva in input sempre e solo l'anno, scartando eventuali mesi e giorni.</li>
                                    <li>Il metodo <strong>add_coci_data</strong>, che aggiunge al dataset le citazioni presenti in COCI ma non in Crossref free, cerca adesso i lavori tramite il tipo fabio:Expression o non più fabio:JournalArticle, dato che la gestione dei tipi è divenuta più sofisticata per considerare anche tutte le altre tipologie di lavori accademici.</li>
                                    <li>I namespaces richiamati in tutti i metodi della classe <strong>DatasetAutoEnhancer</strong> sono adesso delle variabili e vengono richiamati dalle proprietà della classe GraphEntity di oc-ocdm.</li>
                                    <li>È stato rimosso da tutti i metodi un passaggio superfluo per cui il grafo ritornato da SPARQLWrapper settando il formato a RDFXML veniva ulteriormente trasformato in un grafo.</li>
                                </ol>
                                <p>Novità relative al Time Agnostic Browser</p>
                                <ol>
                                    <li>Il metodo <strong>get_state_at_time</strong> è stato completamente riscritto ed è ora molto più potente. Ecco un elenco delle novità:
                                        <ol>
                                            <li>Restituisce solo il conjunctive graph della risorsa al tempo specificato, senza computare i grafi intermedi. Questa magia è frutto di un'ipotesi che vorrei discutere con te e che illustro al <a href="#i0906cnhc3">punto 3 della sezione <em>Cosa non ho capito</em>.</a></li>
                                            <li>Il tempo specificato dall'utente non deve corrispondere precisamente a uno snapshot, può essere qualsiasi tempo. Se mi si chiede in che stato siano i miei baffi alle 13:19 del 06/06/2021, la risposta è che sono rasati anche se non li ho rasati esattamente in quell'istante: allo stesso modo, se un utente chiede lo stato di una risorsa a un certo tempo, viene restituito il grafo corrispondente all'ultima modifica prima di quel tempo.</li>
                                            <li>Il tempo specificato dall'utente può essere in qualunque formato standardizzato. La funzione parser del modulo <strong>dateutile</strong> si occupa di ricondurre tutti i formati a delle istanze della classe datetime.datetime.</li>
                                            <li>Se il parametro get_hooks è False (valore di default) viene ritornato il conjunctive graph della risorsa al tempo specificato. Se è True, viene ritornata una tupla, in cui il primo elemento è il conjunctive graph, mentre il secondo è un dizionario nella forma uri_snapshot:tempo_snapshot, ad esempio:</li>
                                            <div class="card bg-primary shadow-inset border-light">
                                                <pre><code class="prettyprint">
    (
        &lt;Graph identifier=N2fe380ea28e147249344a79f49647ddf (&lt;class 'rdflib.graph.ConjunctiveGraph'&gt;)&gt;,
        {
            'https://github.com/arcangelo7/time_agnostic/ar/1/prov/se/2': '2021-06-05T18:06:34.000Z',
            'https://github.com/arcangelo7/time_agnostic/ar/1/prov/se/3': '2021-06-05T18:06:41.000Z',
            'https://github.com/arcangelo7/time_agnostic/ar/1/prov/se/5': '2021-06-05T18:07:00.000Z'
        }
    )
                                                </code></pre>
                                            </div>
                                            <p>Gli snapshot nel dizionario sono tutti e soli quelli diversi dallo snapshot di cui è stato ritornato il conjunctive graph. In questo modo, all'utente basta rilanciare la funzione cambiando il tempo per ottenere un nuovo stato.</p>
                                        </ol>
                                    </li>
                                    <li>
                                        <p>È stato introdotto il metodo <strong>_manage_update_queries</strong>, che aggira il bug di rdflib discusso al punto in <a href="#d0306cnhc6">03/06/2021, <em>Cosa non ho capito</em>, punto 6</a>, ovvero l'errore nella ricorsione in caso di query di update troppo lunghe. La soluzione adottata, come da te suggerito, è stata tagliare la query troppo lunga in query più brevi, soluzione che si è rilevata più complessa del previsto, dato che è proprio la funzione che si occupa di parsare la stringa della query a sollevare l'eccezione RecursionError. Ho quindi parsato autonomamente la query, gestendo anche il caso di query che contengano più query al loro interno, ciascuna delle quali viene ulteriormente suddivisa nel caso sia troppo lunga. Riporto il codice utilizzato:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    @classmethod
    def _manage_update_queries(cls, graph: ConjunctiveGraph, update_query: str) -> None:
        update_query = update_query.replace("INSERT", "%temp%").replace("DELETE", "INSERT").replace("%temp%", "DELETE")
        triples_end_pattern = r">\s*\."
        operations_pattern = r"((?:DELETE|INSERT)\s(?:DATA)\s?{\s?(?:GRAPH)\s?<[\w\W]+?>\s{)"
        matches = re.split(triples_end_pattern, update_query)
        # 90 is the maximum number of triples after which recursion error occurs
        if len(matches) > 90:
            split_by_operations = re.split(operations_pattern, update_query, re.IGNORECASE)
            operations = split_by_operations[1::2]
            for operation in operations:
                triples = split_by_operations[split_by_operations.index(operation) + 1]
                operation_and_query = operation + triples
                matches = re.split(triples_end_pattern, operation_and_query)
                if len(matches) > 90:
                    matches_left = len(matches)
                    # Remove operation and trailing "} }"
                    matches = [match.replace(operation, "") for match in matches][:-1]
                    while matches_left > 0:
                        cut_update_query = operation + "> .".join(matches[0:90]) + "> .} }"
                        processUpdate(graph, cut_update_query)
                        matches_left -= 90
                        matches = matches[90:]
                else:
                    processUpdate(graph, operation_and_query)                
        else:
            processUpdate(graph, update_query)
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        run_the_sock si chiama adesso <strong>_rebuild_past_graphs</strong>, un nome più esplicito. Rispetto a prima, utilizza il predicato pro:wasDerivedFrom per ottenere gli snapshot successivi al primo, dato che, come discusso, la query di update non è sempre presente. È stato inoltre risolto un bug per cui venivano ricreate tutte le entità, comprese quelle presenti, ma non quelle relative agli snapshot di creazione. Il comportamento desiderato è esattamente l'opposto: devono essere ricreati tutti e soli i grafi del passato, ovvero i grafi di entità che nel presente sono diverse rispetto al passato, per occupare meno risorse possibili sia sul disco che sulla RAM. Per la stessa ragione, sono state rimosse da questi grafi le informazioni sulla provenance, che possono essere recuperate dai grafi del presente in qualunque momento.
                                    </li>
                                    <li>Risolto un bug nel metodo <strong>_get_tuples_from_triplestores</strong> per cui gli elementi nella tupla non rispettavano l'ordine delle variabili nella query, ma l'ordine era casuale.</li>
                                    <li>Tutti i test sono stati aggiornati e superati.</li>
                                    <li>La documentazione è stata aggiornata.</li>
                                    <li>Ho aperto i seguenti issue:
                                        <ol>
                                            <li><a href="https://github.com/RDFLib/rdflib/issues/1336" target="_blank">https://github.com/RDFLib/rdflib/issues/1336</a></li>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/15" target="_blank">https://github.com/opencitations/oc_ocdm/issues/15</a></li>
                                            <li><a href="https://github.com/opencitations/metadata/issues/9" target="_blank">https://github.com/opencitations/metadata/issues/9</a></li>
                                            <li><a href="https://github.com/opencitations/metadata/issues/10" target="_blank">https://github.com/opencitations/metadata/issues/10</a></li>
                                        </ol>
                                    </li>
                                    <li>Ho creato la seguente pull request a oc-ocdm: <a href="https://github.com/opencitations/oc_ocdm/pull/16" target="_blank">https://github.com/opencitations/oc_ocdm/pull/16</a>.</li>
                                    <li>Ho verificato che Sphinx sia compatibile con Read the Docs e lo è oltremodo. Basta configurare lo specifico progetto nella dashboard su readthedocs.org, passando come parametro il percorso del file conf.py, dopodiché la documentazione viene automaticamente ricostruita a ogni modifica.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="i0906cnhc1">
                                        <p>C'è un problema nelle query di update per quanto riguarda la cancellazione e l'aggiunta di <strong>date</strong>. Probabilmente è in rdflib e quindi in SPARQLWrapper, ma oc-ocdm non lo gestisce come fa in altri casi. Le date in formato gYearMonth o in formato gYear vengono recuperate dal triplestore e parsate aggiungendo il mese e il giorno mancanti, per cui 2000 diventa 2000-01-01 e 2000-01 diventa parimenti 2000-01-01. Ho scritto del codice utile a riprodurre il problema.</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.prov import ProvSet
    from oc_ocdm.reader import Reader
    from oc_ocdm.storer import Storer
    from oc_ocdm.support import create_date
    
    
    my_graphset = GraphSet("http://dataset_base_iri/", info_dir="./data/info_dir/graph/")
    my_br = my_graphset.add_br("http://responsible_agent_uri/")
    date_string = create_date([2020, 5])
    my_br.has_pub_date(date_string)
    my_provset = ProvSet(my_graphset, "http://dataset_base_iri/", info_dir="./data/info_dir/prov/")
    my_provset.generate_provenance()
    my_graph_storer = Storer(my_graphset)
    my_prov_storer = Storer(my_provset)
    my_graph_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    my_prov_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    
    another_graphset = GraphSet("http://dataset_base_iri/", info_dir="./data/info_dir/graph/")
    previous_br = Reader.import_entity_from_triplestore(
        g_set=another_graphset, 
        ts_url="http://localhost:9999/blazegraph/sparql", 
        res=URIRef("http://dataset_base_iri/br/1"), 
        resp_agent="someone")
    same_date_string = create_date([2020, 5])
    previous_br.has_pub_date(same_date_string)
    another_provset = ProvSet(another_graphset, "http://dataset_base_iri/", info_dir="./data/info_dir/prov/")
    another_provset.generate_provenance()
    another_graph_storer = Storer(another_graphset)
    another_prov_storer = Storer(another_provset)
    another_graph_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    another_prov_storer.upload_all("http://localhost:9999/blazegraph/sparql")
                                            </code></pre>
                                        </div>
                                        <p>Quello che succede è che, nonostante la data non sia stata modificata, viene generato un nuovo snapshot con una query di update rivelatoria:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    DELETE DATA { 
        GRAPH &lt;http://dataset_base_iri/br/&gt; { 
            &lt;http://dataset_base_iri/br/1&gt; 
            &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate&gt; 
            "2020-05-01"^^&lt;http://www.w3.org/2001/XMLSchema#gYearMonth&gt; .
        } 
    }; 
    INSERT DATA { 
        GRAPH &lt;http://dataset_base_iri/br/&gt; { 
            &lt;http://dataset_base_iri/br/1&gt;
            &lt;http://prismstandard.org/namespaces/basic/2.0/publicationDate&gt; 
            "2020-05"^^&lt;http://www.w3.org/2001/XMLSchema#gYearMonth&gt; .
        } 
    }
                                            </code></pre>
                                        </div>
                                        <p>Questo bug rompe il time agnostic browser che sto creando, perché DELETE DATA funziona solo se il match è perfetto e, dato che il formato di inserimento e di cancellazione cambiano, il DELETE DATA non funziona mai per le date.</p>
                                    </li>
                                    <li id="d0906cnhc2">
                                        <p>C'è un problema nelle query di update per quanto riguarda la cancellazione e l'aggiunta di <strong>stringhe</strong>, come ad esempio un DOI. Quando la stringa viene cancellata, nella query di update viene riportato il tipo, quando viene aggiunta no. Questo è un problema perché, nel momento in cui vado a invertire la query per recuperare un grafo del passato, l'INSERT DATA diventa DELETE DATA e, senza l'indicazione del tipo, non trova la tripla e non cancella nulla. Riporto prima un esempio di query di update problematica e poi del codice per riprodurre il problema:</p>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    DELETE DATA { 
        GRAPH &lt;http://dataset_base_iri/id/&gt; { 
            &lt;http://dataset_base_iri/id/1&gt; 
            &lt;http://www.essepuntato.it/2010/06/literalreification/hasLiteralValue&gt; 
            "10.1007/978-3-319-90698-0_26"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; .
        } 
    }; 
    INSERT DATA { 
        GRAPH &lt;http://dataset_base_iri/id/&gt; { 
            &lt;http://dataset_base_iri/id/1&gt; 
            &lt;http://www.essepuntato.it/2010/06/literalreification/hasLiteralValue&gt; 
            "10.1016/j.jplph.2012.09.012" .
        } 
    }
                                            </code></pre>
                                        </div>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    from oc_ocdm.graph import GraphSet
    from oc_ocdm.prov import ProvSet
    from oc_ocdm.reader import Reader
    from oc_ocdm.storer import Storer
    
    
    my_graphset = GraphSet("http://dataset_base_iri/", info_dir="./data/info_dir/graph/")
    my_id = my_graphset.add_id("http://responsible_agent_uri/")
    my_id.create_doi("10.1007/978-3-319-90698-0_26")
    my_provset = ProvSet(my_graphset, "http://dataset_base_iri/", info_dir="./data/info_dir/prov/")
    my_provset.generate_provenance()
    my_graph_storer = Storer(my_graphset)
    my_prov_storer = Storer(my_provset)
    my_graph_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    my_prov_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    
    another_graphset = GraphSet("http://dataset_base_iri/", info_dir="./data/info_dir/graph/")
    previous_id = Reader.import_entity_from_triplestore(
        g_set=another_graphset, 
        ts_url="http://localhost:9999/blazegraph/sparql", 
        res=URIRef("http://dataset_base_iri/id/1"), 
        resp_agent="someone")
    previous_id.remove_identifier_with_scheme()
    previous_id.create_doi("10.1016/j.jplph.2012.09.012")
    another_provset = ProvSet(another_graphset, "http://dataset_base_iri/", info_dir="./data/info_dir/prov/")
    another_provset.generate_provenance()
    another_graph_storer = Storer(another_graphset)
    another_prov_storer = Storer(another_provset)
    another_graph_storer.upload_all("http://localhost:9999/blazegraph/sparql")
    another_prov_storer.upload_all("http://localhost:9999/blazegraph/sparql")
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li id="i0906cnhc3">Durante lo scorso incontro mi hai chiesto di modificare get_state_at_time in modo che restituisca solo il grafo al tempo t, senza calcolare quelli precedenti e successivi. Dopo aver studiato il problema mi sono reso conto che non è possibile ottenere il grafo al tempo t senza calcolare quelli successivi. Infatti, per ottenere il grafo al tempo t-2 ho bisogno del grafo al tempo t-1 e così via. Tuttavia, ho elaborato un'ipotesi che per il momento non sono riuscito a confutare, ovvero che sia possibile ottenere il grafo al tempo t-n sommando nel giusto ordine le stringhe delle query di update inverse da t a t-n+1. Dato che le varie operazioni vengono eseguite in maniera sequenziale, il grafo finale è finora risultato sempre corretto. Hai ragione di credere che non sia così?</li>
                                </ol>                
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-14" data-target="#panel-14" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-14">
                            <span class="h6 mb-0 font-weight-bold">17/06/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-14">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Tutti i metodi <strong>add_*</strong> di oc-ocdm utilizzati specificano adesso la fonte, ovvero Crossref. </li> 
                                    <li>La funzione <strong>_hack_dates</strong> è stata introdotta ovunque ci sia un parsing ad opera di rdflib. </li>
                                    <li>Risolto un bug in <strong>_get_tuples_from_triplestores</strong> per cui il metodo crashava se la query conteneva un OPTIONAL. Ora il metodo ritorna tuple sempre della stessa lunghezza. In caso di valori opzionali mancanti, il valore inserito nella tupla è None. </li>
                                    <li>Risolti dei bug in <strong>_manage_update_queries</strong> per cui la funzione crashava in presenza di più INSERT DATA o più DELETE DATA in una stessa query di update.</li>
                                    <li>Novità relative a <strong>get_state_at_time</strong>:
                                        <ol>
                                            <li>Il metodo ritorna sempre una tupla di tre elementi: il primo è il conjunctive graph della risorsa a quel tempo, il secondo sono i metadati sullo snapshot di cui è stato ritornato lo stato. Se il parametro get_hooks è True, il terzo elemento della tupla sono i metadati sugli altri snapshot, altrimenti un dizionario vuoto. Logica vuole che il terzo dizionario sia vuoto anche nel caso in cui esista un solo snapshot.</li>
                                            <li>I metadati sugli snapshot comprendono il tempo di generazione, l'agente responsabile e la fonte primaria.</li>
                                            <li>Ecco un esempio di possibile output:
                                                <div class="card bg-primary shadow-inset border-light">
                                                    <pre><code class="prettyprint">
    (
        &lt;Graph identifier=N75add58bd55b4d9f88082787e3e2c888 (&lt;class 'rdflib.graph.ConjunctiveGraph'&gt;)&gt;,
        {
            'https://github.com/arcangelo7/time_agnostic/id/1/prov/se/2': {
                'http://www.w3.org/ns/prov#generatedAtTime': '2021-06-04T09:36:53.000Z',
                'http://www.w3.org/ns/prov#hadPrimarySource': 'https://www.crossref.org/',
                'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696'
            }
        },
        {
            'https://github.com/arcangelo7/time_agnostic/id/1/prov/se/1': {
                'http://www.w3.org/ns/prov#generatedAtTime': '2021-05-29T16:20:22.000Z',
                'http://www.w3.org/ns/prov#hadPrimarySource': 'https://www.crossref.org/',
                'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696'
            },
            'https://github.com/arcangelo7/time_agnostic/id/1/prov/se/3': {
                'http://www.w3.org/ns/prov#generatedAtTime': '2021-06-04T11:15:32.000Z',
                'http://www.w3.org/ns/prov#hadPrimarySource': 'https://www.crossref.org/',
                'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696'
            }
        }
    )
                                                    </span></code></pre>
                                                </div>
                                            </li>
                                            <li>Risolto un bug per cui non veniva ritornato lo snapshot di creazione, in quanto privo di query di update.</li>
                                        </ol>
                                    </li>
                                    <li>Il principale problema affrontato questa settimana ha riguardato la possibilità di effettuare <strong>query agnostiche sul tempo</strong>. Il problema è stato risolto, ma la soluzione trovata è un'arma di distruzione di hardware, come viene discusso al <a href="#i1706cnhc5">punto 5 della sezione <em>Cosa non ho capito</em></a>. Il codice completo può essere visionato <a href="https://github.com/arcangelo7/time-agnostic-browser/blob/main/time_agnostic_browser/agnostic_query.py" target="_blank">su GitHub</a>, mentre in questo paragrafo viene illustrato in dettaglio il procedimento adottato:
                                        <ol>
                                            <li>La classe <strong>AgnosticQuery</strong> si istanzia passando come argomenti la posizione dei grafi del passato, la destinazione in cui salvarli nel caso non esistano, e la stringa di query. Se i grafi del passato non esistono, vengono automaticamente ricreati.
                                                <ol>
                                                    <li>È possibile passare come posizione e come destinazione dei grafi sia un percorso che un URL. Se viene passato un percorso i grafi vengono recuperati / salvati su un file JSON; se viene passato un URL i grafi vengono recuperati / salvati su un triplestore.</li>
                                                </ol>
                                            </li>
                                            <li>Il metodo <strong>_rebuild_past_graphs</strong> viene lanciato automaticamente nel caso in cui l'utente non abbia specificato la posizione dei grafi del passato. La ricostruzione avviene in due passaggi: prima vengono ricostruiti i delta, poi i dataset interi a partire dai delta.
                                                <ol>
                                                    <li>Il metodo per ricostruire i delta, <strong>_rebuild_deltas</strong>, è il medesimo già discusso in <a href="#d0306cnhc7">03/06/2021, <em>Cosa non ho capito</em>, punto 7</a>.</li>
                                                    <li>Il metodo per ricostruire i dataset del passato completi, <strong>_complete_past_graphs</strong>, è la vera arma di distruzione di hardware, perché richiede di recuperare l'intero dataset presente. A partire da esso, ricostruisce quelli passati nel giusto ordine, cancellando i preexisting graph delle entità di cui sono stati trovati dei delta e sostituendoli con i delta stessi.</li>
                                                </ol>
                                            </li>
                                            <li>Il metodo <strong>_save_past_graphs</strong>, infine, salva i grafi del passato o su file o su triplestore.
                                                <ol>
                                                    <li>I conjunctive graph del passato vengono denominati tramite un URI che ha la forma "https://time_agnostic_browser/[snapshot_generation_time]". Inoltre a ciascuno di questi URI viene associata la data di generazione tramite predicato dcterms:date. Dopodiché, tutti i conjunctive graph del passato così denominati vengono uniti in un unico conjunctive graph.</li>
                                                    <li>Infine, se la destinazione è un triplestore, il metodo genera la stringa di update ed effettua la query, mentre se la destinazione è un file, serializza il conjunctive graph in formato json-ld e lo salva.</li>
                                                </ol>
                                            </li>
                                            <li>Nessuno dei precedenti metodi è esposto all'utente. L'unico metodo esposto all'utente di questa classe è <strong>run_agnostic_query</strong>, che lancia una query su ogni stato della conoscenza presente e passata. L'output è un dizionario, in cui le chiavi sono gli snapshot esistenti, mentre i valori sono set di tuple contenenti i risultati di quella query a quel tempo.
                                                <ol>
                                                    <li>Se la posizione dei grafi del passati è un URI, la query viene effettuata su un triplestore. Prima vengono recuperati tutti i grafi i cui URI hanno una proprietà dcterms:date. Dopodiché, la query viene effettuata cu ciascuno di essi, modificando la query dell'utente e aggiungendo "FROM [URI_del_grafo_passato]" prima del WHERE.</li>
                                                    <li>Se la posizione dei grafi del passati è un percorso, la query viene effettuata su un file. Il contenuto del file viene parsato in un conjunctive graph, vengono recuperati i contesti esistenti e su ciascuno viene effettuata la query dell'utente, senza modificarla.</li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho iniziato a disegnare l'interfaccia del Time Agnostic Browser.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Ho scoperto che il problema discusso in <a href="#d0906cnhc2">09/06/2021, <em>Cosa non ho capito</em>, punto 2</a> è causato da Blazegraph, perché non si verifica lavorando su file. Come faccio a configurare Blazegraph in modo che non aggiunga dei datatype che prima non c'erano?</li>
                                    <li>Qual è il modo migliore per lanciare Blazegraph da Python?</li>
                                    <li>Se voglio indicare Crossref come fonte primaria di uno snapshot, come lo indico? Immagino che l'URL "https://www.crossref.org/" non sia appropriato.</li>
                                    <li>Ha senso specificare la fonte primaria dello snapshot in caso di merge tra due entità?</li>
                                    <li id="i1706cnhc5">
                                        <p>Parliamo adesso della madre di tutti i problemi, ovvero le query agnostiche sul tempo. Dopo essere giunto alla conclusione che per effettuare una query agnostica sul tempo il presente non basti, ho esplorato la possibilità di effettuarla ricostruendo tutti i delta del passato.</p>
                                        <p>L'idea era quella di ottenere i dataset per ogni snapshot al volo, a partire dai delta e dallo stato presente del dataset, utilizzando un meccanismo simile a quello di <strong>Git</strong>, ma con l'asse temporale invertito: Ogni volta che si fa un commit, Git scatta una foto di come appaiono tutti i file in quel momento e memorizza un riferimento a quell'istantanea. Se i file non sono cambiati, Git <strong>non</strong> memorizza nuovamente il file, ma solo un link al precedente file identico che ha già memorizzato. Invertendo l'asse temporale, se un'entità passata è diversa da quella nello snapshot immediatamente successivo, l'entità passata viene ricostruita e salvata; se non è diversa, viene recuperato lo stato immediatamente successivo della risorsa, senza creare copie.</p>
                                        <p>Purtroppo, ad oggi non sono riuscito a replicare questo meccanismo e il codice che ho scritto ricrea i dataset per intero, quindi è impraticabile per dataset con milioni o miliardi di triple. Come faccio a replicare questo meccanismo?</p>
                                    </li>
                                </ol>                
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-15" data-target="#panel-15" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-15">
                            <span class="h6 mb-0 font-weight-bold">25/06/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-15">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Il problema delle query agnostiche sul tempo è stato risolto ed è adesso possibile effettuare un considerevole numero di query a una velocità impensabile fino alla settimana scorsa. Per dare un'idea, effettuare una query agnostica su un triplestore con oltre 2 milioni di triple, 27 snapshot con in media 20,000 entità modificate a snapshot richiedeva circa 24 ore. Ora occorrono pochi istanti.</li>
                                    <li>Sono state testate con successo le seguenti query agnostiche sul tempo, ordinate dalla più semplice alla più complessa:
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    PREFIX pro: &lt;http://purl.org/spar/pro/&gt;
    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
    SELECT DISTINCT ?o
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy ?o;
            rdf:type pro:RoleInTime.
    }   
                                            </code></pre>
                                        </div>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    PREFIX pro: &lt;http://purl.org/spar/pro/&gt;
    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
    SELECT ?o
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy ?o.
        OPTIONAL {&lt;https://github.com/arcangelo7/time_agnostic/ar/4&gt; rdf:type pro:RoleInTime.}
    }
                                            </code></pre>
                                        </div>
                                        <div class="card bg-primary shadow-inset border-light">
                                            <pre><code class="prettyprint">
    PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
    PREFIX pro: &lt;http://purl.org/spar/pro/&gt;
    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
    SELECT DISTINCT ?o ?id ?value
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy ?o.
        OPTIONAL {&lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; rdf:type pro:RoleInTime.}
        ?o datacite:hasIdentifier ?id.
        ?id literal:hasLiteralValue ?value.
    }
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>L'algoritmo adottato per effettuare query agnostiche sul tempo in maniera veloce verrà adesso illustrato nel dettaglio e accompagnato dal relativo codice. È inoltre possibile visionare il codice su <a href="https://github.com/arcangelo7/time-agnostic-browser/blob/main/time_agnostic_browser/agnostic_query.py" target="_blank" alt="Link to the AgnosticQuery class' code">GitHub</a>.
                                        <ol>
                                            <li>La classe <strong>AgnosticQuery</strong> viene istanziata passando come argomento una query.</li>
                                            <li>Al momento dell'inizializzazione, la query viene processata. Vengono anzitutto <strong>estrapolate</strong> le <strong>triple</strong>, sfruttando il metodo <strong>prepareQuery</strong> del processor di rdflib. Dal risultato, è possibile ricavare l'algebra della query. Per ottenere le triple dall'algebra occorre <strong>navigarne l'albero</strong> in maniera <strong>ricorsiva</strong> fino a trovare i valori delle chiavi "triples". In questo modo è possibile gestire qualunque struttura dell'albero, la cui variabilità è notevole a seconda della query.</li>
                                            <li>Il metodo <strong>_process_query</strong> si occupa infine di sollevare un'eccezione ValueError nel caso in cui la query dell'utente non sia valutabile, argomento che viene approfondito al <a href="#i25061cnhc">punto 1 della sezione <em>Cosa non ho capito</em></a>.</li>
                                            <div class="card bg-primary shadow-inset border-light">
                                                <pre><code class="prettyprint">
    def _process_query(self) -> List[Tuple]:
        algebra:CompValue = prepareQuery(self.query).algebra
        if algebra.name != "SelectQuery":
            raise ValueError("Only SELECT queries are allowed.")
        triples = list()
        # The algebra can be extremely variable in case of one or more OPTIONAL in the query: 
        # it is necessary to navigate the dictionary recursively in search of the values of the "triples" keys.
        self._tree_traverse(algebra, "triples", triples)
        triples_with_hook = [triple for triple in triples if isinstance(triple[0], URIRef) or isinstance(triple[2], URIRef)]
        if not triples_with_hook:
            raise ValueError("Could not perform a generic time agnostic query. Please, specify at least one subject entity within the query.")
        triples_with_subject = [triple for triple in triples_with_hook if isinstance(triple[0], URIRef)]
        if not triples_with_subject:
            raise ValueError("Specify at least one subject.")
        triples_with_path = [triple for triple in triples_with_hook if isinstance(triple[1], Path)]
        if triples_with_path:
            raise ValueError("Cannot evaluate time agnostic queries containing paths.")
        return triples

    def _tree_traverse(self, tree:dict, key:str, values:List[Tuple]) -> None:
        for k, v in tree.items():
            if k == key:
                values.extend(v)
            elif isinstance(v, dict):
                found = self._tree_traverse(v, key, values)
                if found is not None:  
                    values.extend(found)
                                                </code></pre>
                                            </div>
                                        </li>
                                        <li>A partire dalle triple viene ricostruita la storia delle entità rilevanti tramite il metodo <strong>_rebuild_relevant_graphs</strong>, in due passaggi: prima vengono ricostruiti i grafi del passato delle entità esplicite nella query, poi vengono ricostruiti anche i grafi delle entità ricavate esplicitando le variabili nella query.</li>
                                        <li>Il metodo <strong>_rebuild_relevant_entity</strong> si occupa appunto di ricostruire la storia di un'entità, verificando che sia effettivamente un'entità, che non sia già stata ricostruita e infine che l'entità sia mai esistita nel dataset.</li>
                                        <li>Infine, <strong>_align_snapshots</strong> allinea gli snapshots di tutte le entità ricostruite. Prima unisce i grafi delle entità basandosi sugli snapshots, dopodiché copia le entità che non sono cambiate se si verificano le seguenti condizioni:
                                            <ol>
                                                <li>l'entità è presente in tn-1 ma non in tn;</li>
                                                <li>l'entità è assente in tn perché non è cambiata e non perché è stata cancellata.</li>
                                            </ol>
                                            <div class="card bg-primary shadow-inset border-light">
                                                <pre><code class="prettyprint">
    def _rebuild_relevant_graphs(self) -> None:
        # First, the graphs of the hooks are reconstructed
        for triple in self.triples:
            self._rebuild_relevant_entity(triple[0])
            self._rebuild_relevant_entity(triple[2])
        self._align_snapshots()
        # Then, the graphs of the entities obtained from the hooks are reconstructed
        self._solve_variables()
    
    def _rebuild_relevant_entity(self, entity:Union[URIRef, Literal]):
        if isinstance(entity, URIRef) and entity not in self.reconstructed_entities:
            agnostic_entity = AgnosticEntity(entity, False)
            entity_history = agnostic_entity.get_history()
            if entity_history[entity]:
                self.relevant_entities_graphs.update(entity_history) 
            self.reconstructed_entities.add(entity)
    
    def _align_snapshots(self) -> None:
        # Merge entities based on snapshots
        for _, snapshots in self.relevant_entities_graphs.items():
            for snapshot, graph in snapshots.items():
                if snapshot in self.relevant_graphs:
                    for quad in graph.quads():
                        self.relevant_graphs[snapshot].add(quad)
                else:
                    self.relevant_graphs[snapshot] = deepcopy(graph)
        # If an entity hasn't changed, copy it
        ordered_data: List[Tuple[str, ConjunctiveGraph]] = sorted(
            self.relevant_graphs.items(),
            key=lambda x: parser.parse(x[0]),
            reverse=False # That is, from past to present, assuming that the past influences the present and not the opposite like in Dark
        )
        for index, se_cg in enumerate(ordered_data):
            se = se_cg[0]
            cg = se_cg[1]
            if index > 0:
                previous_se = ordered_data[index-1][0]
                for subject in self.relevant_graphs[previous_se].subjects():
                    # To copy the entity two conditions must be met: 
                    #   1) the entity is present in tn but not in tn+1; 
                    #   2) the entity is absent in tn+1 because it has not changed 
                    #      and not because it has been deleted.
                    if (subject, None, None, None) not in cg:
                        if subject in self.relevant_entities_graphs:
                            if se not in self.relevant_entities_graphs[subject]:
                                for quad in self.relevant_graphs[previous_se].quads((subject, None, None, None)):
                                    self.relevant_graphs[se].add(quad)
                                            </code></pre>
                                        </div>
                                        </li>
                                        <li>Il metodo <strong>_solve_variables</strong> si occupa di ricostuire i grafi delle entità che nella query appaiono come variabili, declinati nei vari tempi.</li>
                                        <li><strong>_get_vars_to_explicit_by_time</strong>, basandosi sui grafi degli hooks e sulle triple estrapolate dalla query, genera il dizionario self.vars_to_explicit_by_time, in cui agli snapshots vengono associati dei dizionari che hanno come chiavi le variabili della query e come valore le triple in cui quelle variabili compaiono.</li>
                                        <li><strong>_explicit_solvable_variables</strong> si occupa appunto di esplicitare le variabili nelle triple del dizionario precedentemente generato e, contemporaneamente, di aggiornare la proprietà self.relevant_graphs con i grafi ricostruiti delle entità nel frattempo esplicitate.</li>
                                        <li>Infine, <strong>_update_vars_to_explicit</strong> aggiorna il dizionario self.vars_to_explicit_by_time sostituendo alle variabili gli URI scoperti, così da poter ripetere il ciclo finché non vengono scoperti tutti.
                                            <div class="card bg-primary shadow-inset border-light overflow-auto">
                                                <pre><code class="prettyprint">
    def _solve_variables(self) -> None:
        runs = self._get_vars_to_explicit_by_time()
        while runs:
            solved_variables = self._explicit_solvable_variables()
            if not solved_variables:
                return 
            self._update_vars_to_explicit(solved_variables)
            runs -= 1
    
    def _get_vars_to_explicit_by_time(self) -> int:
        number_of_vars = set()
        for se, _ in self.relevant_graphs.items():
            self.vars_to_explicit_by_time[se] = dict()
            for triple in self.triples:
                variables = [el for el in triple if isinstance(el, Variable)]
                number_of_vars.update(variables)
                for variable in variables:
                    self.vars_to_explicit_by_time[se].setdefault(variable, list()) 
                    self.vars_to_explicit_by_time[se][variable].append(triple)
        return len(number_of_vars)
    
    def _explicit_solvable_variables(self) -> Dict[str, Dict[str, str]]:
        explicit_triples:Dict[str, Dict[str, str]] = dict()
        for se, vars in self.vars_to_explicit_by_time.items():
            for var, triples in vars.items():
                for triple in triples:
                    solvable_triple = [el.n3() for el in triple]
                    variables = [x for x in solvable_triple if x.startswith("?")]
                    if len(variables) == 1:
                        variable = variables[0]
                        variable_index = solvable_triple.index(variable)
                        query_to_identify = f"""
                            CONSTRUCT {{{solvable_triple[0]} {solvable_triple[1]} {solvable_triple[2]}}}
                            WHERE {{{solvable_triple[0]} {solvable_triple[1]} {solvable_triple[2]}}}
                        """
                        results = self.relevant_graphs[se].query(query_to_identify)
                        for result in results:
                            explicit_var = result[variable_index]
                            explicit_triples.setdefault(se, dict())
                            explicit_triples[se][var] = explicit_var
                            self._rebuild_relevant_entity(explicit_var)
                        self._align_snapshots()
        return explicit_triples
    
    def _update_vars_to_explicit(self, solved_variables:Dict[str, Dict[str, str]]) -> None:
        for se, vars in self.vars_to_explicit_by_time.items():
            for var, triples in vars.items():
                for triple in triples:
                    for explicit_var, explicit_el in solved_variables[se].items():
                        new_triple = tuple(explicit_el if el == explicit_var else el for el in triple)
                        new_list_of_triples = [new_triple if x == triple else x for x in self.vars_to_explicit_by_time[se][var]]
                        self.vars_to_explicit_by_time[se][var] = new_list_of_triples
                                            </code></pre>
                                        </div>
                                        </li>
                                        <li>Una volta che i grafi rilevanti per risolvere la query sono stati ricostruiti, effettuare la query agnostica sul tempo diventa un problema triviale: <strong>run_agnostic_query</strong> cicla su tutti gli snapshot e su tutti i grafi ricostruiti e restituisce un dizionario in cui a ciascuno snapshot sono associati i set di tuple corrispondenti al risultato di quella query in quel tempo.
                                            <div class="card bg-primary shadow-inset border-light overflow-auto">
                                                <pre><code class="prettyprint">
    def run_agnostic_query(self) -> Dict[str, Set[Tuple]]:
        agnostic_result = dict()
        for snapshot, graph in self.relevant_graphs.items():
            results = graph.query(self.query)
            output = set()
            for result in results:
                result_tuple = tuple(str(var) for var in result)
                output.add(result_tuple)
            agnostic_result[snapshot] = output
        return agnostic_result
                                                </code></pre>
                                            </div>
                                        </li>
                                    </ol>
                                    <li>Tutti i nuovi metodi introdotti sono stati testati con successo.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="i25061cnhc">Ci sono query agnostiche che non è possibile effettuare con l'approccio adottato, ovvero le seguenti tre categorie di query:
                                        <ol>
                                            <li>Query che non specificano alcun URI come soggetto o oggetto.</li>
                                            <li id="i25061_2cnhc">Query in cui gli URI specificati sono solo oggetti. Occorrerebbe ricostruire tutte le entità del dataset per scoprire quali entità hanno avuto quell'oggetto nel passato.</li>
                                            <li>Query in cui sono presenti dei path. Quest'ultimo caso richiede un discorso a parte e viene affrontato nel prossimo punto.</li>
                                        </ol>
                                        <p>Ritieni che la libreria debba supportare anche queste tre tipologie di query? Se no, la libreria solleva già dei ValueError. Se sì, mi piacerebbe discutere il come.</p>
                                    </li>
                                    <li>Discutiamo il come relativo alle <strong>property paths</strong>. Esistono cinque tipi di property paths: sequence path, alternative path, multiple path, inverse path e negated path (<a href="https://www.w3.org/2009/sparql/docs/property-paths/Overview.xml" target="_blank" alt="Link to the property paths documentation">https://www.w3.org/2009/sparql/docs/property-paths/Overview.xml</a>):
                                        <ol>
                                            <li>Una <strong>sequence path</strong> ha la forma elt1 / elt2, dove elt è un elemento del path. È il caso più semplice e può essere risolto riscrivendo la query aggiungendo delle variabili placeholder tra un elemento del path e l'altro. In questo modo:
                                                <div class="card bg-primary shadow-inset border-light overflow-auto">
                                                    <pre><code class="prettyprint">
    PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
    PREFIX pro: &lt;http://purl.org/spar/pro/&gt;
    SELECT DISTINCT ?value
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy/datacite:hasIdentifier/literal:hasLiteralValue ?value.
    }
    
    # La precedente query può essere riscritta come:

    SELECT DISTINCT ?value
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ar/15519&gt; pro:isHeldBy ?a.
            ?a datacite:hasIdentifier ?b.
            ?b literal:hasLiteralValue ?value.
    }
                                                    </code></pre>
                                                </div>
                                            </li>
                                            <li>Una <strong>multiple path</strong> ha la forma elt* o elt+ o elt? o elt{n,m} o elt{n} o elt{n,} o elt{,n}. Questo caso è simile al precedente, con la differenza che il numero di variabili placeholder da inserire non è noto. Una soluzione potrebbe essere inserire variabili placeholder in maniera progressiva, risolverle, ricostruire le relative entità e continuare questo processo finché il CONSTRUCT ricavato dalla query dell'utente non porta a identificare un'entità concreta. <p>Problema: cosa succede se non è possibile risolvere tutte le variabili perché la query è sbagliata? Con quale criterio si evita il loop infinito?</p></li>
                                            <li>Un'<strong>alternative path</strong> ha la forma elt1 | elt2. Una soluzione potrebbe essere quella di valutare tutte le possibili alternative e di ricostruire tutte le entità così ottenute.</li>
                                            <li>Un'<strong>inverse path</strong> ha la forma ^elt. L'inverse path trasforma il soggetto in oggetto e genera un caso riconducibile a quello già discusso al <a href="#i25061_2cnhc">punto 1.2 di questa sezione</a>, per il quale non vedo soluzioni che non siano ricostruire tutti i dataset del passato.</li>
                                            <li>Una <strong>negated path</strong> ha la forma !uri o !(uri1|...|urin). Questo caso non sembra costituire alcun problema. Tutti gli oggetti collegati a un soggetto noto da URI diversi da quelli negati vengono ricostruiti.</li>
                                        </ol>
                                    </li>
                                    <li>
                                        <p>C'è un altro problema collegato alle property path e cioè che queste possono essere combinate tra di loro. La priorità con cui vengono valutate è la seguente, dalla più alta alla più bassa: negated path, multiple path, inverse path, sequence path, alternative path (<a href="https://www.w3.org/2009/sparql/docs/property-paths/Overview.xml" target="_blank" alt="Link to the property paths documentation">https://www.w3.org/2009/sparql/docs/property-paths/Overview.xml</a>). Tuttavia, poiché lo scopo nel parsare la query non è quello di valutarla ma quello di estrapolare le entità rilevanti, non sembra necessario rispettare tale priorità, di cui già si occupano rdflib e il triplestore.</p>
                                        <p>Poniamo il seguente caso, che comprende una sequence path e una multiple path.</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    PREFIX fabio: &lt;http://purl.org/spar/fabio/&gt;
    PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
    PREFIX frbr: &lt;http://purl.org/vocab/frbr/core#&gt;
    PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    PREFIX cito: &lt;http://purl.org/spar/cito/&gt;
    
    SELECT DISTINCT ?citingDOI ?citedDOI
    WHERE {
        ?res a fabio:Expression;
            datacite:hasIdentifier ?doiEntity;
            frbr:partOf+/datacite:hasIdentifier/literal:hasLiteralValue "0138-9130".
        ?doiEntity literal:hasLiteralValue ?citingDOI.
        OPTIONAL {
            ?res cito:cites/datacite:hasIdentifier/literal:hasLiteralValue ?citedDOI.
        }
    }
    
    # Può essere riscritta come
    
    SELECT DISTINCT ?citingDOI ?citedDOI
    WHERE {
        ?res a fabio:Expression;
            datacite:hasIdentifier ?doiEntity;
            frbr:partOf ?a
            ?a datacite:hasIdentifier ?b.
            ?b literal:hasLiteralValue "0138-9130".
        ?doiEntity literal:hasLiteralValue ?citingDOI.
        OPTIONAL {
            ?res cito:cites ?c.
                    ?c datacite:hasIdentifier ?d.
                    ?d literal:hasLiteralValue ?citedDOI.
        }
    }
    
    # Nel caso in cui il CONSTRUCT ricavato dalla query 
    # non porti alla risoluzione di tutte le variabili, 
    # la query viene ulteriormente riscritta nel seguente modo:
    
    SELECT DISTINCT ?citingDOI ?citedDOI
    WHERE {
        ?res a fabio:Expression;
            datacite:hasIdentifier ?doiEntity;
            frbr:partOf ?a1.
            ?a1 frbr:partOf ?a.
            ?a datacite:hasIdentifier ?b.
            ?b literal:hasLiteralValue "0138-9130".
        ?doiEntity literal:hasLiteralValue ?citingDOI.
        OPTIONAL {
            ?res cito:cites ?c.
                    ?c datacite:hasIdentifier ?d.
                    ?d literal:hasLiteralValue ?citedDOI.
        }
    }
    
    # E così via finché non vengono risolte tutte le variabili
                                            </code></pre>
                                        </div>
                                        <p>Cosa ne pensi di questo ragionamento?</p>
                                    </li>
                                    <li>Per quanto riguarda l'interfaccia del browser, vorrei discutere un aspetto di usabilità che potrebbe andare a discapito della sua genericità: se voglio mostrare all'utente dei metadati  <em>human comprehensible</em> e non delle triple devo tradurre gli URI in inglese. Questa operazione è però specifica per l'OpenCitations Data Model, non è generica. Cosa ne pensi?</li>
                                </ol>                
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-16" data-target="#panel-16" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-16">
                            <span class="h6 mb-0 font-weight-bold">30/06/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-16">
                            <div class="pt-3">
                                <ol>
                                    <li>È ora possibile effettuare query agnostiche sul tempo contenenti triple isolate. Una tripla è isolata quando si verificano le seguenti condizioni:
                                        <ol>
                                            <li>La tripla contiene una variabile in luogo di soggetto.</li>
                                            <li>La variabile del soggetto non compare come oggetto in nessun'altra tripla della query.</li>
                                        </ol>
                                    </li>
                                    <li>Il metodo <strong>_is_isolated</strong> si occupa di verificare se una tripla è isolata o meno, con il seguente codice:
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    def _is_isolated(self, triple:tuple) -> bool:
        if isinstance(triple[2], Variable) and isinstance(triple[0], URIRef):
            return False
        variables = [el for el in triple if isinstance(el, Variable)]
        if not variables:
            return False
        for variable in variables:
            other_triples = {t for t in self.triples if t != triple}
            for other_triple in other_triples:
                if variable in other_triple and other_triple.index(variable) == 2:
                    return False
        return True
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>Tutte e sole le triple isolate vengono processate separatamente in due passaggi:
                                        <ol>
                                            <li>Prima vengono identificate le entità rilevanti nel presente, che potrebbero non comparire in nessuna query di update nel caso in cui non abbiano mai subito modifiche.</li>
                                            <li>Dopodiché vengono ricercate le entità rilevanti nelle query di update. Se ne occupa il metodo <strong>_find_entities_in_update_queries</strong>, con il seguente codice:
                                                <div class="card bg-primary shadow-inset border-light overflow-auto">
                                                    <pre><code class="prettyprint">
    def _find_entities_in_update_queries(self, triple:tuple):
        uris_in_triple = {el for el in triple if isinstance(el, URIRef)}
        relevant_entities_found = set()
        query_to_identify = f"""
            PREFIX oco: &lt;https://w3id.org/oc/ontology/&gt;
            PREFIX prov: &lt;http://www.w3.org/ns/prov#&gt;
            SELECT DISTINCT ?updateQuery 
            WHERE {{
                ?snapshot oco:hasUpdateQuery ?updateQuery.
        """ 
        for uri in uris_in_triple:
            if triple.index(uri) == 0 or triple.index(uri) == 2:
                self._rebuild_relevant_entity(uri)
            query_to_identify += f"FILTER CONTAINS (?updateQuery, '&lt;{uri}&gt;')"
        query_to_identify += "}"
        results = Sparql(query_to_identify).run_select_query()
        pbar = tqdm(total=len(results))
        print(f"[AgnosticQuery:INFO] Searching for relevant entities in relevant update queries.")
        for result in results:
            try:
                update = parseUpdate(result[0])
                for request in update["request"]:
                    for quadsNotTriples in request["quads"]["quadsNotTriples"]:
                        for triple in quadsNotTriples["triples"]:
                            triple = [el["string"] if "string" in el else el for el in triple]
                            relevant_entities = (set(triple).difference(uris_in_triple) 
                                if len(uris_in_triple.intersection(triple)) == len(uris_in_triple) else None)
                            if relevant_entities is not None:
                                relevant_entities_found.update(relevant_entities)
                        pbar.update(1)
            except RecursionError:
                # print(f"[AgnosticQuery:INFO] The following update query raised a RecursionError: {result[0]}")
                pbar.update(1)
        pbar.close()
        print(f"[AgnosticQuery:INFO] Rebuilding relevant entities' history.")
        pbar = tqdm(total=len(relevant_entities_found))
        for relevant_entity_found in relevant_entities_found:
            self._rebuild_relevant_entity(relevant_entity_found)
            pbar.update(1)
        pbar.close()
                                                    </code></pre>
                                                </div>
                                                <p><strong>Nota bene</strong>: le query di update vengono parsate tramite il metodo <em>parseUpdate</em> del processor di rdflib, per cui non è necessario che le triple compaiano come triple e possono presentare prefissi. Tuttavia, com'è noto, il metodo crasha in caso di query di update contenenti più di 90 triple. Per quegli sfortunati casi, è presente un try except che salta quelle query e con esse le entità al loro interno. A questo proposito, si veda il <a href="#i3006cnhc3">punto 3 della sezione <em>Cosa non ho capito</em></a>.</p>
                                                <p>Il metodo <em>_find_entities_in_update_queries</em> verrà prossimamente migliorato rendendo la query una variabile impostabile tramite file di configurazione, che permetta di specificare i predicati "magici" di Blazegraph per effettuare ricerche testuali. Inoltre, il metodo verrà migliorato aggiungendo un parametro che permetta di specificare il tipo di entità rilevanti.</p>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ecco tre query agnostiche sul tempo prima impossibili che sono state testate con successo. 
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    # Query in cui è presente una tripla con una variabile come soggetto, 
    # variabile che non è ricavabile dal resto della query. 
    # Solo tale tripla isolata viene esplicitata cercando tra le query di update.
    
    prefix literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
    prefix pro: &lt;http://purl.org/spar/pro/&gt;
    SELECT DISTINCT ?a ?id
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ra/4&gt; datacite:hasIdentifier ?id.
        ?id literal:hasLiteralValue ?literalValue.
        OPTIONAL {?a pro:isHeldBy &lt;https://github.com/arcangelo7/time_agnostic/ra/15519&gt;.}
    }
                                            </code></pre>
                                        </div>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    # Query simile alla precedente, ma in cui la tripla isolata ha 
    # come variabili sia il soggetto che il predicato
        
    prefix literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
    prefix pro: &lt;http://purl.org/spar/pro/&gt;
    SELECT DISTINCT ?s ?p
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/ra/4&gt; datacite:hasIdentifier ?id.
        ?id literal:hasLiteralValue ?literalValue.
        OPTIONAL {?s ?p &lt;https://github.com/arcangelo7/time_agnostic/ra/15519&gt;.}
    }
                                            </code></pre>
                                        </div>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    # Query in cui una tripla isolata ha come variabili sia il soggetto che l'oggetto.
    # Quest'ultima query funziona, ma richiede ore per essere portata a termine, 
    # mentre le precedenti si completano nell'arco di pochi istanti o secondi a parità 
    # di dataset. 

    prefix pro: &lt;http://purl.org/spar/pro/&gt;
    SELECT DISTINCT ?s ?o
    WHERE {
        ?s pro:isHeldBy ?o.
    }        
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>Il metodo <strong>_solve_variables</strong> è stato reso <strong>più efficiente</strong>, poiché conclude il ciclo appena sono state esplicitate tutte le variabili o finché non si è raggiunto un punto morto, come in caso di entità inesistenti. Prima ciclava tante volte quante erano le variabili, ma c'erano dei casi in cui alcuni cicli erano a vuoto poiché in solo ciclo veniva esplicitata più di una variabile. Il controllo sulla presenza di variabili avviene tramite il nuovo metodo <strong>_there_are_variables</strong>, con il seguente codice:
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    def _there_are_variables(self) -> bool:
        for _, variables in self.vars_to_explicit_by_time.items():
            for _, triples in variables.items():
                for triple in triples:
                    vars = [el for el in triple if isinstance(el, Variable)]
                    if vars:
                        return True
        return False
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>Risolto un bug nel codice per costruire il dataset per cui non veniva creato il collegamento tra le entità di tipo fabio:Expression e quelle di tipo pro:RoleInTime nel caso in cui l'<em>agent role</em> fosse pro:withRole pro:publisher. Il bug è stato individuato da Cristian.</li>
                                    <li>Il codice per costruire il dataset e la provenance specifica ora la fonte delle entità tramite un preciso riferimento all'URL dell'API da cui sono state ricavate le informazioni.</li>
                                    <li>Gli oggetti della classe Sparql si istanziano adesso passando la query come argomento. In questo modo si evita molto codice ripetuto, dato che la query viene processata nell'inizializzazione e non in ogni singolo metodo.</li>
                                    <li>Tutti i nuovi metodi introdotti e quelli modificati sono stati testati con esito positivo.</li>
                                </ol>
                            <h3>Cosa non ho capito</h3>
                            <ol>
                                <li>Se la provenance si trova insieme ai dati nello stesso triplestore o nello stesso file, è possibile che da una query agnostica sul tempo vengano ritornati elementi legati alla provenance. Bisogna ometterli? Se non li si omette, ecco un possibile output:
                                    <div class="card bg-primary shadow-inset border-light overflow-auto">
                                        <pre><code class="prettyprint">
    # Data la query 
    query = """
    SELECT DISTINCT ?s ?p
    WHERE {
        ?s ?p &lt;https://github.com/arcangelo7/time_agnostic/ra/4&gt;.
    }
    """
    
    # L'output di una query agnostica è il seguente
    output = {
        '2021-05-07 09:59:15': {('https://github.com/arcangelo7/time_agnostic/ar/4',
                                'http://purl.org/spar/pro/isHeldBy'),
                                ('https://github.com/arcangelo7/time_agnostic/ra/4/prov/se/1',
                                'http://www.w3.org/ns/prov#specializationOf')},
        '2021-05-31 18:19:47': {('https://github.com/arcangelo7/time_agnostic/ar/4',
                                'http://purl.org/spar/pro/isHeldBy'),
                                ('https://github.com/arcangelo7/time_agnostic/ra/4/prov/se/1',
                                'http://www.w3.org/ns/prov#specializationOf')},
        '2021-06-01 18:46:41': {('https://github.com/arcangelo7/time_agnostic/ar/15519',
                                'http://purl.org/spar/pro/isHeldBy'),
                                ('https://github.com/arcangelo7/time_agnostic/ar/4',
                                'http://purl.org/spar/pro/isHeldBy'),
                                ('https://github.com/arcangelo7/time_agnostic/ra/4/prov/se/1',
                                'http://www.w3.org/ns/prov#specializationOf'),
                                ('https://github.com/arcangelo7/time_agnostic/ra/4/prov/se/2',
                                'http://www.w3.org/ns/prov#specializationOf')}
    }
                                        </code></pre>
                                    </div>
                                </li>
                                <li>Chi lancia il triplestore? La libreria o l'utente? Se l'utente, occorre istruirlo su come ricostruire l'indice testuale del triplestore. Se la libreria, il problema è complesso, perché l'utente dovrebbe fornire nel file di configurazione non l'URL del triplestore ma la cartella in cui si trova e il tipo di triplestore, dopodiché la libreria dovrebbe cercare tutti i file compatibili all'interno di tutte le cartelle fornite a aprire le connessioni cambiando di volta in volta la porta adoperata, dopo aver verificato se la porta è già in uso.</li>
                                <li id="i3006cnhc3">L'issue su rdflib che ho aperto riguardo problemi di ricorsione in caso di query di update con più di 90 triple ha portato a un fix: <a href="https://github.com/RDFLib/rdflib/issues/1336" alt="Link to the rdflib issue about RecursionError" target="_blank">https://github.com/RDFLib/rdflib/issues/1336</a>. Tuttavia, non è ancora stata rilasciata una nuova versione della libreria che includa il fix. In passato, la libreria ha avuto una release ogni due / tre anni e l'ultima è stata nel 2020. Una soluzione temporanea, che però mi sembra assurda dato che la soluzione vera esiste già, è applicare nel nuovo contesto il metodo da me già realizzato che spezza la query di update in più parti e ne parsa le parti. Che ne pensi?</li>
                                <li>Nel caso in cui la query agnostica sul tempo contenga una o più triple isolate in cui viene specificato solo il predicato il tempo di esecuzione si misura in ore se il dataset contiene milioni di triple.
                                    La ragione è che sono numerose le query di update che possono contenere un singolo predicato e, anche specificando il tipo del soggetto e dell'oggetto, <strong>non</strong> si passa dalle ore ai secondi, ma occorrono comunque ore. 
                                    Pensi che sia un compromesso accettabile o ritieni che occorra individuare una strategia ancora più scaltra di quella già scaltra adottata?
            
                                    <p>La difficoltà nel portare a termine questo genere di query è la ragione per cui non sono ancora riuscito a testare il seguente boss di fine livello:
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    # Dammi tutte le entità che nella storia hanno avuto due diversi id associati 
    # e dimmi quando questa condizione si è verificata. 

    prefix cito: &lt;http://purl.org/spar/cito/&gt;
    prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
    prefix literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    select distinct ?elt_1
    where {
        ?elt_1 datacite:hasIdentifier ?id_1;
                datacite:hasIdentifier ?id_2.
        ?id_1 literal:hasLiteralValue ?literal_1.
        ?id_2 literal:hasLiteralValue ?literal_2.
        FILTER (?literal_1 != ?literal_2)
    } 
                                            </code></pre>
                                        </div>
                                    </p>
                                </li>
                            </ol>            
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-17" data-target="#panel-17" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-17">
                            <span class="h6 mb-0 font-weight-bold">07/07/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-17">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>
                                        <p>I metodi che restituiscono la storia di un'entità e delle entità correlate ricevono un nuovo argomento, <strong>include_prov_metadata</strong> che, se impostato a True, permette di ottenere metadati sulla provenance. Nella fattispecie, il metodo <strong>get_history</strong> ritorna ora sempre una tupla, il cui primo elemento è la risposta secca a quale sia la storia dell'entità secondo la struttura già descritta in passato, il secondo elemento è un dizionario con i metadati sulla provenance. Se include_prov_metadata è False, la tupla ha un elemento, se True ne ha due. Ecco un esempio di possibile output:</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    (
        {
            'https://github.com/arcangelo7/time_agnostic/ar/15519': {
                '2021-06-01T18:46:41': ConjunctiveGraph, 
                '2021-05-07T09:59:15': ConjunctiveGraph, 
                '2021-05-31T18:19:47': ConjunctiveGraph
            }
        },
        {
            'https://github.com/arcangelo7/time_agnostic/ar/15519': {
                'https://github.com/arcangelo7/time_agnostic/ar/15519/prov/se/3': {
                    'http://www.w3.org/ns/prov#generatedAtTime': '2021-06-01T18:46:41', 
                    'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696', 
                    'http://www.w3.org/ns/prov#hadPrimarySource': None 
                }, 
                'https://github.com/arcangelo7/time_agnostic/ar/15519/prov/se/1': {
                    'http://www.w3.org/ns/prov#generatedAtTime': '2021-05-07T09:59:15', 
                    'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696', 
                    'http://www.w3.org/ns/prov#hadPrimarySource': None
                }, 
                'https://github.com/arcangelo7/time_agnostic/ar/15519/prov/se/2': {
                    'http://www.w3.org/ns/prov#generatedAtTime': '2021-05-31T18:19:47', 
                    'http://www.w3.org/ns/prov#wasAttributedTo': 'https://orcid.org/0000-0002-8420-0696', 
                    'http://www.w3.org/ns/prov#hadPrimarySource': None
                }
            }
        }
    )
                                            </code></pre>
                                        </div>    
                                    </li>
                                    <li>
                                        <p>È stato risolto un bug per cui non veniva gestito il caso in cui una variabile all'interno di una query agnostica, oltre ad assumere valori diversi in diversi tempi, assumeva valori diversi nello stesso tempo. Ecco un esempio di query prima impossibile e ora testata e funzionante:</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    # La variabile ?br assume tanti valori quante sono le risorse bibliografiche citate
    # da &lt;br/2&gt; in ciascun tempo. Ognuno di questi valori viene adesso considerato
    # per individuare i rispettivi ?id e ?value. 
    
    PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    PREFIX cito: &lt;http://purl.org/spar/cito/&gt;
    PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
    SELECT DISTINCT ?br ?id ?value
    WHERE {
        &lt;https://github.com/arcangelo7/time_agnostic/br/2&gt; cito:cites ?br.
        ?br datacite:hasIdentifier ?id.
        ?id literal:hasLiteralValue ?value.
    }
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        <p>vars_to_explicit_by_time, ovvero la struttura dati che memorizza il valore delle variabili in una query in tempi diversi e nello stesso tempo, è stata semplificata. Le triple non vengono più suddivise per tempo e per variabile, ma solo per tempo, dato che la seconda suddivisione si è rivelata non solo superflua, ma prona a causare ripetizioni e inefficienza.</p>
                                    </li>
                                    <li>
                                        <p>La definizione di tripla isolata è stata perfezionata: una triade si dice isolata quando per nessuno dei suoi nodi esiste una relazione transitiva con gli altri nodi del grafo. Nella fattispecie, il grafo è quello costituito dalle triple all'interno della query. È stato quindi introdotto il metodo <strong>_there_is_transitive_closure</strong>, che cerca in maniera ricorsiva un cammino tra una data variabile e un URI soggetto all'interno della query: se lo trova ritorna True, altrimenti ritorna False.</p>
                                    </li>
                                    <li>
                                        <p>È adesso possibile specificare il <strong>tipo di entità</strong> di cui interessa risolvere una query agnostica sul tempo. I tipi vengono specificati in un set e agiscono come filtri, in questo modo:</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    query = """
        prefix cito: &lt;http://purl.org/spar/cito/&gt;
        prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
        prefix literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
        select distinct ?elt_1
        where {
            ?elt_1 datacite:hasIdentifier ?id_1;
                datacite:hasIdentifier ?id_2.
            ?id_1 literal:hasLiteralValue ?literal_1.
            ?id_2 literal:hasLiteralValue ?literal_2.
            FILTER (?literal_1 != ?literal_2)
        } 
    """
    entity_types = {"http://purl.org/spar/fabio/Expression"}
    agnostic_query = AgnosticQuery(query, entity_types)
    agnostic_query.run_agnostic_query()
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>
                                        <p>È adesso possibile velocizzare le query agnostiche sul tempo utilizzando la funzionalità di <strong>ricerca testuale</strong> messa a disposizione da <strong>Blazegraph</strong>. Nella fattispecie, è stata introdotta la classe <strong>BlazegraphQuery</strong>, figlia di AgnosticQuery, che ne eredita tutti i metodi e le proprietà andando a sovrascrivere il metodo <strong>_get_query_to_identify</strong>, che serve appunto a generare la query per cercare le updateQuery rilevanti. L'utente informa la libreria di aver ricostruito l'indice testuale del triplestore passando un valore affermativo nel campo "<strong>blazegraph_full_text_search</strong>" del file di configurazione, che di default è settato a "no", in questo modo:</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    {
        "dataset": {
            "triplestore_urls": ["http://localhost:9999/blazegraph/sparql"],
            "file_paths": []
        },
        "provenance": {
            "triplestore_urls": ["http://localhost:9999/blazegraph/sparql"],
            "file_paths": []
        },
        "blazegraph_full_text_search": "no"
    }
                                            </code></pre>
                                        </div>
                                        <p>Poiché per l'essere umano <strong>riconoscere è più facile che ricordare</strong>, la voce del file di configurazione è esplicita e l'utente deve solo decidere se settarla a vera o a falsa. Il valore di default è "no", non "false", perché la logica booleana potrebbe non essere di immediata comprensione. Ad ogni modo, la libreria accetta un gran numero di valori, convertendoli internamente a True o False, ovvero: "true", "1", 1, "t", "y", "yes", "ok", "false", "0", 0, "n", "f", "no". Infine, i valori sono case insensitive.</p>
                                        <ol>
                                            <li>Perché introdurre una nuova classe e contemporaneamente una nuova voce nel file di configurazione per una singola funzionalità? Non sarebbe stato sufficiente modificare la classe originale? Sì, sarebbe stato sufficiente ma, immaginando di aggiungere molte nuove funzionalità in futuro, avere delle classi dedicate e delle voci nel file di configurazione renderà il codice più modulare, mantenibile e facile da espandere. Per esempio, è facile immaginare che in futuro possano essere aggiunti altri acceleratori per lo stesso triplestore e per triplestore diversi.</li>
                                        </ol>
                                    </li>
                                    <li>
                                        <p>La ricerca di entità rilevanti nelle query di update è una procedura dispendiosa e deve avvenire solo se è strettamente necessario. È stato quindi aggiunto un nuovo controllo: non solo la tripla dev'essere isolata, ma dev'essere anche nuova e non deve contenere gli stessi URI di una tripla già analizzata. Ad esempio, nella seguente query, le triple segnate con # 1 / # 2 e # 3 / #4 porterebbero allo stesso esito in caso di ricerca di quegli URI nelle query di update:</p>
                                        <div class="card bg-primary shadow-inset border-light overflow-auto">
                                            <pre><code class="prettyprint">
    prefix cito: &lt;http://purl.org/spar/cito/&gt;
    prefix datacite: &lt;http://purl.org/spar/datacite/&gt;
    prefix literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
    select distinct ?elt_1
    where {
        ?elt_1 datacite:hasIdentifier ?id_1;      # 1
            datacite:hasIdentifier ?id_2.         # 2
        ?id_1 literal:hasLiteralValue ?literal_1. # 3
        ?id_2 literal:hasLiteralValue ?literal_2. # 4
        FILTER (?literal_1 != ?literal_2)
    }
                                            </code></pre>
                                        </div>
                                    </li>
                                    <li>Novità relative all'interfaccia:
                                        <ol>
                                            <li>
                                                <p>Il <strong>tempo di generazione</strong> degli snapshot viene mostrato in formato <strong>leggibile</strong> e accompagnato da un'etichetta esplicita, ad esempio: <strong>Generated at time</strong>: 07 May 2021, 09:59:15.</p>
                                            </li>
                                            <li>
                                                <p>Vengono mostrati più metadati sulla provenance, ovvero l'<strong>agente responsabile</strong> e la <strong>fonte dei dati</strong>, oltre al già presente tempo di generazione. Inoltre, l'agente responsabile e la fonte dei dati diventano dei link nel caso siano URL.</p>
                                            </li>
                                            <li>
                                                <p>Nella vista sulla storia di un'entità, il nome dell'entità viene ripetuto in ogni card relativa a un suo snapshot, perché il contesto rimanga esplicito in ogni posizione della pagina.</p>
                                            </li>
                                            <li>
                                                <p>È stato introdotto un <strong>file di configurazione</strong> per personalizzare l'interfaccia.</p>
                                                <ol>
                                                    <li>È possibile indicare gli URI base all'interno di una lista. Se ne viene indicato almeno uno, solo gli elementi contenenti quell'URI base vengono mostrati dall'interfaccia come link. Se non viene indicato alcun URI, si assume che tutti gli URL siano entità e tutti vengono mostrati come link. L'URI base viene anche sfruttato per mostrare le entità in maniera leggibile, omettendo appunto l'URI base stesso. L'URI completo viene mostrato in un tooltip.</li>
                                                    <li>È possibile indicare delle <strong>regole sull'ordine delle proprietà</strong>. Le regole sono specifiche per il tipo di entità e consistono in un dizionario nella forma [nome_proprietà]: [intero], dove il valore è un numero intero indicante la posizione dell'elemento. Il tipo di entità è impostato di default a 0.  Se non viene impostata alcuna regola, prevale l'ordine alfabetico delle proprietà.</li>
                                                    <li>Ecco un esempio di file di configurazione:
                                                        <div class="card bg-primary shadow-inset border-light overflow-auto pr-5">
                                                            <pre><code class="prettyprint">
    {
        "base_urls": ["https://github.com/arcangelo7/time_agnostic/"],
        "rules_on_properties_order": {
            "http://purl.org/spar/datacite/Identifier": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0,
                "http://purl.org/spar/datacite/usesIdentifierScheme": 1
            },
            "http://purl.org/spar/pro/RoleInTime": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0,
                "http://purl.org/spar/pro/withRole": 1    
            },
            "http://purl.org/spar/biro/BibliographicReference": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0
            },
            "http://purl.org/spar/fabio/Expression": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0
            },
            "http://purl.org/spar/cito/Citation": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0
            },
            "http://purl.org/spar/fabio/Manifestation": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0
            },
            "http://xmlns.com/foaf/0.1/Agent": {
                "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": 0
            }
        }
    }
                                                            </code></pre>
                                                        </div>
                                                    </li>
                                                </ol>
                                            </li>
                                            <li>
                                                <p>I <strong>predicati</strong> vengono mostrati in formato <strong>leggibile</strong> automaticamente, prendendo l'ultimo elemento dopo uno "/", poi l'ultimo dopo un "#" e infine separando il camel case e aggiungendo la maiuscola al primo termine e la minuscola ai successivi. Inoltre, al passaggio del mouse viene mostrato un tooltip contenente l'URI originale. Lo stesso discorso vale anche per gli <strong>oggetti</strong>, ma solo se nel file di configurazione è stato specificato l'URI base, altrimenti gli oggetti vengono mostrati per esteso. Infine, le stringhe che dopo le varie trasformazioni risultano appartenere a una lista nota di acronimi (e.g. ORCID, DOI), vengono mostrati con tutte le lettere maiuscole, mentre i rispettivi valori letterali vengono mostrati come link.</p>
                                            </li>
                                            <li>
                                                <p>I caratteri riportati da Crossref in formato <strong>Unicode</strong> vengono convertiti in formato stringa.</p>
                                            </li>
                                            <li>
                                                <p>È ora possibile effettuare <strong>query agnostiche sul tempo</strong> tramite l'interfaccia.</p>
                                            </li>
                                        </ol>
                                    </li>
                                </ol>
                            <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        <p>Per ovviare alla lentezza delle query agnostiche effettuate su grafi in locale, abbiamo discusso della possibilità di caricare i grafi del passato su un triplestore creato ad hoc. Problema: per farlo c'è bisogno del file .jar di Blazegraph. Pensi che la libreria dovrebbe scaricarlo al volo, magari sfruttando il multithreading?</p>
                                    </li>
                                </ol>            
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-18" data-target="#panel-18" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-18">
                            <span class="h6 mb-0 font-weight-bold">22/07/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-18">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho scritto la prima versione del capitolo sulla literature review. È coerente dall'inizio alla fine, ma incompleto. Può essere scaricato cliccando sul seguente bottone:
                                        <a class="btn btn-primary d-block my-3 col-md-4 col-sm-12" type="submit" href="assets/documents/lr_1_0_0.pdf" download="literature_review_1-0-0-alpha.pdf">
                                            <span class="mr-1"><span class="fas fa-download"></span></span>
                                            Download literature review 1.0.0-alpha
                                        </a>
                                    </li>
                                    <li>Documenti studiati e inclusi nella literature review:
                                        <ol>
                                            <li>Barabucci, G. (2013). Introduction to the Universal Delta Model. Proceedings of the 2013 ACM Symposium on Document Engineering (p. 47–56). Florence, Italy: Association for Computing Machinery. doi:10.1145/2494266.2494284</li>
                                            <li>Barabucci, G., Ciancarini, P., Iorio, A. D., & Vitali, F. (2016). Measuring the quality of diff algorithms: a formalization. Computer Standards & Interfaces, 46, 52-65. doi:10.1016/j.csi.2015.12.005</li>
                                            <li>Berners-Lee, T. (1999). Weaving the Web: the original design and ultimate destiny of the World Wide Web. San Francisco: Harper San Francisco</li>
                                            <li>Berners-Lee, T., & Connolly, D. (2004). Delta: an ontology for the distribution of differences between RDF graphs. Retrieved from https://www.w3.org/DesignIssues/lncs04/Diff.pdf</li>
                                            <li>Caplan, P. (2017). Understanding PREMIS: an overview of the PREMIS Data Dictionary for Preservation Metadata. Library of Congress. Retrieved from Library of Congress: https://www.loc.gov/standards/premis/understanding-premis-rev2017.pdf</li>
                                            <li>Carroll, J. J., Bizer, C., Hayes, P., & Stickler, P. (2005). Named graphs, provenance and trust. Proceedings of the 14th international conference on World Wide Web (p. 613–622). New York: Association for Computing Machinery. doi:10.1145/1060745.1060835</li>
                                            <li>Ciccarese, P., Wu, E., Kinoshita, J., Wong, G. T., Ocana, M., Ruttenberg, A., & Clark, T. (2008). The SWAN Scientific Discourse Ontology. Journal of biomedical informatics, 41(5), 739–751. doi:10.1016/j.jbi.2008.04.010</li>
                                            <li>Daquino, M., Peroni, S., Shotton, D., Colavizza, G., Ghavimi, B., Lauscher, A., . . . Zumstein, P. (2020). The OpenCitations Data Model. In J. Z. Pan, V. Tamma, C. d’Amato, K. Janowicz, B. Fu, A. Polleres, . . . L. Kagal (A cura di), International Semantic Web Conference. 12507, p. 447-463. Springer, Cham. doi:10.1007/978-3-030-62466-8_28</li>
                                            <li>DCMI Metadata Terms. (2020, 01 20). Retrieved 07 16, 2021, from Dublin Core Metadata Initiative: http://dublincore.org/specifications/dublin-core/dcmi-terms/2020-01-20/</li>
                                            <li>Ding, L., Finin, T., Peng, Y., Silva, P. P., & L., D. (2005). Tracking RDF Graph Provenance. Technical report. Retrieved from http://ebiquity.umbc.edu/get/a/publication/178.pdf</li>
                                            <li>Gil, Y., & Miles, S. (Eds.). (2013, 04 30). PROV Model Primer. Retrieved 07 16, 2021, from W3C: http://www.w3.org/TR/2013/NOTE-prov-primer-20130430/</li>
                                            <li>Lebo, T., Sahoo, S., & McGuinness, D. (Eds.). (2013, 04 30). PROV-O: The PROV Ontology. Retrieved 07 16, 2021, from W3C: http://www.w3.org/TR/2013/REC-prov-o-20130430/</li>
                                            <li>Moreau, L., & Missier, P. (Eds.). (2013, 04 30). PROV-DM: The PROV Data Model. Retrieved 07 16, 2021, from W3C: http://www.w3.org/TR/2013/REC-prov-dm-20130430/</li>
                                            <li>Moreau, L., Clifford, B., Freire, J., Futrelle, J., Gil, Y., Groth, P., . . . Bussche, J. V. (2011). The Open Provenance Model core specification (v1.1). Future Generation Computer Systems,, 27(6), 743-756. doi:10.1016/j.future.2010.07.005</li>
                                            <li>Ognyanov, D., & Kiryakov, A. (2002). Tracking Changes in RDF(S) Repositories. In G.-P. A., & B. V.R. (Ed.), Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web (pp. 373-378). Berlin, Heidelberg: Springer. doi:10.1007/3-540-45810-7_33</li>
                                            <li>Peroni, S., Shotton, D., & Vitali, F. (2016). A Document-inspired Way for Tracking Changes of RDF Data. In L. Hollink, S. Darányi, A. M. Peñuela, & E. Kontopoulos (Ed.), Detection, Representation and Management of Concept Drift in Linked Open Data. 1799, pp. 26-33. Bologna: CEUR Workshop Proceedings. Retrieved from http://ceur-ws.org/Vol-1799/Drift-a-LOD2016_paper_4.pdf</li>
                                            <li>Provenance Incubator Group Charter. (2010). Retrieved July 15, 2021, from https://www.w3.org/2005/Incubator/prov/charter</li>
                                            <li>(08 December 2010). Provenance XG Final Report. W3C. Retrieved from http://www.w3.org/2005/Incubator/prov/XGR-prov-20101214/</li>
                                            <li>Sahoo, S. S., & Sheth, A. P. (2009). Provenir Ontology: Towards a Framework for eScience Provenance Management. Retrieved from https://corescholar.libraries.wright.edu/knoesis/80</li>
                                        </ol>
                                    </li>
                                    <li>Documenti studiati ma esclusi dalla literature review perché poco pertinenti o per ragioni di sintesi:
                                        <ol>
                                            <li>Aggelen, A.V., Hollink, L., & Ossenbruggen, J.V. (2016). Combining Distributional Semantics and Structured Data to Study Lexical Change. Drift-a-LOD@EKAW</li>
                                            <li>Berliner, B. (1990). CVS II: Parallelizing Software Development. Prisma Inc. Retrieved from https://docs.freebsd.org/44doc/psd/28.cvs/paper.pdf</li>
                                            <li>Fokkens, A. S., ter Braake, S., Maks, E., & Ceolin, D. (2016). On the Semantics of Concept Drift: Towards Formal Definitions of Semantic Change. In S. Darányi, L. Hollink, A. Meroño Peñuela, & E. Kontopoulos (Eds.), Proceedings of Drift-a-LOD</li>
                                            <li>Ding, L., Finin, T., Joshi, A., Pan, R., Scott Cost, R., Peng, Y.,  Reddivari, P., Doshi, V., and Sachs, J. (2004). Swoogle: a search and metadata engine for the semantic web. In Proceedings of the thirteenth ACM international conference on Information and knowledge management (CIKM '04). Association for Computing Machinery, New York, NY, USA, 652–659. DOI:https://doi.org/10.1145/1031171.1031289</li>
                                            <li>Meroño-Peñuela, A., Wittek, P., & Darányi, S. (2016). Visualizing the Drift of Linked Open Data Using Self-Organizing Maps. Drift-a-LOD@EKAW</li>
                                            <li>Recchia, G., Jones, E., Nulty, P., Regan, J., & Bolla, P.D. (2016). Tracing Shifting Conceptual Vocabularies Through Time. Drift-a-LOD@EKAW</li>
                                            <li>Stavropoulos, Thanos G., Andreadis, Stelios, Kontopoulos, Efstratios, Riga, Marina, Mitzias, Panagiotis, & Kompatsiaris, Ioannis. (2017). SemaDrift: A Protégé Plugin for Measuring Semantic Drift in Ontologies. Presented at the Detection, Representation and Management of Concept Drift in Linked Open Data (Drift-a-LOD), Bologna, Italy: Zenodo. http://doi.org/10.5281/zenodo.345374</li>
                                            <li>Tichy, W. F. (1984). RCS: A System for Version Control. Department of Computer Science Technical Reports. Paper 394. https://docs.lib.purdue.edu/cstech/394</li>
                                        </ol>
                                    </li>
                                    <li>Il metodo <strong>get_history</strong> restituisce sempre una tupla di due elementi. Il secondo elemento è un dizionario con metadati sulla provenance se il parametro include_prov_metadata è True, altrimenti è None. Allo stesso modo, il metodo <strong>get_state_at_time</strong> restituisce sempre una tupla di tre elementi, dove il terzo elemento è un dizionario con metadati sulla provenance se il parametro include_prov_metadata è True, altrimenti è None.</li>
                                    <li>Il metodo get_history riporta anche la dc:description dello snapshot in caso di parametro include_prov_metadata passato a True. Di conseguenza, anche l'interfaccia riporta questa informazione, particolarmente informativa in caso di merge tra entità o cancellazione. All'interno della descrizione, le entità vengono rese più leggibili nel caso in cui l'utente abbia fornito uno o più URI base nel file di configurazione. Inoltre, le entità nella descrizione vengono mostrate come link, cliccando sui quali viene ricostruita la storia dell'entità corrispondente.
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/se_desc.png" alt="line-through">
                                        </div>
                                    </li>
                                </ol>
                            <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Domande relative all'articolo <em>A document-inspired way for tracking changes of RDF data</em> (<a href="https://rawgit.com/essepuntato/opencitations/master/paper/occ-driftalod2016.html" target="_blank">Peroni, et al., 2016</a>):
                                        <ol>
                                            <li>Nell'articolo si legge:
                                                <blockquote class="blockquote ml-5 mt-3">[...] while preserving provenance information of such
                                                    addition/deletion actions in an appropriate contextual space, i.e. the provenance
                                                    graph associated to such entity (as also suggested in [16]).</blockquote>
                                                <p>[16] sarebbe l'articolo <em>Tracking RDF Graph Provenance using RDF Molecules</em> (<a href="http://ebiquity.umbc.edu/get/a/publication/178.pdf" target="_blank">Ding, et al., 2005</a>). Ho studiato l'articolo citato e non sono riuscito a capire in che modo questo suggerisca di utilizzare il grafo associato a un'entità come contesto per registrare la provenance. L'articolo sembra piuttosto suggerire come livello appropriato una "molecola RDF", ovvero una tripla in assenza di blank nodes, il risultato di un algoritmo di decomposizione secondo un'ontologia data in caso di blank nodes.</p>
                                            </li>
                                            <li>Nel secondo capitolo dell'articolo vengono discussi due approcci per tenere traccia dei cambiamenti in dati RDF, ovvero l'approccio <em>statement-centric</em> e <em>resource-centric</em>: non ho capito se questa tassonomia sia un contributo originale dell'articolo o esista altrove. Nel caso, mi piacerebbe approfondire perché non mi è del tutto chiara. 
                                                <ol>
                                                    <li>Tra gli approcci di tipo <em>statement-centric</em> viene menzionato il <em>massive statement reification</em>. Si riferisce a RDF reification? Ovvero all'unica sintassi standard per catturare provenance RDF (<a href="https://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification" target="_blank">https://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification</a>).</li>
                                                </ol></li>
                                        </ol>
                                    </li>
                                    <li>Potresti suggerirmi della bibliografia in materia di change tracking sui documenti testuali? In particolare, mi piacerebbe ricostruire in che modo l'approccio sulla provenance dell'OCDM si ispira a Microsoft Word e OpenOffice Writer.</li>
                                    <li>È legittimo inserire in bibliografia testi che non corrispondono ad alcuna citazione intratestuale? Ovvero documenti che sono risultati utili alla comprensione del problema, ma che per ragioni di sintesi non sono stati menzionati nel discorso.</li>
                                    <li>Oltre a EXCITE, al Linked Open Citation Database e al Venice Scholar Index, esistono altre repository o progetti che hanno adottato l'OCDM?</li>
                                    <li>Esistono casi in cui un documento RDF costruito seguendo l'OCDM contenga blank nodes?</li>
                                </ol>            
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-19" data-target="#panel-19" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-19">
                            <span class="h6 mb-0 font-weight-bold">28/07/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-19">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho scritto una sezione della literature review in cui introduco tutti i <em>metadata representation models</em> per la provenance esistenti, o almeno questo è l'obiettivo (difficile dire se siano realmente tutti, ogni volta che leggo un nuovo articolo ne scopro di nuovi). Riporto due tabelle che riassumono l'intero discorso a riguardo.
                                        <table class="table shadow-soft rounded table-striped my-4">
                                            <thead>
                                                <tr>
                                                    <th>TYPE OF APPROACH</th>
                                                    <th>METADATA REPRESENTATION MODELS</th>
                                                </tr>
                                            </thead>
                                            <tbody>
                                                <tr>
                                                    <td>Quadruples</td>
                                                    <td>Named graphs, RDF/S graphsets, RDF triple coloring</td>
                                                </tr>
                                                <tr>
                                                    <td>Extension of the RDF data model</td>
                                                    <td>Notation 3 Logic, RDF+, annotated RDF (aRDF) and Annotated RDF Schema, SPOTL(X), RDF*</td>
                                                </tr>
                                                <tr>
                                                    <td>Encapsulating Provenance with RDF Triples</td>
                                                    <td>PaCE, singleton property</td>
                                                </tr>
                                                <tr>
                                                    <td>Data models alternative to RDF</td>
                                                    <td>GSMM, mapping entities to vectors</td>
                                                </tr>
                                                <tr>
                                                    <td>Knowledge organisation system</td>
                                                    <td>OPM, PML, Provenir, PREMIS, SWAN, DC, PROV, OCDM</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                        <table class="table shadow-soft rounded table-striped my-4">
                                            <thead>
                                                <tr>
                                                    <th>APPROACH</th>
                                                    <th>TUPLE TYPE</th>
                                                    <th>COMPLIANCE WITH THE RDF DATA MODEL</th>
                                                    <th>COMPLIANCE WITH SPARQL</th>
                                                    <th>RDF SERIALISATIONS</th>
                                                    <th>EXTERNAL VOCABULARY</th>
                                                    <th>SCALABLE</th>
                                                </tr>
                                            </thead>
                                            <tbody>
                                                <tr>
                                                    <td>Named graphs</td>
                                                    <td>Quadruple</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>TriG, TriX, N-Quads</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>RDF/S graphsets</td>
                                                    <td>Quadruple</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>TriG, TriX, N-Quads</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>RDF triple coloring</td>
                                                    <td>Quadruple</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>TriG, TriX, N-Quads</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>N3Logic</td>
                                                    <td>Triple (in N3)</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                    <td>N3</td>
                                                    <td>N3 Logic Vocabulary</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>aRDF</td>
                                                    <td>Nonstandard</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>Annotated RDF Schema</td>
                                                    <td>Nonstandard</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>RDF+</td>
                                                    <td>Quintuple</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>SPOTL(X)</td>
                                                    <td>Quintuple/sextuple</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>RDF*</td>
                                                    <td>Nonstandard</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>PaCE</td>
                                                    <td>Triple</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>RDF/XML, N3, Turtle, N-Triples, RDF-JSON, JSON-LD, RDFa, HTML5 Microdata</td>
                                                    <td>Provenir ontology</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>Singleton property</td>
                                                    <td>Triple</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>RDF/XML, N3, Turtle, N-Triples, RDF-JSON, JSON-LD, RDFa, HTML5 Microdata</td>
                                                    <td>singletonPropertyOf property</td>
                                                    <td>-</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                    </li>
                                    <li>
                                        <p>Nuovi documenti studiati e citati:
                                            <ol>
                                                <li>Beckett, D. (2010, April 10). RDF Syntaxes 2.0. Retrieved 07 22, 2021, from W3C: https://www.w3.org/2009/12/rdf-ws/papers/ws11</li>
                                                <li>Damiani, E., Oliboni, B., Quintarelli, E., & Tanca , L. (2019). A graph-based meta-model for heterogeneous data management. Knowledge and Information Systems, 61(1), 107–136. doi:10.1007/s10115-018-1305-8</li>
                                                <li>Dividino, R., Sizov, S., Staab, S., & Schueler, B. (2009). Querying for provenance, trust, uncertainty and other meta knowledge in RDF. Journal of Web Semantics, 7(3), 204-219. doi:10.1016/j.websem.2009.07.004</li>
                                                <li>Flouris, G., Fundulaki, I., Pediaditis, P., Theoharis, Y., & Christophides, V. (2009). Coloring RDF Triples to Capture Provenance. The Semantic Web - ISWC 2009. Springer, Berlin, Heidelberg. doi:10.1007/978-3-642-04930-9_13</li>
                                                <li>Groth, P., Gibson, A., & Velterop, J. (2010, 09 21). The anatomy of a nanopublication. Information Services & Use, 30(1-2), 51-56. doi:10.3233/ISU-2010-0613</li>
                                                <li>Hartig, O., & Thompson, B. (2019, March 20). Foundations of an Alternative Approach to Reification in RDF. Retrieved from https://arxiv.org/abs/1406.3399</li>
                                                <li>Hoffart, J., Suchanek, F. M., Berberich, K., & Weikum, G. (2013). YAGO2: A spatially and temporally enhanced knowledge base. Artificial Intelligence, 194, 28-61. doi:10.1016/j.artint.2012.06.001</li>
                                                <li>Keskisärkkä, R., Blomqvist, E., Lind, L., & Hartig, O. (2019, November 04). RSP-QL* : Enabling Statement-Level Annotations in RDF Streams. The Power of AI and Knowledge Graphs. SEMANTiCS 2019. Lecture Notes in Computer Science. 11702. Karlsruhe, Germany: Springer, Cham. doi:10.1007/978-3-030-33220-4_11</li>
                                                <li>Manola, F., & Miller, E. (Eds.). (2004, February 10). RDF Primer. Retrieved 07 22, 2021, from W3C: http://www.w3.org/TR/2004/REC-rdf-primer-20040210/</li>
                                                <li>Nguyen, V., Bodenreider, O., & Sheth, A. (2014). Don't like RDF reification?: making statements about statements using singleton property. WWW '14: Proceedings of the 23rd international conference on World wide web (pp. 759–770). New York, NY, USA: Association for Computing Machinery. doi:10.1145/2566486.2567973</li>
                                                <li>Noy, N., & Rector, A. (Eds.). (2006, 04 12). Defining N-ary Relations on the Semantic Web. Retrieved 07 22, 2021, from W3C: http://www.w3.org/TR/2006/NOTE-swbp-n-aryRelations-20060412/</li>
                                                <li>Pediaditis, P., Flouris, G., Fundulaki, I., & Christophides, V. (2009). On Explicit Provenance Management in RDF/S Graphs. First Workshop on the Theory and Practice of Provenance. San Francisco, CA, USA: USENIX. Retrieved from https://www.usenix.org/legacy/event/tapp09/tech/full_papers/pediaditis/pediaditis.pdf</li>
                                                <li>Pinheiro da Silva, P., McGuinness, D. L., & Fikes, R. (2006). A proof markup language for Semantic Web services. Information Systems, 31(4–5), 381-395. doi:10.1016/j.is.2005.02.003</li>
                                                <li>Sahoo, S. S., Bodenreider, O., Hitzler, P., Sheth, A., & Thirunarayan, K. (2010). Provenance Context Entity (PaCE): Scalable Provenance Tracking for Scientific RDF Data. In G. M., & L. B., Scientific and Statistical Database Management (Vol. 6187, pp. 461-470). Berlin, Heidelberg: Springer. doi:10.1007/978-3-642-13818-8_32</li>
                                                <li>Sikos, L., & Philp, D. (2020). Provenance-Aware Knowledge Representation: A Survey of Data Models and Contextualized Knowledge Graphs. Data Science and Engineering, 5(3), 293-316. doi:10.1007/s41019-020-00118-0</li>
                                                <li>Suchanek, F. M., Lajus, J., Boschin, A., & Weikum, G. (2019). Knowledge Representation and Rule. In M. Krötzsch, & D. Stepanova (Ed.), Reasoning Web. Explainable Artificial Intelligence: 15th International Summer School 2019, Bolzano, Italy, September 20-24, 2019, Tutorial Lectures (pp. 110-152). Springer International Publishing. doi:10.1007/978-3-030-31423-1_4</li>
                                                <li>Udrea, O., Recupero, D. R., & Subrahmanian, V. S. (2010, January). Annotated RDF. ACM Transactions on Computational Logic, 11(2), 1–41. doi:10.1145/1656242.1656245</li>
                                                <li>Zimmermann, A., Lopes, N., Polleres, A., & Straccia, U. (2012). A general framework for representing, reasoning and querying with annotated Semantic Web data. Journal of Web Semantics, 11, 72-95. doi:10.1016/j.websem.2011.08.006</li>
                                            </ol>
                                        </p>
                                    </li>
                                    <li>Lo stato corrente del capitolo sulla <em>literature review</em> può essere scaricato cliccando sul seguente bottone:
                                        <a class="btn btn-primary d-block my-3 col-md-4 col-sm-12" type="submit" href="assets/documents/lr_1_0_0_alpha_2.pdf" download="literature_review_1-0-0-alpha.pdf">
                                            <span class="mr-1"><span class="fas fa-download"></span></span>
                                            Download literature review 1.0.0-alpha
                                        </a>
                                    </li>
                                </ol>
                            <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>C'è un'aurea di mistero intorno all'articolo <em>Context in graphs</em>, di tale Hogan A (2018). Viene citato da <a href="https://doi.org/10.1007/s41019-020-00118-0" target="_blank">https://doi.org/10.1007/s41019-020-00118-0</a> e <a href="https://doi.org/10.1007/978-3-030-67681-0" target="_blank">https://doi.org/10.1007/978-3-030-67681-0</a> a proposito della <em>Hoganification</em>, una tecnica per catturare la provenance di un grafo. Ecco la citazione completa, identica in entrambe le fonti:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            Hogan A (2018) Context in graphs. In: Proceedings of the 1st International Workshop on Conceptualized Knowledge Graphs. RWTH Aachen University, Aachen
                                        </blockquote>
                                        <p>Tuttavia, tale documento sembra non esistere. Non è nemmeno menzionato sul sito personale dell'autore (<a href="https://aidanhogan.com/" target="_blank">https://aidanhogan.com/</a>) o su quello istituzionale (<a href="https://dblp.org/pid/h/AidanHogan.html" target="_blank">https://dblp.org/pid/h/AidanHogan.html</a>). Conosci la <em>Hoganification</em>? Conosci fonti a riguardo? Mi piacerebbe parlarne per completezza, ma non trovo informazioni significative.</p>
                                    </li>
                                    <li>Ha senso inserire una sezione della <em>literature review</em> in cui si parla di algoritmi di delta tra dati RDF?</li>
                                </ol>
                            <h3>Punto della situazione</h3>
                                <p>In questa sezione ricapitolo cosa rimane da fare.</p>
                                <p>Per quanto riguarda il codice:</p>
                                <ol>
                                    <li>Supporto per le inverse path nelle query SPARQL.</li>
                                    <li>Nuovo parametro di configurazione che permetta di caricare i grafi del passato ricostruiti su un triplestore, al fine di rendere più veloci le successive query agnostiche.</li>
                                </ol>
                                <p>Per quanto riguarda la <em>literature review</em>:</p>
                                <ol>
                                    <li>Sezione sul change tracking. La stesura è già avviata, proseguirà dopo la pubblicazione del diario.</li>
                                </ol>            
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-20" data-target="#panel-20" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-20">
                            <span class="h6 mb-0 font-weight-bold">04/08/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-20">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho scritto l'ultima sezione della literature review, in cui indago lo stato dell'arte sul change tracking in RDF, introduco le possibili politiche di archiviazione, le tipologie di query, riporto casi d'uso e li catalogo. Riporto tre tabelle che riassumono l'intero discorso a riguardo.
                                        <table class="table shadow-soft rounded table-striped my-4">
                                            <caption>Table 1 Datasets and software divided by storage policy.</caption>
                                            <thead>
                                                <tr>
                                                    <th>Archiving policy</th>
                                                    <th>Datasets / Software</th>
                                                </tr>
                                            </thead>
                                            <tbody>
                                                <tr>
                                                    <td>Independent copies (IC)</td>
                                                    <td>DBPedia, Wikidata, YAGO, Dynamic Linked Data Observatory, SemVersion, PromptDiff</td>
                                                </tr>
                                                <tr>
                                                    <td>Change-based (CB)</td>
                                                    <td>(Im, Lee, & Kim, 2012),  (Papavasileiou, Flouris, Fundulaki, Kotzinos, & Christophides, 2013), R&Wbase</td>
                                                </tr>
                                                <tr>
                                                    <td>Timestamp-based (TB)</td>
                                                    <td>x-RDF-3X, v-RDFCSA</td>
                                                </tr>
                                                <tr>
                                                    <td>Hybrid</td>
                                                    <td>OSTRICH (CB/TB), OpenCitations Corpus (CB/TB), (Tanon & Suchanek, 2019) (IC/CB/TB)</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                        <table class="table shadow-soft rounded table-striped my-4">
                                            <caption>Table 2 Retrieval functionalities according to (Fernández, Umbrich, Polleres, & Knuth, 2016).</caption>
                                            <tr>
                                                <th rowspan="2">&nbsp;</th>
                                                <th rowspan="2">Materialization</th>
                                                <th colspan="2" class="text-center">Structured queries</th>
                                            </tr>
                                            <tr class="text-center">
                                                <th>Single time</th>
                                                <th>Cross time</th>
                                            </tr>
                                            <tr>
                                                <th>Version</th>
                                                <td>
                                                    <p>Version materialization</p>
                                                    <p><em>Get snapshot at time t<sub>i</sub></em></p>
                                                </td>
                                                <td>
                                                    <p>Single-version structured queries</p>
                                                    <p><em>Articles written by a specific author at time t<sub>i</sub></em></p>
                                                </td>
                                                <td>
                                                    <p>Cross-version structured queries</p>
                                                    <p><em>Articles associated with the same DOI simultaneously</em></p>
                                                </td>
                                            </tr>
                                            <tr>
                                                <th>Delta</th>
                                                <td>
                                                    <p>Delta materialization</p>
                                                    <p><em>Get delta at time t<sub>i</sub></em></p>
                                                </td>
                                                <td>
                                                    <p>Single-delta structured queries</p>
                                                    <p><em>DOI modified between two consecutive snapshots</em></p>
                                                </td>
                                                <td>
                                                    <p>Cross-delta structured queries</p>
                                                    <p><em>The most significant change in the number of articles in the history of the dataset</em></p>
                                                </td>
                                            </tr>
                                        </table>
                                        <table class="table shadow-soft rounded table-striped my-4">
                                            <caption>Table 3 Software catalogued by allowed query types and the need for indexing.</caption>
                                            <thead>
                                                <tr>
                                                    <th>Software</th>
                                                    <th>Version materialization</th>
                                                    <th>Delta materialization</th>
                                                    <th>Single version structured query</th>
                                                    <th>Cross version structured query</th>
                                                    <th>Single delta structured query</th>
                                                    <th>Cross delta structured query</th>
                                                    <th>On the fly</th>
                                                </tr>
                                            </thead>
                                            <tbody>
                                                <tr>
                                                    <td>PromptDiff</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>SemVersion</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>(Im, Lee, & Kim, 2012)</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>R&Wbase</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                </tr>
                                                <tr>
                                                    <td>x-RDF-3X</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>v-RDFCSA</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>OSTRICH</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                    <td>-</td>
                                                </tr>
                                                <tr>
                                                    <td>(Tanon & Suchanek, 2019)</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>+</td>
                                                    <td>-</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                    </li>
                                    <li>
                                        <p>Nuovi documenti studiati e citati:
                                            <ol>
                                                <li>Barabucci, G., Tomasi, F., & Vitali, F. (2021). Supporting Complexity and Conjectures in Cultural Heritage Descriptions. CEUR Workshop Proceedings, 2810, 104-115. Retrieved from http://ceur-ws.org/Vol-2810/paper9.pdf</li>
                                                <li>Cerdeira-Pena, A., Farina, A., Fernández, J. D., & Martınez-Prieto, M. A. (2016). Self-indexing rdf archives. Proceedings of IEEE Data Compression Conference.</li>
                                                <li>Dooley, P., & Božić, B. (2019). Towards Linked Data for Wikidata Revisions and Twitter. iiWAS2019: Proceedings of the 21st International Conference on Information Integration and Web-based Applications & Services (pp. 166–175). New York, NY, USA: Association for Computing Machinery. doi:10.1145/3366030.3366048</li>
                                                <li>Erxleben, F., Günther, M., Krötzsch, M., Mendez, J., & Vrandečić, D. (2014). Introducing Wikidata to the Linked Data Web. The Semantic Web – ISWC 2014 (pp. 50–65). Springer International Publishing.</li>
                                                <li>Fernández, J. D., Polleres, A., & Umbrich, J. (2015). Towards Efficient Archiving of Dynamic Linked. DIACRON@ESWC (pp. 34–49). Portorož, Slovenia : Computer Science.</li>
                                                <li>Fernández, J. D., Umbrich, J., Polleres, A., & Knuth, M. (2016). Evaluating Query and Storage Strategies for RDF Archives. Proceedings of the 12th International Conference on Semantic Systems.</li>
                                                <li>Käfer, T., Abdelrahman, A., Umbrich, J., Byrne, P. O., & Hogan, A. (2013). Observing Linked Data Dynamics. The Semantic Web: Semantics and Big Data (pp. 213-227). Berlin, Heidelberg: Springer. Retrieved from https://link.springer.com/content/pdf/10.1007%2F978-3-642-38288-8_15.pdf</li>
                                                <li>Im, D.-H., Lee, S.-W., & Kim, H.-J. (2012). A Version Management Framework for RDF Triple Stores. International Journal of Software Engineering and Knowledge Engineering, 22, 85-106. doi:10.1142/S0218194012500040</li>
                                                <li>Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P. N., . . . Bizer, C. (2015). DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web, 6(2), 167-195. doi:10.3233/SW-140134</li>
                                                <li>Neumann, T., & Weikum, G. (2010). x-RDF-3X: Fast Querying, High Update Rates, and Consistency for RDF Databases. Proceedings of the VLDB Endowment, 3, pp. 256–263.</li>
                                                <li>Noy, N. F., & Musen, M. A. (2002). Promptdiff: A Fixed-Point Algorithm for Comparing Ontology Versions. Proc. of IAAI, (pp. 744–750).</li>
                                                <li>Sande, M. V., Colpaert, P., Verborgh, R., Coppens, S., Mannens, E., & Walle, R. V. (2013). R&Wbase: Git for triples. Proceedings of the 6th Workshop on Linked Data on the Web. 996. CEUR Workshop Proceedings.</li>
                                                <li>Snodgrass, R. (1986). Temporal Databases. IEEE Computer, 19, 35–42. Retrieved from https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.464.8688&rep=rep1&type=pdf</li>
                                                <li>Orlandi, F., & Passant, A. (2011). Modelling provenance of DBpedia resources using Wikipedia contributions. Journal of Web Semantics, 9(2), 149-164. doi:10.1016/j.websem.2011.03.002</li>
                                                <li>Papavasileiou, V., Flouris, G., Fundulaki, I., Kotzinos, D., & Christophides, V. (2013). High-level Change Detection in RDF(S) KBs. ACM Transactions on Database Systems, 38(1). doi:10.1145/2445583.2445584</li>
                                                <li>Taelman, R., Sande, M. V., & Verborgh, R. (2018). OSTRICH: Versioned Random-Access Triple Store. Companion Proceedings of the Web Conference 2018, (pp. 127-130). Retrieved from https://core.ac.uk/download/pdf/157574975.pdf</li>
                                                <li>Tanon, T. P., & Suchanek, F. M. (2019). Querying the Edit History of Wikidata. Extended Semantic Web Conference, (pp. 161-166). Portorož, Slovenia. doi:10.1007/978-3-030-32327-1_32</li>
                                                <li>Tanon, T. P., Weikum, G., & Suchanek, F. (2020). YAGO 4: A Reason-able Knowledge Base. The Semantic Web. ESWC 2020 (pp. 583-596). Cham: Springer.</li>
                                                <li>Umbrich, J., Hausenblas, M., Hogan, A., Polleres, A., & Decker, S. (2010). Towards Dataset Dynamics: Change Frequency of Linked Open Data Sources. In C. Bizer, T. Heath, T. Berners-Lee, & M. Hausenblas (Ed.), Proceedings of the WWW2010 Workshop on Linked Data on the Web. Raleigh, USA: CEUR Workshop Proceedings. Retrieved from http://ceur-ws.org/Vol-628/ldow2010_paper12.pdf</li>
                                                <li>Völkel, M., Winkler, W., Sure, Y., Kruk, S., & Synak, M. (2005). SemVersion: A Versioning System for RDF and Ontologies. Proc. of ESWC.</li>
                                            </ol>        
                                        </p>
                                    </li>
                                    <li>
                                        <p>Modifiche apportate in seguito alle tue considerazioni: 
                                            <ol>
                                                <li>
                                                    <p><s>There is a shorthand notation, the rdf:ID attribute in RDF/XML (Figure 2), but RDF Reification has no syntactic support for other serialisations.</s></p>
                                                    <p class="text-center"><i class="fas fa-long-arrow-alt-down"></i></p>
                                                    <p>There is a shorthand notation, the rdf:ID attribute in RDF/XML (Figure 2), but it is not present in other serializations.</p>
                                                </li>
                                                <li>La didascalia di Figura 1 non copre più parte dell'immagine.</li>
                                                <li>Quando parlo di "valid time" apro una finestra sulla complessità del problema, citando il proceeding <em>Supporting Complexity and Conjectures in Cultural Heritage Descriptions</em> di Barabucci, Tomasi e Vitali.</li>
                                                <li>Conclusa la rassegna di software e datasets rilevanti per il change tracking, sottolineo come i software per effettuare time-travel queries esistenti necessitino tutti di pre-indicizzazioni, a differenza del mio che, grazie al modello di provenance dell'OpenCitations Data Model, può elaborare la richiesta al volo.</li>
                                                <li>È stato adottato Chigago XV edizione come stile di citazione, al posto di APA VI edizione.</li>
                                                <li>L'indice prevede ora i seguenti capitoli:
                                                    <ul>
                                                        <li>Table of Contents</li>
                                                        <li>Abstract</li>
                                                        <li>Introduction</li>
                                                        <li>Literature review</li>
                                                        <li>Materials and Methods</li>
                                                        <li>The Time Agnostic Library</li>
                                                        <li>Library implementation</li>
                                                        <li>Discussion</li>
                                                        <li>Conclusion</li>
                                                        <li>References</li>
                                                    </ul>
                                                </li>
                                                <li>Modifiche di layout, sulla base di <a href="https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge/information-on-writing-a-dissertation" target="_blank">https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge/information-on-writing-a-dissertation</a>:
                                                    <ol>
                                                        <li>Font: Times New Roman, 12pt.</li>
                                                        <li>Interlinea: 1.5.</li>
                                                        <li>Margini: 2-2-2-2.</li>
                                                        <li>Ho aggiunto il frontespizio.</li>
                                                    </ol>
                                                </li>
                                            </ol>
                                        </p>
                                    </li>
                                    <li>Considero il capitolo potenzialmente concluso. Pertanto, avanzo la versione da alpha a beta:
                                        <a class="btn btn-primary d-block my-3 col-md-4 col-sm-12" type="submit" href="assets/documents/lr_1_0_0_beta_2.pdf" download="literature_review_1-0-0-beta.pdf">
                                            <span class="mr-1"><span class="fas fa-download"></span></span>
                                            Download literature review 1.0.0-beta
                                        </a>
                                    </li>
                                </ol>
                            <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Ho scoperto che Wikidata utilizza la reificazione solo a livello metadatale, ma per il change tracking utilizza dei veri e propri phisical snapshots. Ogni volta che c'è una revisione, l'intero stato della risorsa dopo la revisione viene salvato. Nella literature review ho incluso una descrizione più dettagliata di cosa avviene. Sei d'accordo nel catalogare la politica di archiviazione di Wikipedia come independent-copies/physical snapshots?</li>
                                    <li>Consideriamo un autore di nome Dong-Hyuk Im, dove Dong-Hyuk è il nome. Come va abbreviato per citarlo in APA style? Im, D.-H.? Im, D.? Im, D.H.?</li>
                                    <li>Cristian vorrebbe citare il mio dataset. Come si potrebbe fare? Momentaneamente l'ho caricato su Zenodo con questo DOI: <a href="https://doi.org/10.5281/zenodo.5151264" target="_blank">https://doi.org/10.5281/zenodo.5151264</a>.</li>
                                    <li>Qual è il miglior modo per citare l'API di Crossref?</li>
                                    <li>Alla luce della classificazione riassunta in Tabella 2, il mio software al momento è in grafo di effettuare una version materialization (get_state_at_time) e una cross-version structured query (run_agnostic_query). Non è in grado di effettuare una single-version structured query, ovvero una query soddisfatta in un tempo specifico, né un qualsiasi tipo di query sui delta. Infine, la delta materialization è possibile by design, perché oco:hasUpdateQuery contiene già il delta tra due risorse. Sei d'accordo con queste affermazioni?</li>
                                    <li>Quali sono le parole italiane solitamente utilizzate per tradurre <em>proceeding</em> e <em>literature review</em>?</li>
                                    <li>Il font prescritto per la tesi è "Times o simili". È possibile utilizzare un'alternativa senza grazie?</li>
                                    <li>Tra le regole di layout per la tesi non ci sono indicazioni circa la spaziatura tra paragrafi. Solitamente, lascio 10 pt dopo ogni paragrafo, perché mi sembra una soluzione più leggibile dell'indentazione, ma so che le dissertazioni scientifiche usano l'indentazione. Cosa mi suggerisci di usare?</li>
                                    <li>Relativamente al frontespizio, qual è la materia di riferimento per la tesi? Computational thinking? Open Science?</li>
                                </ol>            
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-21" data-target="#panel-21" class="accordion-panel-header" data-toggle="collapse" role="button"
                            aria-expanded="false" aria-controls="panel-21">
                            <span class="h6 mb-0 font-weight-bold">11/08/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-21">
                            <div class="pt-3">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>    
    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0.0/jquery.counterup.min.js" integrity="sha512-d8F1J2kyiRowBB/8/pAWsqUl0wSEOkG5KATkVV4slfblq9VRQ6MyDZVxWl2tWd+mPhuCbpTB4M7uU/x9FlgQ9Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.countdown/2.2.0/jquery.countdown.min.js" integrity="sha512-lteuRD+aUENrZPTXWFRPTBcDDxIGWe5uu0apPEn+3ZKYDwDaEErIK9rvR0QzUGmUQ55KFE2RqGTVoZsKctGMVw==" crossorigin="anonymous"></script>    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/smooth-scroll/16.1.3/smooth-scroll.polyfills.min.js" integrity="sha512-LZ6YBzwuQvIG41twjliX3HUVeAd+ErnJ0UsqRnkI4firX2l71jxbKJoax/hu7XY2tiyLl0YA2kcnz/XEW+9O3g==" crossorigin="anonymous"></script>  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jarallax/1.12.5/jarallax.min.js" integrity="sha512-DI98Iva+99hqdsd+etSf/W9iJcmz5jornxiWr5nkr/kcKWlaCDwIsWW6AGxXu/X5u/yylsLYJowdPzIcLUDklw==" crossorigin="anonymous"></script>  
    <script src="./assets/js/neumorphism.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>