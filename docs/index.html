<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Time agnostic</title>
    <link type="text/css" href="./css/neumorphism.css" rel="stylesheet">
    <link type="text/css" href="./css/custom.css" rel="stylesheet">
    <!-- Fontawesome -->
    <link type="text/css" href="./vendor/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
</head>

<body>
    <header class="header-global">
        <div class="nav-wrapper container position-relative">
            <ul class="nav nav-pills nav-fill flex-column flex-sm-row">
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0 active" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#next-meet">Prossimo incontro</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link mb-sm-3 mb-md-0" href="#past-meet">Archivio</a>
                </li>
            </ul>
        </div>
    </header>

    <main>
        <!-- Hero -->
        <section class="section section bg-soft pb-5 overflow-hidden z-2">
            <div class="container z-2">
                <div class="row justify-content-center text-center pt-6">
                    <div class="col-lg-8 col-xl-8">
                        <h1 class="display-2 mb-3">Time agnostic</h1>
                        <p class="lead px-md-6 mb-5">Diario di bordo del tirocinio presso <a
                                href="http://opencitations.net/" alt="Link to the OpenCitations main page"
                                target="_blank"><span class="oc-purple">Open</span><span
                                    class="oc-blue">Citations</span></a>.</p>
                        <div class="d-flex flex-column flex-wrap flex-md-row justify-content-md-center mb-5">
                            <a href="https://github.com/arcangelo7/time_agnostic" target="_blank"
                                class="btn btn-primary mb-3 mb-lg-0 mr-3" alt="Link to the repository"><i
                                    class="fab fa-github mr-2"></i>Vai
                                alla repository</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section pb-5" id="next-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Prossimo incontro (14/04/2021)</h2>
                </div>
                <h3>Cosa ho fatto</h3>
                    <ol>
                        <li>Ho implementato l'algoritmo <strong>add_reference_data_without_doi</strong> che, preso in input un file json contenente l'output di Crossref con le informazioni sui lavori di una determinata rivista, controlla quali reference non hanno DOI name e hanno altri metadati oltre all'unstructured. Dopodiché, chiama il metodo privato <strong>_generate_crossref_query_from_metadata</strong>, che trasforma i metadati in una stringa di query da lanciare tramite API di Crossref. Se la ricerca va a buon fine, seleziona il primo risultato, cerca sul triplestore l'entità citante corrente, verifica che il DOI name della citata trovata non esista già (FILTER NOT EXISTS) e solo allora crea Identifier, BibliographicResource e Citation corrispondenti e li aggiunge al preexisting graph dell'entità citante corrente. </li>
                        <li>Ho implementato il metodo <strong>add_crossref_reference_data</strong>, che arricchisce il dataset corredando le risorse bibliografiche citate di numerose informazioni tratte da Crossref, quali publisher, titolo, sottotitolo, volume, issue, data di pubblicazione, resource embodiment e autori, ciascuna correttamente mappata seguendo l'OCDM.  </li>
                        <li>Il metodo <strong>merge_by_id_from_triplestore</strong> unisce adesso anche le entità di tipo datacite:Identifier associate alle entità unite sulla base dell'identificativo.</li>
                        <li>È stato implementato il metodo <strong>_get_entity_and_ids_from_res</strong>, che si occupa di restituire un'entità e la lista delle entità degli id associati dato un URIRef e un triplestore. </li>
                        <li>Il metodo <strong>_manage_br_type</strong> si occupa adesso di assegnare a ciascuna risorsa bibliografica il tipo corretto, tra book, book-chapter, component, journal-article, monograph, other, posted-content, proceeding-article, report e report-series. </li>
                        <li>Sono stati estratti dal metodo generate_graph tre nuovi metodi privati statici, i quali sono stati riutilizzati dal metodo add_crossref_reference_data:
                            <ol>
                                <li><strong>_manage_author_ra_ar</strong>;</li>
                                <li><strong>_manage_volume_issue</strong>;</li>
                                <li><strong>_manage_resource_embdiment</strong>.</li>    
                            </ol>
                        </li>
                        <li>Prima di avventurarmi nella scrittura di un programma di editing interattivo per Knowledge Graph, ho esplorato la letteratura esistente sull'argomento:
                            <ol>
                                <li>Wright J., Rodríguez Méndez S.J., Haller A., Taylor K., Omran P.G. (2020) Schímatos: A SHACL-Based Web-Form Generator for Knowledge Graph Editing. In: Pan J.Z. et al. (eds) The Semantic Web – ISWC 2020. ISWC 2020. Lecture Notes in Computer Science, vol 12507. Springer, Cham. https://doi.org/10.1007/978-3-030-62466-8_5.
                                    <ol>
                                        <li>L'articolo introduce <strong>Schímatos</strong>, un editor di KG interattivo, che si occupa della validazione degli input utilizzando SHACL Shapes Constraint Language (https://www.w3.org/TR/shacl/).  Non sembra esserci però un modo immediato per riutilizzare il software con KG che non siano WikiData. Sembra infatti essere soltanto una demo a scopo dimostrativo, per quanto molto interessante.</li>
                                    </ol>
                                </li>
                                <li>Heyvaert P., Dimou A., Verborgh R., Mannens E., Van de Walle R. (2016) Graph-Based Editing of Linked Data Mappings Using the RMLEditor. In: Sack H., Rizzo G., Steinmetz N., Mladenić D., Auer S., Lange C. (eds) The Semantic Web. ESWC 2016. Lecture Notes in Computer Science, vol 9989. Springer, Cham. https://doi.org/10.1007/978-3-319-47602-5_25.
                                    <ol>
                                        <li><strong>RMLEditor</strong> offre una GUI per consentire ai data publishers, esperti di dominio, di modellare la conoscenza derivata da dati di origini multiple ed eterogenee. RMLEditor utilizza RML come linguaggio di mappatura sottostante. Ad oggi è presente una demo che permette di modellare fino a 20 nodi per un massimo di 2 MB per singolo file. </li>
                                    </ol>
                                </li>
                                <li>Ho letto la documentazione e ho provato a usare <strong>Web-Karma</strong> (<a href="https://github.com/usc-isi-i2/Web-Karma" target="_blank" alt="Web-Karm documentation">https://github.com/usc-isi-i2/Web-Karma</a>), un tool per modificare un KG in base a un'ontologia specificata. Problema: non ha un sistema di validazione rispetto all'ontologia ed è possibile modificare l'ontologia stessa, intenzionalmente o per errore. In sintesi, utilizzabile solo da chi conosce già molto bene il data model. </li>
                            </ol>
                        </li>
                        <li>Ho letto la documentazione di <strong>Flask</strong>, allo scopo di utilizzare Python per il back-end della mia applicazione di KG editing. Ho anche letto la documentazione di <strong>Jinja</strong>, dato che è il template-engine predefinito di Flask.</li>
                        <li>Utilizzando Flask per il server, Jinja2 come motore di template e Blazegraph come database ho implementato una prima interfaccia grafica che mostra dieci triple. </li>
                    </ol>
                <h3>Cosa non ho capito</h3>
                    <ol>
                        <li>Oltre ai due bug del metodo merge di oc_ocdm già rilevati in <a href="#d0704cnhc1">07/04/2021 (Cosa non ho capito, punti 1 e 2)</a> credo di averne trovato un terzo specifico per il merge tra entità di tipo datacite:Identifier. Ho miniaturizzato e commentato il codice utilizzato al fine di rendere l'errore più facile da comprendere e riprodurre. Inoltre, mi piacerebbe capire se io stia procedendo nel modo corretto prima di mandare un'altra mail a Simone. 
                            <div class="card bg-primary shadow-inset border-light">
                                <pre>
                                    <code class="prettyprint">
        from oc_ocdm.graph.graph_entity import GraphEntity
        from oc_ocdm.graph import GraphSet
        from rdflib import URIRef, Graph
        from SPARQLWrapper import SPARQLWrapper, JSON, RDFXML


        def get_entity_from_res(res:URIRef, graphset:GraphSet) -> GraphEntity:
            res = URIRef(res)
            query = f"""
                CONSTRUCT {{&lt;{res}&gt; ?p ?o}}
                WHERE {{&lt;{res}&gt; ?p ?o}}
            """
            sparql.setQuery(query)
            sparql.setReturnFormat(RDFXML)
            results = sparql.query().convert()
            preexisting_graph = Graph().parse(data=results.serialize(format='xml'), format='xml')
            entity = graphset.add_id(resp_agent="https://orcid.org/0000-0002-8420-0696", res=res, preexisting_graph=preexisting_graph)
            return entity

        # Estrai Identifier e DOI name di tutte le br dal triplestore
        queryString = f"""
            PREFIX datacite: &lt;http://purl.org/spar/datacite/&gt;
            PREFIX fabio: &lt;http://purl.org/spar/fabio/&gt;
            PREFIX literal: &lt;http://www.essepuntato.it/2010/06/literalreification/&gt;
            SELECT ?id ?literalValue
            WHERE
            {{
                ?s a fabio:Expression;
                    datacite:hasIdentifier ?id.
                ?id literal:hasLiteralValue ?literalValue.
            }}
        """
        sparql = SPARQLWrapper("http://localhost:9999/bigdata/sparql")
        sparql.setQuery(queryString)
        sparql.setReturnFormat(JSON)
        results = sparql.query().convert()
        dois_found = dict()
        graphset = GraphSet(base_iri="https://github.com/arcangelo7/time_agnostic/", wanted_label=False)
        for result in results["results"]["bindings"]:
            # Se il DOI name è già stato trovato e l'Identifier è diverso da quello già trovato
            if result["literalValue"]["value"] in dois_found and result["id"]["value"] != dois_found[result["literalValue"]["value"]]:
                id_duplicated_entity = get_entity_from_res(result["id"]["value"], graphset)
                id_preexisting_entity = get_entity_from_res(dois_found[result["literalValue"]["value"]], graphset)
                print(f"Merging entity {id_preexisting_entity.res} with {id_duplicated_entity.res}")
                id_preexisting_entity.merge(id_duplicated_entity)
            else:
                # Registra di aver trovato questo DOI name e che è associato a questo Identifier
                dois_found[result["literalValue"]["value"]] = result["id"]["value"]
                                </code>
                            </pre>
                        </div>
                        Se a questo punto provo a fare l'upload attraverso il metodo upload_all passando il graphset, l'upload non avviene a causa di un errore nella formattazione della query di upload. In particolare, pare che manchi un ";" prima di un INSERT.
                    </li>    
                    <li>Per qualche ragione a volte l'update_query vuota non è "" ma None. Il metodo upload_all della classe Storer di oc_ocdm non gestiste il caso in cui l'update_query sia None, sollevando un'eccezione nel momento in cui viene fatta la seguente concatenazione: query_string += " ; " + update_query, dato che non è possibile concatenare una stringa con None. Ho risolto in locale aggiungendo:
                        <div class="card bg-primary shadow-inset border-light">
                            <pre>
                                <code class="prettyprint">
        # for idx, entity in enumerate(self.a_set.res_to_entity.values()):
            # update_query, n_added, n_removed = get_update_query(entity, entity_type=self._class_to_entity_type(entity))
                if update_query == "" or update_query is None:
                    skipped_queries += 1
                                </code>
                            </pre>
                        </div>
                        Tuttavia, dato che l'update_query vuota dovrebbe sempre essere "" e non None, credo ci sia un altro bug a monte. 
                    </li>    
                    <li>Non ho trovato documentazione su tutti i possibili metadati delle reference di un work restituite da Crossref, quindi ho scoperto tramite un algoritmo che, sui quasi 7000 articoli citanti di Scientometrics, i citati posso riportare le chiavi sotto elencate. Di fianco a ciascuna viene indicato tramite quale notazione sono state usate nella query a Crossref al fine di recuperare il DOI name:
                        <ul>
                            <li>journal-title → query.bibliographic=value&</li>
                            <li>author → query.author=value&</li>
                            <li>journal-title → query.container-title=value&</li>
                            <li>ISBN → filter=isbn:value</li>
                            <li>year → filter=from-index-date:value,
                                <ul>
                                    <li>Perché proprio from-index-date? Perché nella documentazione si legge:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            When using time filters to retrieve periodic, incremental metadata updates, the from-index-date filter should be used over from-update-date, from-deposit-date, from-created-date and from-pub-date. The timestamp that from-index-date filters on is guaranteed to be updated every time there is a change to metadata requiring a reindex.
                                        </blockquote>
                                    </li>
                                </ul>
                            </li>    
                        </ul>
                        Tuttavia, ci sono molti altri campi per i quali Crossref non sembra non fornire né field queries né filtri, ovvero: 
                            <ul>
                                <li>issue</li>
                                <li>volume-title</li>
                                <li>series-title</li>
                                <li>edition</li>
                                <li>key</li>
                                <li>first-page</li>
                                <li>volume</li>
                                <li>isbn-type</li>
                            </ul>
                        Le domande a questo punto sono:
                            <ol>
                                <li>Ti risulta che io non possa fare query a Crossref utilizzando quelle informazioni?</li>
                                <li>È possibile utilizzare volume-title e series-title come valore della field query <em>query.container-title</em>?</li>
                                <li>I risultati ottenuti finora sono sorprendentemente buoni, vale la pena restringere ulteriormente la ricerca?</li>
                            </ol>
                        <li>Cristian mi ha detto che per il suo progetto un'informazione rilevante è l'affiliazione degli autori. Credo che l'OCDM non la preveda, giusto?</li>
                        <li>Se una risorsa bibliografica viene indicata da Crossref come di tipo "posted-content" con quale tipo di BibliographicResource presente nell'OCDM devo mapparla? </li>
                    </li>
                </ol>
            </div>
        </section>

        <section class="section pb-5" id="past-meet">
            <div class="container">
                <div class="row justify-content-center mb-5">
                    <h2>Archivio</h2>
                </div>
                <div class="accordion shadow-soft rounded" id="accordionExample1">
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-1" data-target="#panel-1" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-1">
                            <span class="h6 mb-0 font-weight-bold">10/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-1">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                        <ol>
                                            <li>Peroni, S., Shotton, D., Vitali, F. (2016). <em>A document-inspired way for
                                                    tracking changes
                                                    of RDF data</em>. <a
                                                    href="https://w3id.org/oc/paper/occ-driftalod2016.html"
                                                    alt="Link to the paper">https://w3id.org/oc/paper/occ-driftalod2016.html</a>.
                                            </li>
                                            <li>Daquino, M., Peroni, S., Shotton D. (2020). <em>The OpenCitations Data
                                                    Model</em>. <a
                                                    href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                                    alt="Link to the OpenCitations Data Model documentation">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>
                                            </li>
                                            <li>Documentazione delle seguenti librerie e API: <em>Crossref REST API</em>,
                                                <em>REST API for
                                                    COCI</em>, <em>oc_ocdm</em>.
                                            </li>
                                            <li>Ho sfogliato il lavoro fin qui svolto da Arianna Moretti.</li>
                                        </ol>
                                    </li>
                                    <li>Ho scritto il seguente codice con lo scopo di comprendere il funzionamento di
                                        tutte le librerie e API su menzionate, nonché di produrre output utili in vista
                                        della realizzazione del dataset su "Scientometrics": <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/scientometrics.py"
                                            target="_blank" alt="Link to scientometrics.py">scientometrics.py</a>.</li>
                                    <div class="col-12 mt-3 mb-4">
                                        <div class="card bg-primary shadow-inset border-light">
                                            <div class="card-body p-5">
                                                <pre>
                                                    <code class="prettyprint">
import requests, requests_cache, json
from oc_ocdm.graph import GraphSet
from oc_ocdm.storer import Storer
from oc_ocdm.support import create_date
from rdflib import URIRef


def get_journal_data(journal_issn, i_am_polite):
    journal_data = requests.get(url = f'http://api.crossref.org/journals/{{{journal_issn}}}/works?mailto={i_am_polite}')
    journal_data_json = journal_data.json()
    return journal_data_json


def get_all_references_from_journal(journal_data_json):
    journal_data_items = journal_data_json["message"]["items"]
    all_references = list()
    for item in journal_data_items:
        references = requests.get(url = f'https://w3id.org/oc/index/coci/api/v1/references/{item["DOI"]}?format=json')
        references_json = references.json()
        all_references.append(references_json)
    return all_references


def update_graph(journal_data_json, graphset):
    journal_data_items = journal_data_json["message"]["items"]
    for item in journal_data_items:
        try:
            # a volte gli articoli ritornati da crossref non hanno il campo "author". È molto raro, ma accade.
            responsible_agent_name = item["author"][0]["given"] + " " + item["author"][0]["family"]
            responsible_agent = scientometrics_graphset.add_ra(responsible_agent_name)
            responsible_agent.has_given_name(item["author"][0]["given"])
            responsible_agent.has_family_name(item["author"][0]["family"])
            # non ho ancora gestito il problema dell'omonimia, ma sono consapevole che vada gestito
            scientometrics_br = scientometrics_graphset.add_br(responsible_agent)
            scientometrics_br.has_title(item["title"][0])
            iso_date_string = create_date([item["published-print"]["date-parts"][0][0]])
            scientometrics_br.has_pub_date(iso_date_string)
        except KeyError:
            pass
    graphset.commit_changes()


# Crossref test
requests_cache.install_cache('cache')
issn_web_scientometrics = "1588-2861"
my_mail = "arcangelo.massari@studio.unibo.it"
journal_data = get_journal_data(issn_web_scientometrics, my_mail)

# REST API for COCI test
all_references = get_all_references_from_journal(journal_data)

# oc_ocdm test
scientometrics_graphset = GraphSet("https://arcangelo7.github.io/time_agnostic/")
update_graph(journal_data, scientometrics_graphset)

# Retrieved data dump
with open('data/scientometrics.json', 'w') as outfile:
    json.dump(journal_data, outfile)
with open('data/references.json', 'w') as outfile:
    json.dump(all_references, outfile)
storer = Storer(scientometrics_graphset)
storer.store_graphs_in_file("data/graph.json", "./")                      
                                                    </code>
                                                </pre>
                                            </div>
                                        </div>
                                    </div>
                                    <li>Ho ottenuto in output tre file json:</li>
                                    <ol>
                                        <li>Una lista di 20 lavori (articoli, libri, atti di convegni, etc) presenti
                                            nella rivista "Scientometrics", con relativi metadati: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/scientometrics.json"
                                                alt="Link to scientometrics.json"
                                                target="_blank">scientometrics.json</a>.</li>
                                        <li>I dati citazionali per tutti i riferimenti in uscita relativi ai DOI di 20
                                            lavori presenti nella rivista "Scientometrics": <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/references.json"
                                                target="_blank" alt="Link to references.json">references.json</a>.</li>
                                        <li>Una lista di grafi conformi a OCDM v2.0.1 contenente 20 entità di tipo
                                            BibliographicResource relative a 20 lavori presenti in "Scientometrics", con
                                            metadati relativi a titolo e data di pubblicazione: <a
                                                href="https://github.com/arcangelo7/time_agnostic/blob/main/data/graph.json"
                                                target="_blank" alt="Link to the graph">graph.json</a>.</li>
                                    </ol>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non posso accedere a <em>The OpenCitations Data Model</em> su SpringerLink (<a
                                            href="https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28"
                                            alt="Link to the OpenCitations Data Model docuemntation on Springer"
                                            target="_blank">https://link.springer.com/chapter/10.1007%2F978-3-030-62466-8_28</a>)
                                        neanche tramite proxy Unibo. È diverso rispetto a quello pubblicato su figshare?
                                        (<a href="https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7"
                                            alt="Link to the OpenCitations Data Model documentation on Figshare">https://figshare.com/articles/online_resource/Metadata_for_the_OpenCitations_Corpus/3443876/7</a>)
                                    </li>
                                    <li>Gli articoli di "Scientometrics" ritornati tramite GET a
                                        http://api.crossref.org/journals/{1588-2861}/works, dove 1588-2861 è l'ISSN web
                                        di "Scientometrics", sono 20 alla volta. Tramite il parametro rows è possibile
                                        aumentare questo numero a massimo 1000. Come faccio a ritornarli tutti?</li>
                                    <li>A quale ISSN di "Scientometrics" devo fare riferimento, quello web (1588-2861),
                                        quello stampa (0138-9130) o entrambi?</li>
                                    <li>Per creare una nuova entità di qualunque tipo, sia essa una Bibliographic
                                        Resource o una Citation, occorre passare in input al metodo corrispondente il
                                        responsible agent. Ad esempio, <code
                                            class="prettyprint">scientometrics_graphset.add_br(responsible_agent)</code>.
                                        Dato che l'OCDM prevede il Responsible agent come tipo, credo ci si riferisca ad
                                        esso. Su questo ho due dubbi:
                                        <ol>
                                            <li>Nel grafo finale non vedo il collegamento tra i Responsible Agents e le
                                                corrispondenti Bibliographic Resources. Entrambi vengono indicati da un
                                                URL nella forma [dataset URL][entity dataset identifier] - ad esempio
                                                https://arcangelo7.github.io/time_agnostic/ra/6 - ma il grafo non
                                                riporta qual è la Bibliographic Resource corrispondente. Fa fede il
                                                numero? Ovvero il Responsible Agent 6 è responsabile della Bibliographic
                                                Resource 6?</li>
                                            <li>Di alcuni autori viene riportato l'ORCID, di altri no. Suggerimenti su
                                                come gestire le omonimie?</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-2" data-target="#panel-2" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-2">
                            <span class="h6 mb-0 font-weight-bold">17/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-2">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho riorganizzato il codice utilizzando la programmazione a oggetti per rendere
                                        lo sviluppo più mantenibile.</li>
                                    <li>Utilizzando il parametro <em>cursor</em>, ho aumentato da 20 a 6065 il numero di
                                        works contenuti in "Scientometrics" ritornati tramite Crossref. Ora ci sono
                                        tutti.</li>
                                    <li>Ho generato 3 output corrispondenti a 3 grafi, di cui si elencano i rispettivi
                                        metadati considerati:
                                        <ol>
                                            <li>MetadataSet (dcat:Dataset)
                                            <ol>
                                                <li>Titolo (dcterms:title)</li>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>Data di modifica (dcterms:modified)</li>
                                            </ol></li>
                                            <li>Provenance (prov:Entity)
                                            <ol>
                                                <li>Descrizione (dcterms:description)</li>
                                                <li>is snapshot of (prov:specializationOf)</li>
                                                <li>Attribuzione (prov:wasAttributedTo)</li>
                                                <li>Data di creazione (prov:generatedAtTime)</li>
                                            </ol></li>
                                            <li>Graphset (set di grafi sulle entità bibliografiche)
                                            <ol>
                                                <li>BibliographicResource, per la rivista, i suoi articoli e gli articoli
                                                    citati dai suoi articoli (fabio:Expression, fabio:Journal,
                                                    fabio:JournalArticle)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Titolo (dcterms:title)</li>
                                                    <li>Sottotitolo, laddove presente (fabio:hasSubtitle)</li>
                                                    <li>Identificatore, DOI per gli articoli, ISSN per la rivista
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Se articolo, autore (pro:isDocumentContextFor)</li>
                                                    <li>Se articolo, data di pubblicazione, laddove presente
                                                        (prism:publicationDate)</li>
                                                    <li>Se articolo, rivista di appartenenza (frbr:partOf)</li>
                                                    <li>Formato (frbr:embodiment)</li>
                                                </ol></li>
                                                <li>Citation (cito:Citation, cito:JournalSelfCitation,
                                                    cito:AuthorSelfCitation, cito:DistantCitation)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Documento citante (cito:hasCitingEntity)</li>
                                                    <li>Documento citato (cito:hasCitedEntity)</li>
                                                    <li>Data di creazione (cito:hasCitationCreationDate)</li>
                                                    <li>Time span (cito:hasCitationTimeSpan)</li>
                                                </ol></li>
                                                <li>ResourceEmbodiment (fabio:manifestation, fabio:DigitalManifestation,
                                                    fabio:PrintObject)
                                                <ol>
                                                    <li>Tipo, laddove presente (rdf:type)</li>
                                                    <li>Media type, laddove presente (dcterms:format)</li>
                                                    <li>Pagina iniziale, laddove presente (prism:startingPage)</li>
                                                    <li>Pagina finale, laddove presente (prism:endingPage)</li>
                                                    <li>URL, laddove presente (frbr:exemplar)</li>
                                                </ol></li>
                                                <li>ResponsibleAgent (foaf:Agent)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Identificativo, ovvero ORCID, laddove presente
                                                        (datacite:hasIdentifier)</li>
                                                    <li>Nome, laddove presente (foaf:givenName)</li>
                                                    <li>Cognome, laddove presente (foaf:familyName)</li>
                                                    <li>Nome completo, laddove presente (foaf:name)</li>
                                                </ol></li>
                                                <li>AgentRole (pro:RoleInTime)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di ruolo (pro:withRole)</li>
                                                    <li>Is held by (pro:isHeldBy)</li>
                                                </ol></li>
                                                <li>Identifier (datacite:Identifier)
                                                <ol>
                                                    <li>Tipo (rdf:type)</li>
                                                    <li>Tipo di identificatore (datacite:usesIdentifierScheme)</li>
                                                    <li>Stringa (literalreification:hasLiteralValue)</li>
                                                </ol></li>
                                            </ol></li>
                                        </ol>
                                    </li>
                                    <li>La mappatura è avvenuta nel modo più generico e rivista-indipendente possibile.
                                        Dovrebbe essere possibile utilizzare lo stesso software per mappare seguendo
                                        l'ocdm qualunque rivista ritornata da Crossref.</li>
                                    <li>È stata implementata una barra di caricamento per mostrare all'utente quanto
                                        occorra aspettare per ricevere tutti i dati tramite le varie API.</li>
                                    <li>Il codice è disponibile alla seguente repository: <a
                                            href="https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py"
                                            target=_blank
                                            alt="Link to the Python script">https://github.com/arcangelo7/time_agnostic/blob/main/dataset_builder.py</a>.
                                        Purtroppo, non è stato possibile caricare gli output su GitHub date le loro
                                        dimensioni.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Come faccio a esprimere che un certo issue fa parte di un certo volume
                                        utilizzando l'OCDM? Una BibliographicResource ha come metodo has_number, che
                                        però è generico.</li>
                                    <li>Un AgentRole ha tra i suoi metodi has_next. A questo proposito, la
                                        documentazione di oc_ocdm dice:
                                        <blockquote class="blockquote ml-5 mt-3">The previous role in a sequence of
                                            agent
                                            roles of the same type associated with the same bibliographic resource (so
                                            as to define, for instance, an ordered list of authors).</blockquote>
                                        Tuttavia, il documento <em>The Open Citations Data Model</em> parla di
                                        "following role", non di "previous". Credo faccia quindi fede quest'ultimo. In
                                        ogni caso, non ho capito a cosa ci si sta riferendo, per ragioni affini a quelle
                                        della domanda successiva.
                                    </li>
                                    <li>Come faccio a collegare una BibliographicResource ai suoi ResponsibleAgents? È
                                        presente il metodo has_contributor, che però riceve come argomento un AgentRole,
                                        non un ResponsibleAgent. Devo quindi avere tanti AgentRoles quanti sono i
                                        ResponsibleAgents e usarli come intermediari tra la BibliographicResource e i
                                        ResponsibleAgents? Sembrerebbe di sì, dato che tale predicato viene mappato con
                                        pro:isDocumentContextFor, che presenta la stessa contorsione logica: è come se
                                        invece di dire "<em>The Open Citations Data Model</em> è scritto da Silvio
                                        Peroni" dicessi "<em>The Open Citations Data Model</em> è il contesto nel quale
                                        il ruolo di autore di Silvio Peroni si manifesta". </li>
                                    <li>Alcuni articoli di rivista hanno sia una data di pubblicazione a stampa, che una
                                        data per la pubblicazione digitale. Tuttavia, OCDM prevede un'unica data di
                                        pubblicazione attraverso il metodo has_pub_date. Quale uso delle due? Oltre a
                                        questo, come tipo di ResourceEmbodiment devo indicare sia il formato stampa che
                                        il formato digitale?</li>
                                    <li>Tra i predicati di una Citation c'è has_citation_characterization. A questo
                                        proposito, la documentazione dell'OCDM recita:
                                        <blockquote class="blockquote ml-5 mt-3">The citation function characterizing
                                            the
                                            purpose of the citation.</blockquote>
                                        Tuttavia, non ho capito cosa voglia dire.
                                    </li>
                                    <li>Ogni articolo di Scientometrics cita altri articoli sia della rivista stessa che
                                        di riviste esterne. Devo creare una BibliograficResource per ciascuna degli
                                        articoli esterni a Scientometrics?</li>
                                    <li>Crossref riporta le reference di ciascun articolo indicando, tra gli altri, la
                                        "key". Che cosa indica? Ad esempio:
                                        <pre>
                                            <code class="prettyprint">
{
    "DOI": "10.1126/science.280.5364.698",
    "author": "M. Heller",
    "doi-asserted-by": "crossref",
    "first-page": "698",
    "journal-title": "Science",
    <strong>"key"</strong>: "207_CR21",
    "unstructured": "Heller, M., H. Eisenberg (1998), Can Patents Deter Innovation? The Anticommons in Biomedical Research, Science, 280: 698\u2013701.",
    "volume": "280",
    "year": "1998"
}
                                            </code>
                                        </pre>
                                    </li>
                                    <li>I dati sulle reference riportati da Crossref e dall'API for COCI sono spesso
                                        uguali, ma non sempre: a volte il COCI riporta citazioni che Crossref non
                                        riporta e viceversa. Quale dei due fa fede? Entrambi?</li>
                                    <li>A causa dell'elevato numero di articoli, i tempi di risposta di Crossref e
                                        dell'API for COCI sono biblici, si parla di ore. Per questo motivo, una volta
                                        ottenuti i dati la prima volta, li ho messi in cache per non doverli più
                                        richiedere. Inoltre, attraverso un dizionario, ho evitato di chiedere due volte
                                        metadati già richiesti al COCI. Oltre a queste misure, c'è qualcosa che non so e
                                        che dovrei fare per rendere i tempi di esecuzione accettabili?</li>
                                    <li>Ammettendo che le esecuzioni dei GET a Crossref e COCI siano istantanee, la
                                        creazione del grafo vero e proprio avendo i dati occupa il 95% di 16 GB di RAM
                                        DDR4 a 3000 MHz e dura ugualmente uno sproposito (per rendere l'idea, aprire
                                        qualunque altro programma che non sia Python causa il crash dello script per out
                                        of memory). Inoltre, volendo esportare il risultato in json, questo pesa oltre
                                        400 MB. Non è quindi caricabile su GitHub, neanche usando Git Large File
                                        Storage, perché andrei velocemente oltre il GB mensile gratuito consentito. Con
                                        questa mole di dati diventa insomma veramente complicato lavorare. Consigli?
                                    </li>
                                    <li>Idealmente, mi sarebbe piaciuto creare un software che, preso in input un ISSN,
                                        ritornasse in output il grafo relativo a quella rivista rispettando l'OCDM.
                                        Tuttavia, a causa dei lunghi tempi d'attesa, e quindi del rischio di crash lungo
                                        il percorso, sono stato costretto a spezzettare il processo su più funzioni da
                                        chiamare manualmente. È possibile realizzare l'ideale?</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-3" data-target="#panel-3" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-3" id="d2303cnhc7p">
                            <span class="h6 mb-0 font-weight-bold">23/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-3">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho letto:
                                    <ol>
                                        <li>Peroni, S., Shotton, D., Vitali, F. (2012). Scholarly publishing and Linked
                                            Data: describing roles, statuses, temporal and contextual extents.
                                            Association for Computing Machinery, New York. Retrieved from <a
                                                href="https://doi.org/10.1145/2362499.2362502" alt="Link to the article"
                                                target="_blank">https://doi.org/10.1145/2362499.2362502</a>.</li>
                                        <ol>
                                            <li>Vi ho appreso con entusiasmo che dietro l'entità apparentemente contorta
                                                pro:RoleInTime si cela invece la possibilità di definire feature tempo e
                                                contesto-dipendenti e di effettuare query più profonde.</li>
                                        </ol>
                                        <li>Peroni, S., Shotton, D. (2012). FaBiO and CiTO: Ontologies for describing
                                            bibliographic resources and citations. Journal of Web Semantics, Volume 17,
                                            Pages 33-43.
                                            Retrieved from <a href="https://doi.org/10.1016/j.websem.2012.08.001"
                                                alt="Link to the article"
                                                target="_blank">https://doi.org/10.1016/j.websem.2012.08.001</a>.</li>
                                        <ol>
                                            <li>Ne ho tratto informazioni sulla genesi di FaBio E CiTO e in particolare
                                                sul valore del predicato cito:hasCitationCharacterization.</li>
                                        </ol>
                                        <li>Documentazione di Blazegraph.</li>
                                    </ol></li>
                                    <li>Il Graphset contiene adesso informazioni sui volumi e sugli issues,
                                        rappresentati come BibliographicResources rispettivamente di tipo JournalVolume
                                        e JournalIssue, aventi come predicati fabio:hasSequenceIdentifier e frbr:partOf.
                                    </li>
                                    <li>Gli AgentRoles (pro:RoleInTime) presentano adesso il predicato oco:hasNext,
                                        avente come oggetto il ruolo successivo in una sequenza di agent roles dello
                                        stesso tipo associati alla stessa risorsa bibliografica, allo scopo di definire,
                                        ad esempio, una lista ordinata di autori.</li>
                                    <li>La data di pubblicazione considerata (prism:publicationDate) è adesso quella
                                        contenuta nel campo "issued". Sono stati inoltre creati tanti ResourceEmbodiment
                                        quante sono le manifestazioni, indicando la pagina iniziale e finale per i
                                        formati a stampa, il media-type e l'URL per i formati digitali.</li>
                                    <li>Sono state aggiunte tante entità di tipo BibliographicReference quante sono le
                                        reference riportate da Crossref con un DOI. Il loro contenuto, proveniente dal
                                        campo "unstructured", è stato riportato tramite c40:hasContent. Ciascuna
                                        BibliographicReference è stata collegata alla rispettiva BibliographicResource
                                        tramite biro:references. Allo stesso tempo, ogni BiblographicResource è stata
                                        collegata alle rispettive BibliographicReferences tramie frbr:part.</li>
                                    <li>È stata create una nuova classe, DatasetAutoEnhancer, pensata per contenere
                                        metodi e proprietà che migliorino la qualità del dataset in maniera automatica.
                                        Al momento presenta:
                                    <ol>
                                        <li>il metodo merge_by_id, il quale unisce due entità nel caso in cui gli
                                            identificativi ad esse associate coincidano. Nella fattispecie, unisce due
                                            BibliographicResources con lo stesso DOI e due ResponsibleAgents con lo
                                            stesso ORCID.</li>
                                        <li>Il metodo privato _generate_snaphot, che si occupa di aggiornare il grafo
                                            sulla provenance registrando i delta rispetto a quello precedente, in
                                            particolare:
                                        <ol>
                                            <li>Crea lo snaphsot.</li>
                                            <li>Definisce la data di creazione (has_generation_time).</li>
                                            <li>Definisce di quale entità è lo snaphot (is_snapshot_of).</li>
                                            <li>Definisce l'azione di update effettuata tramite una SPARQL query
                                                (has_update_action). La SPARQL query è stata a sua volta generata
                                                tramite il metodo get_update_query del modulo support.query_utils di
                                                oc_ocdm.</li>
                                            <li>Associa allo snapshot precedente la data di invalidazione
                                                (has_invalidation_time).</li>
                                            <li>Collega lo snapshot a quello precedente (derives_from).</li>
                                            <li>Definisce la risorsa primaria da cui hanno origine i metadati nello
                                                snapshot, ovvero Crossref (has_primary_source).</li>
                                            <li>Definisce l'agente responsabile delle modifiche (has_resp_agent).</li>
                                        </ol></li>
                                    </ol></li>
                                    <li>È stata implementata una nuova classe, Support, che contiene i seguenti metodi:
                                    
                                    <ol>
                                        <li>zip_data, per comprimere i dati in un archivio.</li>
                                        <li>minify_json, per minificare i file json in modo che siano parsabili dal
                                            metodo load della classe Reader di oc_ocdm.</li>
                                        <li>measure_runtime, per misurare i tempi di esecuzione delle varie funzioni.
                                        </li>
                                        <li>dump_dataset, per conservare i dataset in un file json.</li>
                                        <li>dump_json, per salvare gli oggetti json in un file.</li>
                                    </ol>
                                </ol></li>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>Non mi è mai capitato per "Scientometrics", ma come mi comporto se viene
                                        indicato l'issue e non il volume? Creo un entità Volume vuota e le collego
                                        l'issue?</li>
                                    <li>
                                        <p>Con il seguente codice ho espresso la relazione oco:hasNext per tutti gli
                                            AgentRoles (riporto solo le parti del codice rilevanti)</p>
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <pre>
                                                <code class="prettyprint">
author_agent_roles = list()
for author in item["author"]:
        # [...]
        author_ar = journal_graphset.add_ar(your_orcid)
        author_ar.create_author()
        author_ar.is_held_by(item_ra)
        item_br.has_contributor(author_ar)
        author_agent_roles.append(author_ar)
for index, author_agent_role in enumerate(author_agent_roles):
        if index+1 &lt; len(author_agent_roles):
                author_agent_role.has_next(author_agent_roles[index+1])
                                                </code>
                                            </pre>
                                        </div>
                                        A livello di AgentRole, il grafo finale riporta correttamente il successivo
                                        AgentRole. Tuttavia, guardando le proprietà pro:isDocumentContextFor delle
                                        BibliographicResources, la lista risulta disordinata. Sto sbagliando io o è un
                                        bug della libreria?
                                    </li>
                                    <li>Nel caso in cui per un articolo venga riportata una sola pagina e non un
                                        intervallo di pagine, è corretto indicare quella pagina sia come pagina iniziale
                                        che come pagina finale? Perché OCDM prevede solo i predicati per gli intervalli
                                        di pagine e non per le pagine singole.</li>
                                    <li>Una BibliographiResource ha tra i suoi predicati has_edition. La documentazione
                                        riporta:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An identifier for one of several alternative editions of a particular
                                            bibliographic resource.
                                        </blockquote>
                                        Non ho capito a cosa ci si riferisca.
                                    </li>
                                    <li>Sono riuscito ad avviare il NanoSparqlServer di Blazegraph utilizzando il
                                        comando: <code
                                            class="prettyprint">java -server -Xmx4g -jar blazegraph.jar</code>.
                                        In questo modo, la workbench è diventata disponibile all'indirizzo
                                        http://localhost:9999/blazegraph/.
                                        Tuttavia, non ho capito cosa passare come triplestore_url al metodo upload_all
                                        della classe Storer di oc_ocdm. Ho provato a passare sia
                                        http://localhost:9999/blazegraph/ che http://localhost:9999/bigdata/sparql e
                                        l'output del metodo è stato in entrambi i casi True, il che sembrerebbe
                                        incoraggiante, ma attraverso l'endpoint SPARQL della workbench mi sono accorto
                                        di non aver caricato alcuna tripla. A quale URL devo fare il POST?
                                    </li>
                                    <li>Allo scopo di aumentare la provenance, ho inizialmente provato ad aprire il
                                        grafo precedentemente creato utilizzando i metodi load e
                                        import_entities_from_graph della classe Reader di oc_ocdm. Tuttavia, il secondo
                                        metodo ritorna una lista di GraphEntity, che non mi permette di utilizzare
                                        metodi propri della classe GraphSet, come get_id() o get_br(), costringendomi a
                                        ciclare su tutti gli elementi della lista e controllare manualmente di che
                                        classe sono istanza. Inoltre, per qualche motivo che ignoro, il metodo
                                        import_entities_from_graph richiede notevolmente più tempo che creare il grafo
                                        da zero. Ho quindi deciso di aumentare la provenance direttamente durante la
                                        creazione del GraphSet, ovvero tramite più commit successivi. È corretto?</li>
                                    <li id="d2303cnhc7">Per quanto riguarda la creazione di uno snapshot, quanto deve essere esplicitato
                                        e quanto viene fatto in automatico? Mi spiego meglio: quando unisco un'entità a
                                        un'altra tramite il metodo merge, tutto ciò che ruota attorno a quelle entità
                                        rimane invariato. Ad esempio, se unisco due risorse bibliografiche dopo aver
                                        scoperto che hanno lo stesso DOI, le rispettive entità di tipo Identifier non
                                        vengono unite a loro volta. Confermi che io lo debba fare manualmente? Allo
                                        stesso tempo, la risorsa che è scomparsa dopo essere stata unita continua a
                                        essere richiamata in altre parti del grafo anche se non esiste più.</li>
                                    <li>Quando faccio un merge devo generare uno snapshost sia per la risorsa unita che
                                        per quella contenitore? Perché ho notato che così facendo si creano snapshot
                                        duplicati.</li>
                                    <li>Tra i metadati di uno snapshot c'è la sua descrizione. Per ottenerla devo
                                        parsare la stringa con la SPARQL query. Posso certamente farlo io da zero, ma
                                        non vorrei reinventare la ruota. Esistono delle librerie per farlo? Ne ho
                                        trovate alcune ma mi sembrano molto amatoriali.</li>
                                    <li>
                                        Allo scopo di ottenere i DOI a partire dal contenuto delle
                                        BibliographicReferences - ovvero il campo "unstructured" riportato da Crossref -
                                        ho provato a effetturare delle query su Crossref. Per fare un esempio:
                                        <pre class="mt-3">
            <code class="prettyprint">https://api.crossref.org/works?query.bibliographic=Carberry%2C+Josiah.+%E2%80%9CToward+a+Unified+Theory+of+High-Energy+Metaphysics%3A+Silly+String+Theory.%E2%80%9D+Journal+of+Psychoceramics+5.11+%282008%29%3A+1-3.</code>
                                        </pre>
                                        Dopodiché, ho controllato che il campo "score" contenesse un numero maggiore di
                                        90 e, se sì, ho estratto il DOI dall'item. A questo proposito ho tre domande:
                                    
                                    <ol>
                                        <li>Come funziona lo score? Perché ho notato che a volte risultati con score
                                            superiore a 100 sono sbagliati.</li>
                                        <li>La query non ritorna solo il risultato migliore, ma un numero variabile di
                                            hits più o meno pertinenti. Come faccio a ottimizzare la ricerca perché
                                            ritorni solo il risultato più pertinente? Ho provato con il parametro
                                            enable-multiple-hits impostato a false (peraltro di default), ma senza
                                            successo.</li>
                                        <li>È questo il metodo migliore per ottenere un DOI da una reference non
                                            strutturata?</li>
                                    </ol></li>
                                    <li>Persiste il problema della memoria. Ho provato ad aggiungere due banchetti che
                                        avevo in un altro PC e portare la RAM a 32 GB, ho attivato l'X.M.P., l'ho
                                        overclockata e ho portato il file di paging a 8 GB, ma non è bastato, il
                                        processo di creazione ed esportazione dei grafi l'ha nuovamente saturata.
                                        Riporto uno screen del messaggio di errore:
                                        <div class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                            <img src="./assets/img/memory_error.png" alt="memory error image">
                                        </div>
                                        Utilizzando Jupyer Notebook e il monitoraggio risorse, ho potuto eseguire i vari
                                        passaggi singolarmente, per misurare quanta RAM occorresse a ciascuno:
                                        <ol>
                                            <li>
                                                <p>Creare il graphset ha occupato circa 18.4 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_graphset.png"
                                                        alt="RAM left after GraphSet creation">
                                                </div>
                                            </li>
                                            <li>
                                                <p>Creare il grafo sulla provenance ha portato la memoria occupata a
                                                    circa 26.8 GB</p>
                                                <div
                                                    class="card bg-primary shadow-inset border-light col-12 mt-3 mb-4 p-4">
                                                    <img src="./assets/img/ram_after_prov.png"
                                                        alt="RAM left after Provenance creation">
                                                </div>
                                            </li>
                                            <li>Infine, il dump del dataset sulla provenance ha fatto crashare il
                                                programma.</li>
                                        </ol>
                                        Sono infine riuscito a risolvere portando il file di paging a 16 GB, ma in tutta
                                        onestà non la ritengo una soluzione valida, né filosoficamente né guardando al
                                        futuro prossimo in cui i dati saranno molti di più. Cosa ne pensi?
                                    </li>
                                    <li>Durante l'ultimo incontro hai fatto due riferimenti che sono rimasti in sospeso:
                                        <ol>
                                            <li>A proposito di OpenCitations Meta, hai menzionato un articolo che
                                                introduce a OpenCitations. Ho verificato di non averlo letto. Potresti
                                                inviarmelo?</li>
                                            <li>Hai menzionato alcune questioni di cui discutere dopo la risposta alle
                                                domande, che però non sono più state discusse.</li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Note</h3>
                                <p>La documentazione del metodo import_entities_from_graph della classe Reader di
                                    oc_ocdm non è aggiornata. Il metodo vuole infatti tre argomenti obbligatori, non
                                    due: il GraphSet, il Graph e il responsible agent. Quest'ultimo non viene menzionato
                                    dalla documentazione.</p>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-4" data-target="#panel-4" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-4">
                            <span class="h6 mb-0 font-weight-bold">31/03/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-4">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Ho caricato dati e provenance su Blazegraph.</li>
                                    <li>Ho implementato:
                                        <ol>
                                            <li>un sistema di gestione delle richieste tramite api più raffinato, con un timeout, un numero
                                                massimo di tentativi separati da intervalli progressivamente crescenti tramite backoff factor e
                                                con la possibilità di salvare gli errori in un file di log;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di verificare l'esistenza di
                                                entità associate allo stesso id. Nel caso le trovi, recupera i grafi associati e li unisce;</li>
                                            <li>una funzione che effettua query SPARQL su un triplestore al fine di ottenere tutti i DOI degli
                                                articoli appartenenti a una determinata rivista e i DOI di tutti gli articoli da essi citati.
                                                Dopodiché, controlla se su COCI sono presenti altri DOI di articoli citati e, se li trova,
                                                recupera il grafo dell'entità citante e gli aggancia le varie entità relative alla risorsa
                                                citata, ovvero l'Identifier, la BibliographicResource e la Citation (data di creazione,
                                                timespan, journal o author self-citation). Per quanto riguarda i DOI degli articoli citati già
                                                presenti sul triplestore, vengono aggiunte alle rispettive entità Citation il timespan e se sono
                                                autocitazioni della rivista o dell'autore;</li>
                                            <li>una gestione più sofisticata di volume, issue e articoli:
                                                <ol>
                                                    <li>se ci sono sia volume che issue l'articolo fa parte dell'issue, l'issue del volume e il
                                                        volume della rivista;</li>
                                                    <li>se c'è solo il volume, l'articolo fa parte del volume e il volume della rivista;</li>
                                                    <li>se c'è solo l'issue, l'articolo fa parte dell'issue e l'issue della rivista;</li>
                                                    <li>se non ci sono né issue né volume l'articolo fa parte della rivista. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>Ho aperto i seguenti issue:
                                        <ol>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/4" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/4</a></li>
                                            <li><a href="https://github.com/opencitations/oc_ocdm/issues/5" target="_blank"
                                                    alt="Link to the GitHub issue">https://github.com/opencitations/oc_ocdm/issues/5</a></li>
                                        </ol>
                                    </li>
                                    <li>Ho letto:
                                        <ol>
                                            <li>International DOI Foundation. (2019). DOI® Handbook. https://doi.org/10.1000/182.
                                                <ol>
                                                    <li>Sì, ho seriamente letto tutto l'handbook.</li>
                                                </ol>
                                            </li>
                                            <li>Fecher, B., & Friesike, S. (2014). Open Science: One Term, Five Schools of Thought. In S.
                                                Bartling & S. Friesike (Eds.), Opening Science (pp. 17–47). Springer International Publishing.
                                                https://doi.org/10.1007/978-3-319-00026-8_2.</li>
                                            <li>Kramer, B., & Bosman, J. (2015, June 18). The good, the efficient and the open—Changing research
                                                workflows and the need to move from Open Access to Open Science. CERN Workshop on Innovations in
                                                Scholarly Communication (OAI9), University of Geneva, Geneva, Switzerland.
                                                https://www.slideshare.net/bmkramer/the-good-the-efficient-and-the-open-oai9.</li>
                                            <li>Woelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature
                                                Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149.</li>
                                            <li>UNESCO. (2020). First draft of the UNESCO Recommendation on Open Science (Programme and Meeting
                                                Document SC-PCB-SPP/2020/OS/R1; p. 16). https://unesdoc.unesco.org/ark:/48223/pf0000374837.</li>
                                            <li>Documentazione di rdflib.</li>
                                        </ol>
                                    </li>
                                    <li>Ho seguito il corso Semantic Web Technologies del Prof. Harald Sack, reperibile all'indirizzo <a
                                            href="https://open.hpi.de/courses/semanticweb" target="_blank"
                                            alt="Link to the Semantic Web course">https://open.hpi.de/courses/semanticweb</a>, al fine di
                                        approfondire la mia conoscenza di SPARQL 1.1.</li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li>
                                        Allo scopo di ottenere tutte le triple di cui un'entità è soggetto da un triplestore, oc_ocdm fornisce
                                        il metodo import_entity_from_triplestore della classe Reader. Ecco un esempio di come ho provato a
                                        utilizzarla:
                                        <pre><code class="prettyprint">
g = GraphSet("https://github.com/arcangelo7/time_agnostic/")
qres = Reader().import_entity_from_triplestore(g, "http://localhost:9999/blazegraph/sparql", URIRef("https://github.com/arcangelo7/time_agnostic/br/1"),"https://orcid.org/0000-0002-8420-0696")
                                                            </code></pre>
                                        Nonostante l'entità esista ottengo però un ValueError, con messaggio di errore: "The required entity was
                                        not found or was not recognised as a proper OCDM entity". Dico che l'entità esiste perché facendo il
                                        medesimo CONSTRUCT tramite endpoint SPARQL ottengo i risultati attesi. Sto utilizzando correttamente il
                                        metodo? Ho infine risolto il problema utilizzando la classe SPARQLStore di rdflib o SPARQLWrapper, ma
                                        visto che oc_ocdm fornisce già uno shortcut preferirei utilizzare quello.
                                    </li>
                                    <li>La documentazione della classe SPARQLStore di rdflib, reperibile a <a
                                            href="https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.plugins.stores.html#rdflib.plugins.stores.sparqlstore.SPARQLStore"
                                            target="_blank" alt="SPARQLStore class documentation">questo indirizzo</a>, afferma:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            An RDFLib store around a SPARQL endpoint
                                            This is context-aware and should work as expected
                                            when a context is specified.
                                            For ConjunctiveGraphs, reading is done from the "default graph". Exactly
                                            what this means depends on your endpoint, because SPARQL does not offer a
                                            simple way to query the union of all graphs as it would be expected for a
                                            ConjuntiveGraph. This is why we recommend using Dataset instead, which is
                                            motivated by the SPARQL 1.1.
                                        </blockquote>
                                        Non ho capito cosa intenda dire.
                                    </li>
                                    <li>Dopo avere unito le entità associate agli stessi id è necessario caricare le modifiche sul triplestore.
                                        Dal codice di oc_ocdm ho inteso che la libreria si occupa di elaborare la query di update, definendo
                                        cosa vada cancellato e cosa aggiunto, ma dai tentativi fatti mi continuano a risultare 0 modifiche, per
                                        cui la query non viene lanciata. Sospetto c'entri qualcosa il metodo commit_changes(), il cui
                                        funzionamento mi risulta però oscuro.</li>
                                    <li>Non sono riuscito ad aggiornare la provenance a partire da un grafo di provenance preesistente.</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-5" data-target="#panel-5" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-5" id="d0704cnhc1p">
                            <span class="h6 mb-0 font-weight-bold">07/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-5">
                            <div class="pt-3">
                                <h3>Cosa ho fatto</h3>
                                <ol>
                                    <li>Le query SPARQL precedentemente effettuate tramite SPARQLStore utilizzano adesso SPARQLWrapper.</li>
                                    <li>Grafi di provenance e grafi sui dati vengono adesso generati in base alla storia pregressa tramite un
                                        indice.</li>
                                    <li>Effettuati i miglioramenti automatici, le nuove triple e loro provenance vengono adesso caricate
                                        correttamente sul triplestore.</li>
                                    <li>Grazie alla spiegazione fornitami da Simone, sono riuscito a importare correttamente un grafo da file
                                        json tramite metodo import_entities_from_graph della classe Reader di oc_ocdm.</li>
                                    <li>Sono state implementate due nuove funzioni, merge_by_id_from_file e add_coci_data_from_file, che anziché
                                        lavorare a partire da un triplestore lavorano a partire da un file. Di conseguenza, è possibile ottenere
                                        in output un nuovo file modificato anziché un upload su un triplestore. In questo modo potrò sia avere
                                        un file di backup che inviare a Cristian dei file rdf customizzati in base alle sue esigenze.</li>
                                    <li>Cristian mi ha fatto notare che avevo considerato solo gli autori come AgentRole. Ora il grafo iniziale
                                        comprende anche i publisher.</li>
                                    <li>Risolti alcuni bug nella funzione add_coci_data_from_triplestore:
                                        <ol>
                                            <li>Le entità di tipo cito:Citation preesistenti venivano prima ricercate solo tramite il DOI name
                                                del citato e non anche tramite il DOI name del citante.</li>
                                            <li>Le nuove entità venivano prima generate senza tenere conto dell'ultimo indice, provocando
                                                sovrapposizioni.</li>
                                            <li>Specificare l'argomento res nella ricreazione di un grafo preesistente permette adesso di
                                                prescindere dalla funzione di merge poiché, anche se presenti più entità associate ai medesimi
                                                identificativi, viene riprodotto solo il grafo dell'entità di interesse corrente.</li>
                                            <li>Nel caso in cui la funzione venga lanciata più volte sul medesimo triplestore, è stato aggiunto
                                                un controllo per cui le stesse informazioni non vengano aggiunte più volte.</li>
                                        </ol>
                                    </li>
                                    <li>Risolto un bug in merge_by_id_from_triplestore per cui le entità venivano unite a loro stesse.</li>
                                    <li>Aggiunta una nuova funzione di supporto che prende in input il percorso di un file rdf e restituisce in
                                        output un oggetto di tipo GraphSet.</li>
                                    <li>Tutte le funzioni hanno adesso parametri e output tipati, in osservanza di <a
                                            href="https://www.python.org/dev/peps/pep-3107/" alt="Link to PEP 3107" target="_blank">PEP
                                            3107</a>, allo scopo di aumentare la leggibilità del codice, sia per gli altri che per me. Si tratta
                                        quindi di semplici annotazioni, che non causeranno un'eccezione TypeError se non rispettate.</li>
                                    <li>Il vantaggio del lavorare con una certa mole di dati è che, mentre gli script vengono eseguiti, si ha
                                        molto tempo per leggere:
                                        <ol>
                                            <li>Belhajjame, K., B’Far, R., Cheney, J., Coppens, S., Cresswell, S., Gil, Y., Groth, P., Klyne,
                                                G., Lebo, T., McCusker, J., Miles, S., Myers, J., Sahoo, S., & Tilmes, C. (2013). PROV-DM: The
                                                PROV Data Model (L. Moreau & P. Missier, Eds.). World Wide Web Consortium.
                                                https://www.w3.org/TR/prov-dm/.</li>
                                            <li>Wolfe, M. (2017, August 9). CC0 and Data Citation.
                                                https://www.library.ucdavis.edu/news/cc0-and-data-citation/.</li>
                                            <li>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A.,
                                                Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J.,
                                                Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B.
                                                (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific
                                                Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18.</li>
                                            <li>GO FAIR. (2018). FAIR Principles. https://www.go-fair.org/fair-principles/.</li>
                                            <li>Open Knowledge Foundation. (2015). Open Definition 2.1. https://opendefinition.org/od/2.1/en/.
                                            </li>
                                            <li>Landi, A., Thompson, M., Giannuzzi, V., Bonifazi, F., Labastida, I., da Silva Santos, L. O. B.,
                                                & Roos, M. (2020). The “A” of FAIR – As Open as Possible, as Closed as Necessary. Data
                                                Intelligence, 2(1–2), 47–55. https://doi.org/10.1162/dint_a_00027.</li>
                                            <li>Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M.,
                                                L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V.,
                                                Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST
                                                Principles for digital repositories. Scientific Data, 7(1), 144.
                                                https://doi.org/10.1038/s41597-020-0486-7.</li>
                                            <li>Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS
                                                Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525.</li>
                                            <li>Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to
                                                uniquely identify researchers. Learned Publishing, 25(4), 259–264.
                                                https://doi.org/10.1087/20120404.</li>
                                            <li>Meng, X.-L. (2020). Reproducibility, Replicability, and Reliability. Harvard Data Science
                                                Review, 2(4). https://doi.org/10.1162/99608f92.dbfce7f9.</li>
                                            <li>Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 452–454.
                                                https://doi.org/10.1038/533452a.
                                                <ol>
                                                    <li>Vince il premio lettura più sconcertante della settimana, una di quelle che cambia il
                                                        modo di vedere le cose. </li>
                                                </ol>
                                            </li>
                                        </ol>
                                    </li>
                                </ol>
                                <h3>Cosa non ho capito</h3>
                                <ol>
                                    <li id="d0704cnhc1">Talvolta, provando a fare un merge tra due BibliographicResources, ottengo un errore di tipo TypeError
                                        che dice:
                                        <blockquote class="blockquote ml-5 mt-3">
                                            [BibliographicResource.contains_discourse_element] Expected argument type: de. Provided argument
                                            type: BibliographicReference.
                                        </blockquote>
                                        Poiché non sto in alcun modo utilizzando entità di tipo DiscourseElement, sospetto che si tratti di un
                                        bug.
                                    </li>
                                    <li>Portata a termine l'operazione di merge sulle entità associate agli stessi id e caricate le novità sul
                                        triplestore, ho riscontrato le stesse stranezze nell'output già riportate sul diario di bordo in data <a
                                            href="#d2303cnhc7">23/03 (<em>Cosa non ho capito</em>, punto 7)</a>. Da quello che ho visto, se
                                        unisco A a B, ad A viene associato l'id di B e B viene cancellata. Il problema è che A si ritrova ad
                                        essere associata a due entità di tipo datacite:Identifier che in realtà sono identiche, perché appunto A
                                        e B sono state unite in quando associate agli stessi id. Inoltre, l'entità B, nonostante sia stata
                                        cancellata, continua ad avere degli ingoing links da parte di tutte quelle entità che ad essa si
                                        riferivano in passato. C'è qualcosa del metodo merge che non sto capendo o qualcosa che dovrei fare e
                                        non sto facendo?</li>
                                    <li>È preferibile avere funzioni distinte che lavorino su triplestore e file o è meglio avere un'unica
                                        funzione che, a seconda degli argomenti passati, lavora indifferentemente su entrambi?</li>
                                    <li>Non so quanto sia ottimale effettuare query SPARQL su un'entità Graph di rdflib, perché credo manchi
                                        delle ottimizzazioni di un triplestore. Quando mi hai detto che devo lavorare sia su triplestore che su
                                        file intendevi che devo produrre entrambi gli output o che devo creare funzioni che lavorino in entrambe
                                        le modalità?</li>
                                    <li>Cristian mi ha detto che le librerie con cui lavora per produrre graph embeddings tendono a prendere in
                                        input un file tsv e non json. Tramite rdflib posso serializzare rdf in una moltitudine di notazioni,
                                        quali turtle, nt, json-ld, rdf/xml ed n3, ma onestamente non ho mai visto un grafo rdf serializzato in
                                        tsv. Si può fare?</li>
                                    <li>Mi piacerebbe portare avanti nuove idee per quanto riguarda possibili miglioramenti automatici del
                                        grafo, poiché quelle che ho al momento non mi convincono:
                                        <ol>
                                            <li>Correggere i DOI names sbagliati. Dato che è materia d'esame e che coinvolge altre 3 persone non
                                                mi sembra corretto occuparmene da solo per il tirocinio.</li>
                                            <li>Strutturare i campi unstructured forniti da Crossref precedentemente non considerati. Per farlo
                                                dovrei utilizzare nuovamente l'API di Crossref, mentre vorrei capire se è possibile migliorare
                                                ulteriormente quello che ho già senza chiedere altre risorse esterne.</li>
                                            <li>Arricchire i metadati delle entità citate, che mi convince poco per le stesse ragioni del punto
                                                precedente, ovvero che dovrei rivolgermi all'API di Crossref.</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                    <div class="card card-sm card-body bg-primary border-light mb-0">
                        <a href="#panel-6" data-target="#panel-6" class="accordion-panel-header" data-toggle="collapse"
                            role="button" aria-expanded="false" aria-controls="panel-6">
                            <span class="h6 mb-0 font-weight-bold">14/04/2021</span>
                            <span class="icon"><span class="fas fa-plus"></span></span>
                        </a>
                        <div class="collapse" id="panel-6">
                            <div class="pt-3">
                                <p class="mb-0"></p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Core -->
    <script src="./vendor/jquery/dist/jquery.min.js"></script>
    <script src="./vendor/popper.js/dist/umd/popper.min.js"></script>
    <script src="./vendor/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="./vendor/headroom.js/dist/headroom.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>

    <!-- Vendor JS -->
    <script src="./vendor/onscreen/dist/on-screen.umd.min.js"></script>
    <script src="./vendor/nouislider/distribute/nouislider.min.js"></script>
    <script src="./vendor/bootstrap-datepicker/dist/js/bootstrap-datepicker.min.js"></script>
    <script src="./vendor/waypoints/lib/jquery.waypoints.min.js"></script>
    <script src="./vendor/jarallax/dist/jarallax.min.js"></script>
    <script src="./vendor/jquery.counterup/jquery.counterup.min.js"></script>
    <script src="./vendor/jquery-countdown/dist/jquery.countdown.min.js"></script>
    <script src="./vendor/smooth-scroll/dist/smooth-scroll.polyfills.min.js"></script>
    <script src="./vendor/prismjs/prism.js"></script>
    <!-- Neumorphism JS -->
    <script src="./assets/js/neumorphism.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>